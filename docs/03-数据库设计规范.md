# æ•°å­—å‘˜å·¥ç³»ç»Ÿæ•°æ®åº“è®¾è®¡è§„èŒƒ
## Digital Employee System Database Design Standards v1.0

### ğŸ“‹ æ–‡æ¡£ä¿¡æ¯
- **æ–‡æ¡£ç‰ˆæœ¬**: v1.0
- **åˆ›å»ºæ—¥æœŸ**: 2024-01-24
- **é€‚ç”¨èŒƒå›´**: æ•°å­—å‘˜å·¥ç³»ç»Ÿæ‰€æœ‰æ•°æ®åº“è®¾è®¡å·¥ä½œ
- **ç»´æŠ¤éƒ¨é—¨**: æ•°æ®æ¶æ„ç»„

---

## ğŸ—ï¸ æ•°æ®åº“æ¶æ„è®¾è®¡

### 1.1 æ•´ä½“æ•°æ®æ¶æ„

#### 1.1.1 å¤šå±‚æ•°æ®æ¶æ„

```mermaid
graph TB
    subgraph "åº”ç”¨å±‚ Application Layer"
        A1[Agentç®¡ç†æœåŠ¡]
        A2[ä»»åŠ¡è°ƒåº¦æœåŠ¡]
        A3[Promptç®¡ç†æœåŠ¡]
        A4[ç›‘æ§æ²»ç†æœåŠ¡]
    end
    
    subgraph "æ•°æ®è®¿é—®å±‚ Data Access Layer"
        B1[ORMå±‚ SQLAlchemy]
        B2[ç¼“å­˜å±‚ Redis]
        B3[æœç´¢å±‚ Elasticsearch]
        B4[æ—¶åºæ•°æ® InfluxDB]
    end
    
    subgraph "æ•°æ®å­˜å‚¨å±‚ Data Storage Layer"
        C1[ä¸»æ•°æ®åº“ PostgreSQL]
        C2[æ–‡æ¡£æ•°æ®åº“ MongoDB]
        C3[å›¾æ•°æ®åº“ Neo4j]
        C4[å¯¹è±¡å­˜å‚¨ MinIO]
    end
    
    subgraph "æ•°æ®æ²»ç†å±‚ Data Governance Layer"
        D1[æ•°æ®è´¨é‡ç›‘æ§]
        D2[æ•°æ®è¡€ç¼˜è¿½è¸ª]
        D3[æ•°æ®å®‰å…¨å®¡è®¡]
        D4[æ•°æ®å¤‡ä»½æ¢å¤]
    end
    
    A1 --> B1
    A2 --> B2
    A3 --> B3
    A4 --> B4
    
    B1 --> C1
    B2 --> C1
    B3 --> C2
    B4 --> C3
    
    C1 --> D1
    C2 --> D2
    C3 --> D3
    C4 --> D4
```

#### 1.1.2 æ•°æ®åº“é€‰å‹åŸåˆ™

| æ•°æ®ç±»å‹ | æ•°æ®åº“ç±»å‹ | é€‰å‹æ ‡å‡† | ä½¿ç”¨åœºæ™¯ |
|----------|------------|----------|----------|
| **ç»“æ„åŒ–æ•°æ®** | å…³ç³»å‹æ•°æ®åº“ | ACIDç‰¹æ€§ï¼Œå¼ºä¸€è‡´æ€§ | Agenté…ç½®ã€ç”¨æˆ·ä¿¡æ¯ã€ä»»åŠ¡è®°å½• |
| **åŠç»“æ„åŒ–æ•°æ®** | æ–‡æ¡£æ•°æ®åº“ | çµæ´»æ¨¡å¼ï¼ŒJSONæ”¯æŒ | Promptæ¨¡æ¿ã€é…ç½®æ–‡ä»¶ |
| **æ—¶åºæ•°æ®** | æ—¶åºæ•°æ®åº“ | é«˜å†™å…¥æ€§èƒ½ï¼Œå‹ç¼©å­˜å‚¨ | ç›‘æ§æŒ‡æ ‡ã€æ—¥å¿—æ•°æ® |
| **å›¾å…³ç³»æ•°æ®** | å›¾æ•°æ®åº“ | å…³ç³»æŸ¥è¯¢ï¼Œè·¯å¾„åˆ†æ | Agentä¾èµ–å…³ç³» |
| **ç¼“å­˜æ•°æ®** | å†…å­˜æ•°æ®åº“ | é«˜é€Ÿè¯»å†™ï¼ŒæŒä¹…åŒ– | ä¼šè¯çŠ¶æ€ã€ä¸´æ—¶æ•°æ® |
| **æ–‡ä»¶æ•°æ®** | å¯¹è±¡å­˜å‚¨ | æ‰©å±•æ€§å¼ºï¼Œæˆæœ¬ä½ | æ—¥å¿—æ–‡ä»¶ã€å¤‡ä»½æ•°æ® |

### 1.2 æ•°æ®åº“æŠ€æœ¯æ ˆ

#### 1.2.1 ä¸»æ•°æ®åº“ - PostgreSQL 15+

```yaml
ç‰ˆæœ¬è¦æ±‚: PostgreSQL 15.0+
æ¨èé…ç½®:
  max_connections: 200
  shared_buffers: 256MB
  effective_cache_size: 1GB
  work_mem: 16MB
  maintenance_work_mem: 256MB
  
æ‰©å±•ç»„ä»¶:
  - uuid-ossp: UUIDç”Ÿæˆæ”¯æŒ
  - pg_stat_statements: SQLæ€§èƒ½åˆ†æ  
  - pg_trgm: å…¨æ–‡æœç´¢æ”¯æŒ
  - jsonb_plpython3u: JSONæ•°æ®å¤„ç†
  - pg_partman: è¡¨åˆ†åŒºç®¡ç†
  - timescaledb: æ—¶åºæ•°æ®æ‰©å±•(å¯é€‰)

è¿æ¥æ± é…ç½®:
  å·¥å…·: PgBouncer
  pool_mode: transaction
  max_client_conn: 1000
  default_pool_size: 100
```

#### 1.2.2 ç¼“å­˜æ•°æ®åº“ - Redis 7.0+

```yaml
ç‰ˆæœ¬è¦æ±‚: Redis 7.0+
éƒ¨ç½²æ¨¡å¼: Redis Cluster (3ä¸»3ä»)
å†…å­˜é…ç½®:
  maxmemory: 2GB
  maxmemory-policy: allkeys-lru
  
æŒä¹…åŒ–é…ç½®:
  save: "900 1 300 10 60 10000"
  appendonly: yes
  appendfsync: everysec
  
é›†ç¾¤é…ç½®:
  cluster-enabled: yes
  cluster-config-file: nodes.conf
  cluster-node-timeout: 5000
```

#### 1.2.3 æ–‡æ¡£æ•°æ®åº“ - MongoDB 6.0+

```yaml
ç‰ˆæœ¬è¦æ±‚: MongoDB 6.0+
éƒ¨ç½²æ¨¡å¼: å‰¯æœ¬é›† (1ä¸»2ä»)
å­˜å‚¨å¼•æ“: WiredTiger
é…ç½®å‚æ•°:
  journal: true
  directoryPerDB: true
  
ç´¢å¼•ç­–ç•¥:
  - å¤åˆç´¢å¼•ä¼˜å…ˆ
  - ç¨€ç–ç´¢å¼•ç”¨äºå¯é€‰å­—æ®µ
  - TTLç´¢å¼•ç”¨äºè‡ªåŠ¨è¿‡æœŸ
```

---

## ğŸ“Š æ•°æ®æ¨¡å‹è®¾è®¡æ ‡å‡†

### 2.1 æ¦‚å¿µæ•°æ®æ¨¡å‹

#### 2.1.1 æ ¸å¿ƒä¸šåŠ¡å®ä½“å…³ç³»

```mermaid
erDiagram
    USER ||--o{ AGENT : manages
    AGENT ||--o{ TASK : executes
    AGENT ||--o{ CAPABILITY : has
    AGENT ||--o{ PROMPT_TEMPLATE : uses
    
    TASK ||--o{ TASK_EXECUTION : generates
    TASK_EXECUTION ||--o{ EXECUTION_LOG : produces
    
    PROMPT_TEMPLATE ||--o{ PROMPT_VERSION : versioned_by
    PROMPT_VERSION ||--o{ PROMPT_USAGE : used_in
    
    AGENT ||--o{ AGENT_METRIC : monitored_by
    TASK ||--o{ TASK_METRIC : measured_by
    
    USER {
        uuid user_id PK
        string username UK
        string email UK
        string password_hash
        jsonb roles
        timestamp created_at
        timestamp updated_at
        boolean is_active
    }
    
    AGENT {
        uuid agent_id PK
        string agent_type
        string version
        jsonb config
        enum status
        timestamp created_at
        timestamp updated_at
        uuid created_by FK
    }
    
    TASK {
        uuid task_id PK
        string task_type
        enum priority
        jsonb data
        enum status
        timestamp deadline
        timestamp created_at
        timestamp updated_at
        uuid assigned_agent FK
    }
    
    PROMPT_TEMPLATE {
        uuid template_id PK
        string template_name UK
        string agent_type
        string task_type
        text description
        timestamp created_at
        timestamp updated_at
        uuid created_by FK
    }
```

#### 2.1.2 é¢†åŸŸæ¨¡å‹çº¦æŸ

| å®ä½“ç±»å‹ | ä¸»é”®çº¦æŸ | å”¯ä¸€æ€§çº¦æŸ | å¤–é”®çº¦æŸ | æ£€æŸ¥çº¦æŸ |
|----------|----------|------------|----------|----------|
| **ç”¨æˆ·(User)** | UUIDä¸»é”® | username, emailå”¯ä¸€ | æ—  | é‚®ç®±æ ¼å¼éªŒè¯ |
| **Agent** | UUIDä¸»é”® | (agent_type, version)å”¯ä¸€ | created_byå¼•ç”¨User | statusæšä¸¾æ£€æŸ¥ |
| **ä»»åŠ¡(Task)** | UUIDä¸»é”® | æ—  | assigned_agentå¼•ç”¨Agent | priorityæšä¸¾æ£€æŸ¥ |
| **Promptæ¨¡æ¿** | UUIDä¸»é”® | template_nameå”¯ä¸€ | created_byå¼•ç”¨User | agent_typeéç©º |

### 2.2 é€»è¾‘æ•°æ®æ¨¡å‹

#### 2.2.1 åˆ†å±‚æ¨¡å‹è®¾è®¡

```mermaid
graph TB
    subgraph "ä¸šåŠ¡å±‚ Business Layer"
        B1[ç”¨æˆ·ç®¡ç†åŸŸ]
        B2[Agentç®¡ç†åŸŸ]
        B3[ä»»åŠ¡è°ƒåº¦åŸŸ]
        B4[Promptç®¡ç†åŸŸ]
        B5[ç›‘æ§æ²»ç†åŸŸ]
    end
    
    subgraph "æ•°æ®å±‚ Data Layer"
        D1[ç”¨æˆ·æ•°æ®è¡¨]
        D2[Agentæ•°æ®è¡¨]
        D3[ä»»åŠ¡æ•°æ®è¡¨]
        D4[Promptæ•°æ®è¡¨]
        D5[ç›‘æ§æ•°æ®è¡¨]
    end
    
    subgraph "å­˜å‚¨å±‚ Storage Layer"
        S1[ä¸»æ•°æ®åº“åˆ†åŒº]
        S2[ç¼“å­˜æ•°æ®åˆ†ç‰‡]
        S3[æ—¥å¿—æ•°æ®åˆ†åŒº]
    end
    
    B1 --> D1
    B2 --> D2
    B3 --> D3
    B4 --> D4
    B5 --> D5
    
    D1 --> S1
    D2 --> S1
    D3 --> S1
    D4 --> S2
    D5 --> S3
```

#### 2.2.2 æ•°æ®åŸŸå®šä¹‰

| æ•°æ®åŸŸ | ä¸šåŠ¡å«ä¹‰ | è¡¨æ•°é‡ | é¢„ä¼°æ•°æ®é‡ | å¢é•¿ç‡ |
|--------|----------|--------|------------|--------|
| **ç”¨æˆ·ç®¡ç†åŸŸ** | ç”¨æˆ·ä¿¡æ¯ã€è§’è‰²æƒé™ | 5ä¸ªè¡¨ | 10ä¸‡æ¡/å¹´ | 20%/å¹´ |
| **Agentç®¡ç†åŸŸ** | Agenté…ç½®ã€çŠ¶æ€ç®¡ç† | 8ä¸ªè¡¨ | 1ä¸‡æ¡/å¹´ | 50%/å¹´ |
| **ä»»åŠ¡è°ƒåº¦åŸŸ** | ä»»åŠ¡æ‰§è¡Œã€è°ƒåº¦è®°å½• | 6ä¸ªè¡¨ | 100ä¸‡æ¡/æœˆ | 100%/å¹´ |
| **Promptç®¡ç†åŸŸ** | Promptæ¨¡æ¿ã€ç‰ˆæœ¬ç®¡ç† | 4ä¸ªè¡¨ | 1ä¸‡æ¡/å¹´ | 30%/å¹´ |
| **ç›‘æ§æ²»ç†åŸŸ** | ç›‘æ§æŒ‡æ ‡ã€æ—¥å¿—æ•°æ® | 10ä¸ªè¡¨ | 1000ä¸‡æ¡/æœˆ | 200%/å¹´ |

---

## ğŸ—ƒï¸ æ•°æ®åº“è®¾è®¡åŸåˆ™

### 3.1 è§„èŒƒåŒ–è®¾è®¡åŸåˆ™

#### 3.1.1 èŒƒå¼è¦æ±‚

| èŒƒå¼ç­‰çº§ | é€‚ç”¨åœºæ™¯ | è®¾è®¡è¦æ±‚ | ä¾‹å¤–æƒ…å†µ |
|----------|----------|----------|----------|
| **ç¬¬ä¸€èŒƒå¼(1NF)** | æ‰€æœ‰è¡¨ | åŸå­æ€§æ•°æ®ï¼Œæ— é‡å¤åˆ— | æ— ä¾‹å¤– |
| **ç¬¬äºŒèŒƒå¼(2NF)** | ä¸šåŠ¡ä¸»è¡¨ | æ¶ˆé™¤éƒ¨åˆ†ä¾èµ– | å†—ä½™å­—æ®µä¼˜åŒ–é™¤å¤– |
| **ç¬¬ä¸‰èŒƒå¼(3NF)** | æ ¸å¿ƒä¸šåŠ¡è¡¨ | æ¶ˆé™¤ä¼ é€’ä¾èµ– | æ€§èƒ½è€ƒè™‘å¯é€‚å½“å†—ä½™ |
| **BCNFèŒƒå¼** | å…³é”®ä¸»è¡¨ | æ¶ˆé™¤ä¸»å±æ€§ä¾èµ– | å¤æ‚æŸ¥è¯¢åœºæ™¯é™¤å¤– |

#### 3.1.2 åè§„èŒƒåŒ–ç­–ç•¥

```sql
-- ç¤ºä¾‹ï¼šAgentæ‰§è¡Œç»Ÿè®¡è¡¨(åè§„èŒƒåŒ–è®¾è®¡)
CREATE TABLE agent_performance_summary (
    agent_id UUID PRIMARY KEY,
    agent_type VARCHAR(50) NOT NULL,
    
    -- å†—ä½™åŸºç¡€ä¿¡æ¯(å‡å°‘JOIN)
    agent_name VARCHAR(100) NOT NULL,
    created_at TIMESTAMP NOT NULL,
    
    -- æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
    total_tasks_executed INTEGER DEFAULT 0,
    successful_tasks INTEGER DEFAULT 0,
    failed_tasks INTEGER DEFAULT 0,
    average_execution_time DECIMAL(10,3) DEFAULT 0,
    
    -- æœ€è¿‘æ›´æ–°æ—¶é—´
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    CONSTRAINT fk_agent_id FOREIGN KEY (agent_id) REFERENCES agents(agent_id)
);

-- æ›´æ–°è§¦å‘å™¨ç»´æŠ¤ç»Ÿè®¡æ•°æ®
CREATE OR REPLACE FUNCTION update_agent_performance()
RETURNS TRIGGER AS $$
BEGIN
    INSERT INTO agent_performance_summary (agent_id, agent_type, agent_name)
    SELECT NEW.assigned_agent, a.agent_type, a.config->>'name'
    FROM agents a WHERE a.agent_id = NEW.assigned_agent
    ON CONFLICT (agent_id) DO UPDATE SET
        total_tasks_executed = agent_performance_summary.total_tasks_executed + 1,
        successful_tasks = CASE 
            WHEN NEW.status = 'completed' THEN agent_performance_summary.successful_tasks + 1 
            ELSE agent_performance_summary.successful_tasks 
        END,
        failed_tasks = CASE 
            WHEN NEW.status = 'failed' THEN agent_performance_summary.failed_tasks + 1 
            ELSE agent_performance_summary.failed_tasks 
        END,
        last_updated = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;
```

### 3.2 æ•°æ®ç±»å‹é€‰æ‹©æ ‡å‡†

#### 3.2.1 åŸºç¡€æ•°æ®ç±»å‹è§„èŒƒ

| ä¸šåŠ¡ç±»å‹ | PostgreSQLç±»å‹ | é•¿åº¦é™åˆ¶ | ä½¿ç”¨åœºæ™¯ | æ³¨æ„äº‹é¡¹ |
|----------|----------------|----------|----------|----------|
| **ä¸»é”®ID** | UUID | å›ºå®š36å­—ç¬¦ | æ‰€æœ‰ä¸»é”® | ä½¿ç”¨uuid_generate_v4() |
| **ä¸šåŠ¡ID** | VARCHAR(50) | æœ€å¤§50å­—ç¬¦ | ä¸šåŠ¡æ ‡è¯†ç¬¦ | å¿…é¡»å»ºç«‹å”¯ä¸€ç´¢å¼• |
| **åç§°ç±»** | VARCHAR(100) | æœ€å¤§100å­—ç¬¦ | åç§°ã€æ ‡é¢˜ | è€ƒè™‘å›½é™…åŒ–é•¿åº¦ |
| **æè¿°ç±»** | TEXT | æ— é™åˆ¶ | é•¿æ–‡æœ¬æè¿° | é¿å…è¿‡åº¦ä½¿ç”¨ |
| **é…ç½®ç±»** | JSONB | æ— é™åˆ¶ | çµæ´»é…ç½® | å»ºç«‹GINç´¢å¼• |
| **çŠ¶æ€ç±»** | VARCHAR(20) | æœ€å¤§20å­—ç¬¦ | æšä¸¾çŠ¶æ€ | ä½¿ç”¨CHECKçº¦æŸ |
| **æ—¶é—´ç±»** | TIMESTAMP | å›ºå®š8å­—èŠ‚ | æ—¶é—´æˆ³ | ç»Ÿä¸€ä½¿ç”¨UTCæ—¶åŒº |
| **æ•°å€¼ç±»** | DECIMAL(10,2) | æ ¹æ®ç²¾åº¦ | é‡‘é¢ã€æ¯”ä¾‹ | é¿å…æµ®ç‚¹æ•° |
| **å¸ƒå°”ç±»** | BOOLEAN | 1å­—èŠ‚ | æ˜¯å¦æ ‡è¯† | æ˜ç¡®é»˜è®¤å€¼ |

#### 3.2.2 JSONæ•°æ®ç±»å‹è§„èŒƒ

```sql
-- JSONBå­—æ®µè®¾è®¡ç¤ºä¾‹
CREATE TABLE agents (
    agent_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    agent_type VARCHAR(50) NOT NULL,
    
    -- ç»“æ„åŒ–é…ç½®(æ¨èJSONB)
    config JSONB NOT NULL DEFAULT '{}',
    
    -- é…ç½®JSONç»“æ„çº¦æŸ
    CONSTRAINT config_structure_check CHECK (
        config ? 'max_concurrent_tasks' AND
        config ? 'timeout_seconds' AND
        jsonb_typeof(config->'max_concurrent_tasks') = 'number' AND
        jsonb_typeof(config->'timeout_seconds') = 'number'
    ),
    
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- JSONBå­—æ®µç´¢å¼•
CREATE INDEX idx_agents_config_gin ON agents USING GIN (config);
CREATE INDEX idx_agents_config_max_tasks ON agents USING BTREE ((config->>'max_concurrent_tasks'));
```

### 3.3 ç´¢å¼•è®¾è®¡ç­–ç•¥

#### 3.3.1 ç´¢å¼•ç±»å‹é€‰æ‹©

| ç´¢å¼•ç±»å‹ | é€‚ç”¨åœºæ™¯ | åˆ›å»ºè¯­æ³• | æ€§èƒ½ç‰¹ç‚¹ |
|----------|----------|----------|----------|
| **B-Treeç´¢å¼•** | ç­‰å€¼æŸ¥è¯¢ã€èŒƒå›´æŸ¥è¯¢ | `CREATE INDEX idx_name ON table(column)` | å¹³è¡¡æŸ¥è¯¢å’Œæ›´æ–°æ€§èƒ½ |
| **Hashç´¢å¼•** | ç­‰å€¼æŸ¥è¯¢(ä»…é™) | `CREATE INDEX idx_name ON table USING HASH(column)` | ç­‰å€¼æŸ¥è¯¢æœ€å¿« |
| **GINç´¢å¼•** | JSONBã€å…¨æ–‡æœç´¢ | `CREATE INDEX idx_name ON table USING GIN(column)` | å¤æ‚æŸ¥è¯¢æ”¯æŒå¥½ |
| **GiSTç´¢å¼•** | å‡ ä½•æ•°æ®ã€èŒƒå›´ç±»å‹ | `CREATE INDEX idx_name ON table USING GIST(column)` | ç‰¹æ®Šæ•°æ®ç±»å‹ |
| **éƒ¨åˆ†ç´¢å¼•** | æ¡ä»¶è¿‡æ»¤æ•°æ® | `CREATE INDEX idx_name ON table(column) WHERE condition` | èŠ‚çœå­˜å‚¨ç©ºé—´ |
| **å¤åˆç´¢å¼•** | å¤šå­—æ®µæŸ¥è¯¢ | `CREATE INDEX idx_name ON table(col1, col2, col3)` | æ³¨æ„å­—æ®µé¡ºåº |

#### 3.3.2 ç´¢å¼•è®¾è®¡è§„åˆ™

```sql
-- 1. ä¸»é”®è‡ªåŠ¨åˆ›å»ºå”¯ä¸€ç´¢å¼•
CREATE TABLE users (
    user_id UUID PRIMARY KEY,  -- è‡ªåŠ¨åˆ›å»ºå”¯ä¸€ç´¢å¼•
    username VARCHAR(50) UNIQUE,  -- è‡ªåŠ¨åˆ›å»ºå”¯ä¸€ç´¢å¼•
    email VARCHAR(100) UNIQUE     -- è‡ªåŠ¨åˆ›å»ºå”¯ä¸€ç´¢å¼•
);

-- 2. å¤–é”®å­—æ®µå»ºè®®åˆ›å»ºç´¢å¼•
CREATE INDEX idx_tasks_assigned_agent ON tasks(assigned_agent);
CREATE INDEX idx_task_executions_task_id ON task_executions(task_id);

-- 3. é¢‘ç¹æŸ¥è¯¢å­—æ®µåˆ›å»ºç´¢å¼•
CREATE INDEX idx_tasks_status ON tasks(status);
CREATE INDEX idx_tasks_created_at ON tasks(created_at);
CREATE INDEX idx_agents_agent_type ON agents(agent_type);

-- 4. å¤åˆç´¢å¼•è€ƒè™‘å­—æ®µé¡ºåº(æœ€å·¦å‰ç¼€åŸåˆ™)
CREATE INDEX idx_tasks_composite ON tasks(status, priority, created_at);

-- 5. éƒ¨åˆ†ç´¢å¼•ä¼˜åŒ–ç‰¹å®šæŸ¥è¯¢
CREATE INDEX idx_tasks_active ON tasks(assigned_agent) WHERE status IN ('pending', 'running');

-- 6. JSONBå­—æ®µçš„ç´¢å¼•ç­–ç•¥
CREATE INDEX idx_agents_config_gin ON agents USING GIN (config);
CREATE INDEX idx_agents_capabilities ON agents USING GIN ((config->'capabilities'));
```

---

## ğŸ”§ æ•°æ®åº“è®¾è®¡æ¨¡å¼

### 4.1 åˆ†åŒºè¡¨è®¾è®¡

#### 4.1.1 åˆ†åŒºç­–ç•¥é€‰æ‹©

| åˆ†åŒºç±»å‹ | é€‚ç”¨åœºæ™¯ | åˆ†åŒºé”®é€‰æ‹© | ç»´æŠ¤æˆæœ¬ |
|----------|----------|------------|----------|
| **æ—¶é—´åˆ†åŒº** | æ—¶åºæ•°æ®ã€æ—¥å¿—è¡¨ | æ—¶é—´å­—æ®µ | è‡ªåŠ¨åŒ–ç¨‹åº¦é«˜ |
| **å“ˆå¸Œåˆ†åŒº** | å‡åŒ€åˆ†å¸ƒæ•°æ® | å“ˆå¸Œå­—æ®µ | ç»´æŠ¤ç®€å• |
| **èŒƒå›´åˆ†åŒº** | æœ‰åºæ•°æ® | æ•°å€¼/æ—¶é—´èŒƒå›´ | éœ€è¦ç»´æŠ¤è¾¹ç•Œ |
| **åˆ—è¡¨åˆ†åŒº** | æšä¸¾ç±»å‹æ•°æ® | ç¦»æ•£å€¼ | å›ºå®šåˆ†åŒºæ•°é‡ |

#### 4.1.2 åˆ†åŒºè¡¨å®ç°ç¤ºä¾‹

```sql
-- ä»»åŠ¡æ‰§è¡Œæ—¥å¿—æŒ‰æœˆåˆ†åŒº
CREATE TABLE task_executions (
    execution_id UUID DEFAULT uuid_generate_v4(),
    task_id UUID NOT NULL,
    agent_id UUID NOT NULL,
    status VARCHAR(20) NOT NULL,
    execution_time DECIMAL(10,3),
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    
    PRIMARY KEY (execution_id, created_at)
) PARTITION BY RANGE (created_at);

-- åˆ›å»ºåˆ†åŒºè¡¨
CREATE TABLE task_executions_2024_01 PARTITION OF task_executions
    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE task_executions_2024_02 PARTITION OF task_executions  
    FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

-- åˆ›å»ºåˆ†åŒºç»´æŠ¤å‡½æ•°
CREATE OR REPLACE FUNCTION create_monthly_partition(table_name text, start_date date)
RETURNS void AS $$
DECLARE
    partition_name text;
    end_date date;
BEGIN
    partition_name := table_name || '_' || to_char(start_date, 'YYYY_MM');
    end_date := start_date + interval '1 month';
    
    EXECUTE format('CREATE TABLE %I PARTITION OF %I FOR VALUES FROM (%L) TO (%L)',
                   partition_name, table_name, start_date, end_date);
                   
    EXECUTE format('CREATE INDEX %I ON %I (task_id)', 
                   'idx_' || partition_name || '_task_id', partition_name);
END;
$$ LANGUAGE plpgsql;
```

### 4.2 è¯»å†™åˆ†ç¦»è®¾è®¡

#### 4.2.1 ä¸»ä»æ¶æ„é…ç½®

```yaml
# ä¸»æ•°æ®åº“é…ç½®(å†™å…¥)
master_database:
  host: postgres-master.local
  port: 5432
  max_connections: 200
  synchronous_commit: on
  
# ä»æ•°æ®åº“é…ç½®(è¯»å–)  
slave_databases:
  - host: postgres-slave-1.local
    port: 5432
    max_connections: 100
    hot_standby: on
  - host: postgres-slave-2.local
    port: 5432  
    max_connections: 100
    hot_standby: on

# è¯»å†™åˆ†ç¦»è·¯ç”±è§„åˆ™
routing_rules:
  - pattern: "SELECT|EXPLAIN|ANALYZE"
    target: slave
    load_balance: round_robin
  - pattern: "INSERT|UPDATE|DELETE|CREATE|DROP|ALTER"
    target: master
```

#### 4.2.2 æ•°æ®åº“è¿æ¥æ± é…ç½®

```python
# SQLAlchemyè¯»å†™åˆ†ç¦»é…ç½®
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

class DatabaseConfig:
    """æ•°æ®åº“é…ç½®ç®¡ç†"""
    
    def __init__(self):
        # ä¸»åº“é…ç½®(å†™)
        self.master_url = "postgresql+asyncpg://user:pass@master:5432/db"
        self.master_engine = create_engine(
            self.master_url,
            poolclass=QueuePool,
            pool_size=20,
            max_overflow=30,
            pool_pre_ping=True,
            pool_recycle=3600
        )
        
        # ä»åº“é…ç½®(è¯»)
        self.slave_urls = [
            "postgresql+asyncpg://user:pass@slave1:5432/db",
            "postgresql+asyncpg://user:pass@slave2:5432/db"
        ]
        self.slave_engines = [
            create_engine(url, poolclass=QueuePool, pool_size=10, max_overflow=20)
            for url in self.slave_urls
        ]
        
    def get_read_engine(self):
        """è·å–è¯»æ•°æ®åº“å¼•æ“(è´Ÿè½½å‡è¡¡)"""
        import random
        return random.choice(self.slave_engines)
        
    def get_write_engine(self):
        """è·å–å†™æ•°æ®åº“å¼•æ“"""
        return self.master_engine
```

### 4.3 ç¼“å­˜é›†æˆè®¾è®¡

#### 4.3.1 å¤šçº§ç¼“å­˜æ¶æ„

```mermaid
graph TB
    A[åº”ç”¨è¯·æ±‚] --> B{L1ç¼“å­˜}
    B -->|å‘½ä¸­| C[è¿›ç¨‹å†…å­˜ç¼“å­˜]
    B -->|æœªå‘½ä¸­| D{L2ç¼“å­˜}
    D -->|å‘½ä¸­| E[Redisç¼“å­˜]
    D -->|æœªå‘½ä¸­| F{L3ç¼“å­˜}
    F -->|å‘½ä¸­| G[æ•°æ®åº“è¿æ¥æ± ]
    F -->|æœªå‘½ä¸­| H[PostgreSQLæ•°æ®åº“]
    
    C --> I[è¿”å›ç»“æœ]
    E --> I
    G --> I
    H --> J[æ›´æ–°ç¼“å­˜]
    J --> I
```

#### 4.3.2 ç¼“å­˜ç­–ç•¥å®ç°

```python
import asyncio
import json
from typing import Any, Optional, Callable
from functools import wraps

class CacheManager:
    """å¤šçº§ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, redis_client, local_cache_size=1000):
        self.redis = redis_client
        self.local_cache = {}  # ç®€åŒ–çš„æœ¬åœ°ç¼“å­˜
        self.local_cache_size = local_cache_size
        
    async def get_or_set(self, 
                        key: str, 
                        factory: Callable,
                        ttl: int = 3600,
                        use_local: bool = True) -> Any:
        """è·å–æˆ–è®¾ç½®ç¼“å­˜"""
        
        # L1: æœ¬åœ°ç¼“å­˜
        if use_local and key in self.local_cache:
            return self.local_cache[key]['data']
            
        # L2: Redisç¼“å­˜
        cached_value = await self.redis.get(key)
        if cached_value:
            data = json.loads(cached_value)
            if use_local:
                self._set_local_cache(key, data)
            return data
            
        # L3: æ•°æ®æº
        data = await factory()
        
        # æ›´æ–°ç¼“å­˜
        await self.redis.setex(key, ttl, json.dumps(data, default=str))
        if use_local:
            self._set_local_cache(key, data)
            
        return data
    
    def _set_local_cache(self, key: str, data: Any):
        """è®¾ç½®æœ¬åœ°ç¼“å­˜"""
        if len(self.local_cache) >= self.local_cache_size:
            # ç®€å•LRUæ·˜æ±°
            oldest_key = next(iter(self.local_cache))
            del self.local_cache[oldest_key]
            
        self.local_cache[key] = {
            'data': data,
            'timestamp': asyncio.get_event_loop().time()
        }

def cached(ttl: int = 3600, key_prefix: str = ""):
    """ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = f"{key_prefix}:{func.__name__}:{hash(str(args) + str(kwargs))}"
            
            cache_manager = get_cache_manager()  # è·å–ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
            
            return await cache_manager.get_or_set(
                cache_key, 
                lambda: func(*args, **kwargs),
                ttl
            )
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@cached(ttl=300, key_prefix="agent")
async def get_agent_config(agent_id: str):
    """è·å–Agenté…ç½®(å¸¦ç¼“å­˜)"""
    # æ•°æ®åº“æŸ¥è¯¢é€»è¾‘
    pass
```

---

## ğŸ”’ æ•°æ®å®‰å…¨è®¾è®¡

### 5.1 æ•°æ®åŠ å¯†ç­–ç•¥

#### 5.1.1 å­—æ®µçº§åŠ å¯†

```sql
-- åˆ›å»ºåŠ å¯†æ‰©å±•
CREATE EXTENSION IF NOT EXISTS pgcrypto;

-- æ•æ„Ÿæ•°æ®åŠ å¯†å­˜å‚¨
CREATE TABLE users (
    user_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    username VARCHAR(50) UNIQUE NOT NULL,
    
    -- å¯†ç å“ˆå¸Œå­˜å‚¨
    password_hash VARCHAR(255) NOT NULL,
    
    -- æ•æ„Ÿä¿¡æ¯åŠ å¯†å­˜å‚¨
    email_encrypted BYTEA,  -- åŠ å¯†é‚®ç®±
    phone_encrypted BYTEA,  -- åŠ å¯†æ‰‹æœºå·
    
    -- ç”¨äºæœç´¢çš„å“ˆå¸Œå€¼
    email_hash VARCHAR(64),  -- é‚®ç®±å“ˆå¸Œ(ç”¨äºæŸ¥è¯¢)
    phone_hash VARCHAR(64),  -- æ‰‹æœºå·å“ˆå¸Œ
    
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- åˆ›å»ºåŠ å¯†/è§£å¯†å‡½æ•°
CREATE OR REPLACE FUNCTION encrypt_pii(data TEXT, key TEXT)
RETURNS BYTEA AS $$
BEGIN
    RETURN pgp_sym_encrypt(data, key);
END;
$$ LANGUAGE plpgsql;

CREATE OR REPLACE FUNCTION decrypt_pii(encrypted_data BYTEA, key TEXT)
RETURNS TEXT AS $$
BEGIN
    RETURN pgp_sym_decrypt(encrypted_data, key);
END;
$$ LANGUAGE plpgsql;

-- æ’å…¥åŠ å¯†æ•°æ®ç¤ºä¾‹
INSERT INTO users (username, password_hash, email_encrypted, email_hash)
VALUES (
    'john_doe',
    crypt('user_password', gen_salt('bf', 12)),
    encrypt_pii('john@example.com', 'encryption_key'),  
    encode(digest('john@example.com', 'sha256'), 'hex')
);
```

#### 5.1.2 é€æ˜æ•°æ®åŠ å¯†(TDE)

```yaml
# PostgreSQL TDEé…ç½®
postgresql_config:
  # å¯ç”¨æ•°æ®åŠ å¯†
  ssl: on
  ssl_cert_file: 'server.crt'
  ssl_key_file: 'server.key'
  ssl_ca_file: 'ca.crt'
  
  # æ•°æ®ç›®å½•åŠ å¯†
  data_encryption: enabled
  encryption_key_command: '/usr/local/bin/get-encryption-key.sh'
  
  # WALæ—¥å¿—åŠ å¯†
  wal_encryption: on
  
# å¤‡ä»½åŠ å¯†é…ç½®
backup_encryption:
  tool: pg_dump
  encryption: 'gpg --cipher-algo AES256 --compress-algo 2'
  key_management: 'HashiCorp Vault'
```

### 5.2 è®¿é—®æ§åˆ¶è®¾è®¡

#### 5.2.1 è¡Œçº§å®‰å…¨ç­–ç•¥(RLS)

```sql
-- å¯ç”¨è¡Œçº§å®‰å…¨
ALTER TABLE agents ENABLE ROW LEVEL SECURITY;
ALTER TABLE tasks ENABLE ROW LEVEL SECURITY;

-- åˆ›å»ºå®‰å…¨ç­–ç•¥ï¼šç”¨æˆ·åªèƒ½è®¿é—®è‡ªå·±åˆ›å»ºçš„Agent
CREATE POLICY agent_owner_policy ON agents
    FOR ALL
    TO application_user
    USING (created_by = current_setting('app.current_user_id')::UUID);

-- åˆ›å»ºå®‰å…¨ç­–ç•¥ï¼šAgentåªèƒ½è®¿é—®åˆ†é…ç»™è‡ªå·±çš„ä»»åŠ¡
CREATE POLICY task_assignment_policy ON tasks
    FOR SELECT
    TO agent_user
    USING (assigned_agent = current_setting('app.current_agent_id')::UUID);

-- åˆ›å»ºå®‰å…¨ç­–ç•¥ï¼šç®¡ç†å‘˜å¯ä»¥è®¿é—®æ‰€æœ‰æ•°æ®
CREATE POLICY admin_full_access ON agents
    FOR ALL
    TO admin_user
    USING (true);

-- è®¾ç½®åº”ç”¨çº§ç”¨æˆ·ä¸Šä¸‹æ–‡
CREATE OR REPLACE FUNCTION set_user_context(user_id UUID, user_role TEXT)
RETURNS void AS $$
BEGIN
    PERFORM set_config('app.current_user_id', user_id::TEXT, false);
    PERFORM set_config('app.current_user_role', user_role, false);
END;
$$ LANGUAGE plpgsql;
```

#### 5.2.2 æ•°æ®åº“è§’è‰²æƒé™è®¾è®¡

```sql
-- åˆ›å»ºæ•°æ®åº“è§’è‰²
CREATE ROLE readonly_user;
CREATE ROLE application_user;
CREATE ROLE agent_user;
CREATE ROLE admin_user;

-- åªè¯»ç”¨æˆ·æƒé™
GRANT CONNECT ON DATABASE digital_employee TO readonly_user;
GRANT USAGE ON SCHEMA public TO readonly_user;
GRANT SELECT ON ALL TABLES IN SCHEMA public TO readonly_user;

-- åº”ç”¨ç”¨æˆ·æƒé™
GRANT CONNECT ON DATABASE digital_employee TO application_user;
GRANT USAGE ON SCHEMA public TO application_user;
GRANT SELECT, INSERT, UPDATE ON users, agents, tasks TO application_user;
GRANT USAGE ON ALL SEQUENCES IN SCHEMA public TO application_user;

-- Agentç”¨æˆ·æƒé™(å—é™)
GRANT CONNECT ON DATABASE digital_employee TO agent_user;
GRANT USAGE ON SCHEMA public TO agent_user;
GRANT SELECT ON agents, tasks, prompt_templates TO agent_user;
GRANT INSERT, UPDATE ON task_executions, agent_metrics TO agent_user;

-- ç®¡ç†å‘˜æƒé™
GRANT ALL PRIVILEGES ON DATABASE digital_employee TO admin_user;

-- åˆ›å»ºå…·ä½“ç”¨æˆ·å¹¶åˆ†é…è§’è‰²
CREATE USER app_server WITH PASSWORD 'secure_password';
GRANT application_user TO app_server;

CREATE USER agent_runner WITH PASSWORD 'agent_password';
GRANT agent_user TO agent_runner;
```

### 5.3 å®¡è®¡æ—¥å¿—è®¾è®¡

#### 5.3.1 æ•°æ®åº“å®¡è®¡è¡¨è®¾è®¡

```sql
-- æ•°æ®å˜æ›´å®¡è®¡è¡¨
CREATE TABLE audit_log (
    audit_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    table_name VARCHAR(50) NOT NULL,
    record_id UUID NOT NULL,
    operation VARCHAR(10) NOT NULL,  -- INSERT, UPDATE, DELETE
    old_values JSONB,
    new_values JSONB,
    changed_by UUID,
    changed_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    ip_address INET,
    user_agent TEXT,
    
    -- ç´¢å¼•ä¼˜åŒ–
    INDEX idx_audit_log_table_record (table_name, record_id),
    INDEX idx_audit_log_changed_at (changed_at),
    INDEX idx_audit_log_changed_by (changed_by)
) PARTITION BY RANGE (changed_at);

-- åˆ›å»ºå®¡è®¡è§¦å‘å™¨å‡½æ•°
CREATE OR REPLACE FUNCTION audit_trigger_function()
RETURNS TRIGGER AS $$
BEGIN
    INSERT INTO audit_log (
        table_name, 
        record_id, 
        operation, 
        old_values, 
        new_values, 
        changed_by,
        ip_address
    ) VALUES (
        TG_TABLE_NAME,
        COALESCE(NEW.id, OLD.id),
        TG_OP,
        CASE WHEN TG_OP = 'DELETE' THEN to_jsonb(OLD) ELSE NULL END,
        CASE WHEN TG_OP = 'INSERT' THEN to_jsonb(NEW) 
             WHEN TG_OP = 'UPDATE' THEN to_jsonb(NEW) 
             ELSE NULL END,
        current_setting('app.current_user_id', true)::UUID,
        inet_client_addr()
    );
    
    RETURN COALESCE(NEW, OLD);
END;
$$ LANGUAGE plpgsql;

-- ä¸ºå…³é”®è¡¨åˆ›å»ºå®¡è®¡è§¦å‘å™¨
CREATE TRIGGER audit_users_trigger
    AFTER INSERT OR UPDATE OR DELETE ON users
    FOR EACH ROW EXECUTE FUNCTION audit_trigger_function();

CREATE TRIGGER audit_agents_trigger  
    AFTER INSERT OR UPDATE OR DELETE ON agents
    FOR EACH ROW EXECUTE FUNCTION audit_trigger_function();
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–è®¾è®¡

### 6.1 æŸ¥è¯¢ä¼˜åŒ–ç­–ç•¥

#### 6.1.1 æ…¢æŸ¥è¯¢ç›‘æ§

```sql
-- å¯ç”¨æ…¢æŸ¥è¯¢æ—¥å¿—
ALTER SYSTEM SET log_min_duration_statement = 1000;  -- è®°å½•è¶…è¿‡1ç§’çš„æŸ¥è¯¢
ALTER SYSTEM SET log_statement = 'all';
ALTER SYSTEM SET log_duration = on;
SELECT pg_reload_conf();

-- åˆ›å»ºæŸ¥è¯¢æ€§èƒ½ç›‘æ§è§†å›¾
CREATE VIEW slow_queries AS
SELECT 
    query,
    calls,
    total_time,
    mean_time,
    rows,
    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
FROM pg_stat_statements
WHERE mean_time > 100  -- å¹³å‡æ‰§è¡Œæ—¶é—´è¶…è¿‡100ms
ORDER BY total_time DESC;

-- åˆ›å»ºç´¢å¼•ä½¿ç”¨ç‡ç›‘æ§è§†å›¾
CREATE VIEW index_usage AS
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_tup_read,
    idx_tup_fetch,
    idx_scan,
    CASE 
        WHEN idx_scan = 0 THEN 'UNUSED'
        WHEN idx_scan < 100 THEN 'LOW_USAGE'
        ELSE 'NORMAL'
    END as usage_status
FROM pg_stat_user_indexes
ORDER BY idx_scan ASC;
```

#### 6.1.2 æŸ¥è¯¢è®¡åˆ’ä¼˜åŒ–

```sql
-- ç¤ºä¾‹ï¼šAgentä»»åŠ¡æŸ¥è¯¢ä¼˜åŒ–
-- åŸå§‹æŸ¥è¯¢(æ€§èƒ½è¾ƒå·®)
EXPLAIN ANALYZE
SELECT a.agent_id, a.agent_type, COUNT(t.task_id) as task_count
FROM agents a
LEFT JOIN tasks t ON a.agent_id = t.assigned_agent
WHERE a.status = 'active' 
  AND t.created_at >= CURRENT_DATE - INTERVAL '7 days'
GROUP BY a.agent_id, a.agent_type
ORDER BY task_count DESC;

-- ä¼˜åŒ–åæŸ¥è¯¢
-- 1. æ·»åŠ å¤åˆç´¢å¼•
CREATE INDEX idx_tasks_agent_created ON tasks(assigned_agent, created_at) 
WHERE created_at >= CURRENT_DATE - INTERVAL '30 days';

-- 2. ä½¿ç”¨å­æŸ¥è¯¢ä¼˜åŒ–
EXPLAIN ANALYZE
SELECT a.agent_id, a.agent_type, 
       COALESCE(task_stats.task_count, 0) as task_count
FROM agents a
LEFT JOIN (
    SELECT assigned_agent, COUNT(*) as task_count
    FROM tasks 
    WHERE created_at >= CURRENT_DATE - INTERVAL '7 days'
      AND assigned_agent IS NOT NULL
    GROUP BY assigned_agent
) task_stats ON a.agent_id = task_stats.assigned_agent
WHERE a.status = 'active'
ORDER BY task_count DESC;

-- 3. ç‰©åŒ–è§†å›¾è¿›ä¸€æ­¥ä¼˜åŒ–
CREATE MATERIALIZED VIEW agent_task_summary AS
SELECT 
    a.agent_id,
    a.agent_type,
    a.status,
    COUNT(CASE WHEN t.created_at >= CURRENT_DATE - INTERVAL '7 days' THEN t.task_id END) as recent_task_count,
    COUNT(t.task_id) as total_task_count,
    AVG(te.execution_time) as avg_execution_time
FROM agents a
LEFT JOIN tasks t ON a.agent_id = t.assigned_agent
LEFT JOIN task_executions te ON t.task_id = te.task_id
GROUP BY a.agent_id, a.agent_type, a.status;

-- åˆ›å»ºè‡ªåŠ¨åˆ·æ–°ä»»åŠ¡
CREATE OR REPLACE FUNCTION refresh_agent_summary()
RETURNS void AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY agent_task_summary;
END;
$$ LANGUAGE plpgsql;

-- å®šæ—¶åˆ·æ–°(éœ€è¦pg_cronæ‰©å±•)
SELECT cron.schedule('refresh-agent-summary', '*/15 * * * *', 'SELECT refresh_agent_summary();');
```

### 6.2 å­˜å‚¨ä¼˜åŒ–ç­–ç•¥

#### 6.2.1 è¡¨ç©ºé—´è®¾è®¡

```sql
-- åˆ›å»ºä¸åŒç±»å‹æ•°æ®çš„è¡¨ç©ºé—´
-- 1. é«˜é¢‘è®¿é—®æ•°æ®(SSDå­˜å‚¨)
CREATE TABLESPACE hot_data LOCATION '/data/ssd/postgresql';

-- 2. æ¸©æ•°æ®(æ™®é€šSSDå­˜å‚¨)  
CREATE TABLESPACE warm_data LOCATION '/data/sata_ssd/postgresql';

-- 3. å†·æ•°æ®(æœºæ¢°ç¡¬ç›˜å­˜å‚¨)
CREATE TABLESPACE cold_data LOCATION '/data/hdd/postgresql';

-- 4. ç´¢å¼•ä¸“ç”¨è¡¨ç©ºé—´(é«˜é€ŸSSD)
CREATE TABLESPACE index_data LOCATION '/data/nvme/postgresql';

-- å°†è¡¨åˆ†é…åˆ°åˆé€‚çš„è¡¨ç©ºé—´
-- æ ¸å¿ƒä¸šåŠ¡è¡¨ -> çƒ­æ•°æ®è¡¨ç©ºé—´
ALTER TABLE agents SET TABLESPACE hot_data;
ALTER TABLE tasks SET TABLESPACE hot_data;
ALTER TABLE users SET TABLESPACE hot_data;

-- æ—¥å¿—è¡¨ -> æ¸©æ•°æ®è¡¨ç©ºé—´
ALTER TABLE task_executions SET TABLESPACE warm_data;
ALTER TABLE audit_log SET TABLESPACE warm_data;

-- å†å²æ•°æ® -> å†·æ•°æ®è¡¨ç©ºé—´  
ALTER TABLE task_executions_archive SET TABLESPACE cold_data;

-- ç´¢å¼• -> ç´¢å¼•è¡¨ç©ºé—´
ALTER INDEX idx_tasks_status SET TABLESPACE index_data;
ALTER INDEX idx_agents_type SET TABLESPACE index_data;
```

#### 6.2.2 æ•°æ®å½’æ¡£ç­–ç•¥

```sql
-- åˆ›å»ºæ•°æ®å½’æ¡£å‡½æ•°
CREATE OR REPLACE FUNCTION archive_old_data(
    table_name TEXT,
    archive_table_name TEXT,
    date_column TEXT,
    retention_days INTEGER
) RETURNS INTEGER AS $$
DECLARE
    cutoff_date TIMESTAMP;
    archived_count INTEGER;
BEGIN
    cutoff_date := CURRENT_TIMESTAMP - (retention_days || ' days')::INTERVAL;
    
    -- å°†æ—§æ•°æ®ç§»åŠ¨åˆ°å½’æ¡£è¡¨
    EXECUTE format('
        WITH moved_rows AS (
            DELETE FROM %I 
            WHERE %I < %L
            RETURNING *
        )
        INSERT INTO %I SELECT * FROM moved_rows',
        table_name, date_column, cutoff_date, archive_table_name);
    
    GET DIAGNOSTICS archived_count = ROW_COUNT;
    
    -- è®°å½•å½’æ¡£æ—¥å¿—
    INSERT INTO archive_log (table_name, archived_count, archive_date)
    VALUES (table_name, archived_count, CURRENT_TIMESTAMP);
    
    RETURN archived_count;
END;
$$ LANGUAGE plpgsql;

-- åˆ›å»ºå½’æ¡£æ—¥å¿—è¡¨
CREATE TABLE archive_log (
    log_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    table_name VARCHAR(50) NOT NULL,
    archived_count INTEGER NOT NULL,
    archive_date TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- å®šæœŸæ‰§è¡Œå½’æ¡£ä»»åŠ¡
SELECT cron.schedule('archive-task-executions', '0 2 * * *', 
    'SELECT archive_old_data(''task_executions'', ''task_executions_archive'', ''created_at'', 90);');
```

---

## ğŸ”„ æ•°æ®åº“è¿ç»´è§„èŒƒ

### 7.1 å¤‡ä»½æ¢å¤ç­–ç•¥

#### 7.1.1 å¤‡ä»½ç­–ç•¥è®¾è®¡

```yaml
# å¤‡ä»½ç­–ç•¥é…ç½®
backup_strategy:
  # å…¨é‡å¤‡ä»½
  full_backup:
    frequency: weekly
    schedule: "0 2 * * 0"  # æ¯å‘¨æ—¥å‡Œæ™¨2ç‚¹
    retention: 4_weeks
    compression: gzip
    encryption: enabled
    
  # å¢é‡å¤‡ä»½
  incremental_backup:
    frequency: daily
    schedule: "0 3 * * 1-6"  # å‘¨ä¸€åˆ°å‘¨å…­å‡Œæ™¨3ç‚¹
    retention: 7_days
    
  # WALå¤‡ä»½
  wal_backup:
    continuous: true
    archive_command: 'test ! -f /backup/wal/%f && cp %p /backup/wal/%f'
    archive_timeout: 300
    
  # å¤‡ä»½éªŒè¯
  backup_verification:
    schedule: "0 4 * * *"  # æ¯æ—¥å‡Œæ™¨4ç‚¹éªŒè¯
    test_restore: weekly
```

#### 7.1.2 å¤‡ä»½è„šæœ¬ç¤ºä¾‹

```bash
#!/bin/bash
# æ•°æ®åº“å¤‡ä»½è„šæœ¬

set -euo pipefail

# é…ç½®å‚æ•°
DB_HOST="localhost"
DB_PORT="5432"
DB_NAME="digital_employee"
DB_USER="backup_user"
BACKUP_DIR="/backup/postgresql"
LOG_FILE="/var/log/postgresql_backup.log"
RETENTION_DAYS=30

# æ—¥å¿—å‡½æ•°
log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a "$LOG_FILE"
}

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p "$BACKUP_DIR/$(date +%Y/%m)"

# å¤‡ä»½æ–‡ä»¶å
BACKUP_FILE="$BACKUP_DIR/$(date +%Y/%m)/db_backup_$(date +%Y%m%d_%H%M%S).sql.gz"

# æ‰§è¡Œå¤‡ä»½
log "å¼€å§‹æ•°æ®åº“å¤‡ä»½: $BACKUP_FILE"

if pg_dump -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" \
   --verbose --format=custom --compress=9 \
   | gzip > "$BACKUP_FILE"; then
    
    log "å¤‡ä»½æˆåŠŸå®Œæˆ: $(du -h "$BACKUP_FILE" | cut -f1)"
    
    # éªŒè¯å¤‡ä»½æ–‡ä»¶å®Œæ•´æ€§
    if gzip -t "$BACKUP_FILE"; then
        log "å¤‡ä»½æ–‡ä»¶å®Œæ•´æ€§éªŒè¯é€šè¿‡"
    else
        log "é”™è¯¯: å¤‡ä»½æ–‡ä»¶æŸå"
        exit 1
    fi
    
    # æ¸…ç†æ—§å¤‡ä»½
    find "$BACKUP_DIR" -name "*.sql.gz" -mtime +$RETENTION_DAYS -delete
    log "å·²æ¸…ç† $RETENTION_DAYS å¤©å‰çš„æ—§å¤‡ä»½æ–‡ä»¶"
    
else
    log "é”™è¯¯: å¤‡ä»½å¤±è´¥"
    exit 1
fi

log "å¤‡ä»½æµç¨‹å®Œæˆ"
```

### 7.2 ç›‘æ§å‘Šè­¦è®¾è®¡

#### 7.2.1 æ•°æ®åº“ç›‘æ§æŒ‡æ ‡

```yaml
# æ•°æ®åº“ç›‘æ§é…ç½®
monitoring_metrics:
  # è¿æ¥ç›‘æ§
  connections:
    - metric: active_connections
      threshold: 150
      severity: warning
    - metric: max_connections_usage
      threshold: 80%
      severity: critical
      
  # æ€§èƒ½ç›‘æ§
  performance:
    - metric: avg_query_time
      threshold: 2000ms
      severity: warning
    - metric: slow_query_count
      threshold: 10/min
      severity: warning
    - metric: lock_wait_time
      threshold: 5000ms
      severity: critical
      
  # å­˜å‚¨ç›‘æ§
  storage:
    - metric: disk_usage
      threshold: 85%
      severity: warning
    - metric: disk_usage
      threshold: 95%
      severity: critical
    - metric: table_bloat
      threshold: 30%
      severity: warning
      
  # å¤åˆ¶ç›‘æ§(å¦‚æœä½¿ç”¨ä¸»ä»)
  replication:
    - metric: replication_lag
      threshold: 60s
      severity: warning
    - metric: replication_lag  
      threshold: 300s
      severity: critical
```

#### 7.2.2 ç›‘æ§è„šæœ¬ç¤ºä¾‹

```python
#!/usr/bin/env python3
"""PostgreSQLç›‘æ§è„šæœ¬"""

import asyncio
import asyncpg
import logging
from typing import Dict, Any
import json
from datetime import datetime

class PostgreSQLMonitor:
    """PostgreSQLç›‘æ§å™¨"""
    
    def __init__(self, db_url: str):
        self.db_url = db_url
        self.logger = logging.getLogger(__name__)
        
    async def collect_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†æ•°æ®åº“æŒ‡æ ‡"""
        conn = await asyncpg.connect(self.db_url)
        
        try:
            metrics = {}
            
            # è¿æ¥æ•°ç›‘æ§
            metrics['connections'] = await self._check_connections(conn)
            
            # æ€§èƒ½ç›‘æ§
            metrics['performance'] = await self._check_performance(conn)
            
            # å­˜å‚¨ç›‘æ§
            metrics['storage'] = await self._check_storage(conn)
            
            # å¤åˆ¶ç›‘æ§
            metrics['replication'] = await self._check_replication(conn)
            
            return metrics
            
        finally:
            await conn.close()
    
    async def _check_connections(self, conn) -> Dict[str, Any]:
        """æ£€æŸ¥è¿æ¥çŠ¶æ€"""
        query = """
        SELECT 
            count(*) as total_connections,
            count(*) FILTER (WHERE state = 'active') as active_connections,
            count(*) FILTER (WHERE state = 'idle') as idle_connections,
            max_connections
        FROM pg_stat_activity, 
        (SELECT setting::int as max_connections FROM pg_settings WHERE name = 'max_connections') mc
        GROUP BY max_connections
        """
        
        result = await conn.fetchrow(query)
        return {
            'total_connections': result['total_connections'],
            'active_connections': result['active_connections'], 
            'idle_connections': result['idle_connections'],
            'max_connections': result['max_connections'],
            'connection_usage_percent': (result['total_connections'] / result['max_connections']) * 100
        }
    
    async def _check_performance(self, conn) -> Dict[str, Any]:
        """æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡"""
        query = """
        SELECT 
            round(avg(mean_time)::numeric, 2) as avg_query_time,
            count(*) FILTER (WHERE mean_time > 1000) as slow_queries,
            sum(calls) as total_queries,
            round(avg(shared_blks_hit::numeric / nullif(shared_blks_hit + shared_blks_read, 0)) * 100, 2) as cache_hit_ratio
        FROM pg_stat_statements
        WHERE calls > 0
        """
        
        result = await conn.fetchrow(query)
        if result:
            return {
                'avg_query_time_ms': float(result['avg_query_time'] or 0),
                'slow_queries': result['slow_queries'],
                'total_queries': result['total_queries'],
                'cache_hit_ratio': float(result['cache_hit_ratio'] or 0)
            }
        return {}
    
    async def _check_storage(self, conn) -> Dict[str, Any]:
        """æ£€æŸ¥å­˜å‚¨ä½¿ç”¨æƒ…å†µ"""
        query = """
        SELECT 
            pg_database_size(current_database()) as db_size_bytes,
            pg_size_pretty(pg_database_size(current_database())) as db_size_pretty
        """
        
        result = await conn.fetchrow(query)
        
        # æ£€æŸ¥è¡¨ç©ºé—´ä½¿ç”¨æƒ…å†µ
        tablespace_query = """
        SELECT 
            spcname,
            pg_size_pretty(pg_tablespace_size(spcname)) as size
        FROM pg_tablespace
        """
        
        tablespaces = await conn.fetch(tablespace_query)
        
        return {
            'database_size_bytes': result['db_size_bytes'],
            'database_size_pretty': result['db_size_pretty'],
            'tablespaces': [dict(ts) for ts in tablespaces]
        }
    
    async def _check_replication(self, conn) -> Dict[str, Any]:
        """æ£€æŸ¥å¤åˆ¶çŠ¶æ€"""
        query = """
        SELECT 
            application_name,
            client_addr,
            state,
            pg_wal_lsn_diff(pg_current_wal_lsn(), flush_lsn) as replication_lag_bytes
        FROM pg_stat_replication
        """
        
        result = await conn.fetch(query)
        return {
            'replicas': [dict(row) for row in result],
            'replica_count': len(result)
        }

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    monitor = PostgreSQLMonitor("postgresql://user:pass@localhost/db")
    metrics = await monitor.collect_metrics()
    
    print(json.dumps(metrics, indent=2, default=str))
    
    # æ£€æŸ¥å‘Šè­¦æ¡ä»¶
    if metrics['connections']['connection_usage_percent'] > 80:
        print("å‘Šè­¦: æ•°æ®åº“è¿æ¥ä½¿ç”¨ç‡è¿‡é«˜")
        
    if metrics['performance']['avg_query_time_ms'] > 2000:
        print("å‘Šè­¦: å¹³å‡æŸ¥è¯¢æ—¶é—´è¿‡é•¿")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## ğŸ“‹ æ•°æ®åº“å˜æ›´ç®¡ç†

### 8.1 æ•°æ®åº“ç‰ˆæœ¬æ§åˆ¶

#### 8.1.1 è¿ç§»è„šæœ¬è§„èŒƒ

```sql
-- è¿ç§»è„šæœ¬æ¨¡æ¿: V001__Create_agents_table.sql
-- ç‰ˆæœ¬: 1.0.0
-- æè¿°: åˆ›å»ºAgentåŸºç¡€è¡¨
-- ä½œè€…: Database Team
-- æ—¥æœŸ: 2024-01-24

-- å‘å‰è¿ç§»
BEGIN;

-- åˆ›å»ºagentsè¡¨
CREATE TABLE IF NOT EXISTS agents (
    agent_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    agent_type VARCHAR(50) NOT NULL,
    version VARCHAR(20) NOT NULL DEFAULT '1.0.0',
    config JSONB NOT NULL DEFAULT '{}',
    status VARCHAR(20) NOT NULL DEFAULT 'inactive',
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    created_by UUID,
    
    -- çº¦æŸ
    CONSTRAINT agents_status_check CHECK (status IN ('active', 'inactive', 'suspended', 'deleted')),
    CONSTRAINT agents_config_check CHECK (jsonb_typeof(config) = 'object')
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX IF NOT EXISTS idx_agents_type ON agents(agent_type);
CREATE INDEX IF NOT EXISTS idx_agents_status ON agents(status);
CREATE INDEX IF NOT EXISTS idx_agents_created_at ON agents(created_at);

-- åˆ›å»ºè§¦å‘å™¨
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER agents_updated_at 
    BEFORE UPDATE ON agents 
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

-- æ’å…¥åˆå§‹æ•°æ®
INSERT INTO agents (agent_type, config) VALUES 
('system_admin', '{"max_concurrent_tasks": 100, "timeout_seconds": 3600}'),
('default_agent', '{"max_concurrent_tasks": 10, "timeout_seconds": 300}')
ON CONFLICT DO NOTHING;

-- è®°å½•è¿ç§»ç‰ˆæœ¬
INSERT INTO schema_migrations (version, applied_at) VALUES ('V001', CURRENT_TIMESTAMP);

COMMIT;
```

#### 8.1.2 å›æ»šè„šæœ¬è§„èŒƒ

```sql
-- å›æ»šè„šæœ¬: V001__Create_agents_table_rollback.sql
-- ç‰ˆæœ¬: 1.0.0 
-- æè¿°: å›æ»šAgentåŸºç¡€è¡¨åˆ›å»º
-- ä½œè€…: Database Team
-- æ—¥æœŸ: 2024-01-24

BEGIN;

-- åˆ é™¤è§¦å‘å™¨
DROP TRIGGER IF EXISTS agents_updated_at ON agents;
DROP FUNCTION IF EXISTS update_updated_at_column();

-- åˆ é™¤ç´¢å¼•
DROP INDEX IF EXISTS idx_agents_type;
DROP INDEX IF EXISTS idx_agents_status;
DROP INDEX IF EXISTS idx_agents_created_at;

-- åˆ é™¤è¡¨
DROP TABLE IF EXISTS agents;

-- åˆ é™¤è¿ç§»è®°å½•
DELETE FROM schema_migrations WHERE version = 'V001';

COMMIT;
```

### 8.2 æ•°æ®åº“CI/CDé›†æˆ

#### 8.2.1 æ•°æ®åº“éƒ¨ç½²æµæ°´çº¿

```yaml
# .gitlab-ci.yml æ•°æ®åº“éƒ¨ç½²éƒ¨åˆ†
stages:
  - validate
  - test
  - deploy

variables:
  FLYWAY_URL: "jdbc:postgresql://localhost:5432/digital_employee"
  FLYWAY_USER: "flyway_user"
  FLYWAY_SCHEMAS: "public"

# æ•°æ®åº“è„šæœ¬éªŒè¯
db_validate:
  stage: validate
  image: flyway/flyway:latest
  script:
    - flyway validate -url=$FLYWAY_URL -user=$FLYWAY_USER -password=$FLYWAY_PASSWORD
  rules:
    - changes:
      - database/migrations/**/*

# æ•°æ®åº“æµ‹è¯•
db_test:
  stage: test
  services:
    - postgres:15
  variables:
    POSTGRES_DB: test_db
    POSTGRES_USER: test_user
    POSTGRES_PASSWORD: test_password
  script:
    - flyway migrate -url=jdbc:postgresql://postgres:5432/test_db -user=test_user -password=test_password
    - python -m pytest tests/database/
  rules:
    - changes:
      - database/migrations/**/*

# éƒ¨ç½²åˆ°å¼€å‘ç¯å¢ƒ
deploy_dev:
  stage: deploy
  script:
    - flyway info -url=$DEV_DB_URL -user=$DEV_DB_USER -password=$DEV_DB_PASSWORD
    - flyway migrate -url=$DEV_DB_URL -user=$DEV_DB_USER -password=$DEV_DB_PASSWORD
  environment:
    name: development
  rules:
    - if: $CI_COMMIT_BRANCH == "develop"

# éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
deploy_prod:
  stage: deploy
  script:
    - flyway info -url=$PROD_DB_URL -user=$PROD_DB_USER -password=$PROD_DB_PASSWORD
    - flyway migrate -url=$PROD_DB_URL -user=$PROD_DB_USER -password=$PROD_DB_PASSWORD
  environment:
    name: production
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
      when: manual
  before_script:
    - echo "å³å°†åœ¨ç”Ÿäº§ç¯å¢ƒæ‰§è¡Œæ•°æ®åº“è¿ç§»ï¼Œè¯·ç¡®è®¤æ‰€æœ‰å˜æ›´å·²ç»è¿‡å……åˆ†æµ‹è¯•"
```

---

## ğŸ“š é™„å½•

### A.1 æ•°æ®åº“æ£€æŸ¥æ¸…å•

#### A.1.1 è®¾è®¡è¯„å®¡æ£€æŸ¥æ¸…å•

- [ ] **è¡¨ç»“æ„è®¾è®¡**
  - [ ] ä¸»é”®è®¾è®¡åˆç†(æ¨èUUID)
  - [ ] å¤–é”®å…³ç³»æ­£ç¡®å®šä¹‰
  - [ ] å­—æ®µç±»å‹é€‰æ‹©æ°å½“ 
  - [ ] å¿…è¦çš„çº¦æŸæ¡ä»¶å·²æ·»åŠ 
  - [ ] ç´¢å¼•è®¾è®¡ç¬¦åˆæŸ¥è¯¢éœ€æ±‚

- [ ] **æ€§èƒ½è®¾è®¡**
  - [ ] æŸ¥è¯¢è·¯å¾„å·²ä¼˜åŒ–
  - [ ] å¿…è¦çš„ç´¢å¼•å·²åˆ›å»º
  - [ ] åˆ†åŒºç­–ç•¥å·²è€ƒè™‘
  - [ ] ç¼“å­˜ç­–ç•¥å·²è®¾è®¡

- [ ] **å®‰å…¨è®¾è®¡**
  - [ ] æ•æ„Ÿæ•°æ®å·²åŠ å¯†
  - [ ] è®¿é—®æƒé™å·²é™åˆ¶
  - [ ] å®¡è®¡æ—¥å¿—å·²å¯ç”¨
  - [ ] è¡Œçº§å®‰å…¨å·²è€ƒè™‘

- [ ] **è¿ç»´è®¾è®¡**
  - [ ] å¤‡ä»½ç­–ç•¥å·²åˆ¶å®š
  - [ ] ç›‘æ§æŒ‡æ ‡å·²å®šä¹‰
  - [ ] å½’æ¡£ç­–ç•¥å·²è€ƒè™‘
  - [ ] ç¾éš¾æ¢å¤å·²è§„åˆ’

#### A.1.2 éƒ¨ç½²å‰æ£€æŸ¥æ¸…å•

- [ ] **ç¯å¢ƒå‡†å¤‡**
  - [ ] æ•°æ®åº“æœåŠ¡å™¨é…ç½®æ­£ç¡®
  - [ ] ç½‘ç»œè¿æ¥å·²æµ‹è¯•
  - [ ] é˜²ç«å¢™è§„åˆ™å·²é…ç½®
  - [ ] SSLè¯ä¹¦å·²å®‰è£…

- [ ] **æ•°æ®è¿ç§»**
  - [ ] è¿ç§»è„šæœ¬å·²æµ‹è¯•
  - [ ] å›æ»šè„šæœ¬å·²å‡†å¤‡
  - [ ] æ•°æ®å¤‡ä»½å·²å®Œæˆ
  - [ ] è¿ç§»æ—¶é—´çª—å£å·²å®‰æ’

- [ ] **ç›‘æ§å‘Šè­¦**
  - [ ] ç›‘æ§ç³»ç»Ÿå·²é…ç½®
  - [ ] å‘Šè­¦è§„åˆ™å·²è®¾ç½®
  - [ ] é€šçŸ¥æ¸ é“å·²æµ‹è¯•
  - [ ] è¿ç»´æ‰‹å†Œå·²å‡†å¤‡

### A.2 å¸¸ç”¨SQLå·¥å…·å‡½æ•°

```sql
-- å·¥å…·å‡½æ•°é›†åˆ

-- 1. ç”Ÿæˆéšæœºå­—ç¬¦ä¸²
CREATE OR REPLACE FUNCTION generate_random_string(length INTEGER)
RETURNS TEXT AS $$
BEGIN
    RETURN array_to_string(
        ARRAY(
            SELECT chr((97 + round(random() * 25))::integer)
            FROM generate_series(1, length)
        ), 
        ''
    );
END;
$$ LANGUAGE plpgsql;

-- 2. JSONå­—æ®µåˆå¹¶
CREATE OR REPLACE FUNCTION jsonb_merge(json1 JSONB, json2 JSONB)
RETURNS JSONB AS $$
BEGIN
    RETURN json1 || json2;
END;
$$ LANGUAGE plpgsql;

-- 3. åˆ†é¡µæŸ¥è¯¢è¾…åŠ©å‡½æ•°
CREATE OR REPLACE FUNCTION paginate_query(
    base_query TEXT,
    page_size INTEGER DEFAULT 20,
    page_number INTEGER DEFAULT 1
) RETURNS TEXT AS $$
BEGIN
    RETURN base_query || 
           ' LIMIT ' || page_size || 
           ' OFFSET ' || ((page_number - 1) * page_size);
END;
$$ LANGUAGE plpgsql;

-- 4. æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯å‡½æ•°
CREATE OR REPLACE FUNCTION get_table_stats(table_name TEXT)
RETURNS TABLE(
    total_rows BIGINT,
    table_size TEXT,
    index_size TEXT,
    total_size TEXT
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        (SELECT reltuples::BIGINT FROM pg_class WHERE relname = table_name),
        pg_size_pretty(pg_total_relation_size(table_name::regclass)),
        pg_size_pretty(pg_indexes_size(table_name::regclass)),
        pg_size_pretty(pg_total_relation_size(table_name::regclass) + pg_indexes_size(table_name::regclass));
END;
$$ LANGUAGE plpgsql;
```

### A.3 å‚è€ƒèµ„æ–™

- [PostgreSQLå®˜æ–¹æ–‡æ¡£](https://www.postgresql.org/docs/)
- [æ•°æ®åº“è®¾è®¡æœ€ä½³å®è·µ](https://wiki.postgresql.org/wiki/Don%27t_Do_This)
- [PostgreSQLæ€§èƒ½è°ƒä¼˜æŒ‡å—](https://wiki.postgresql.org/wiki/Performance_Optimization)
- [æ•°æ®åº“å®‰å…¨æœ€ä½³å®è·µ](https://www.postgresql.org/docs/current/security.html)

---

**æ–‡æ¡£çŠ¶æ€**: æ­£å¼å‘å¸ƒ  
**æœ€åæ›´æ–°**: 2024-01-24  
**ä¸‹æ¬¡è¯„å®¡**: 2024-04-24  
**æ‰¹å‡†äºº**: æ•°æ®æ¶æ„å§”å‘˜ä¼š