# 数字员工系统数据库设计规范
## Digital Employee System Database Design Standards v1.0

### 📋 文档信息
- **文档版本**: v1.0
- **创建日期**: 2024-01-24
- **适用范围**: 数字员工系统所有数据库设计工作
- **维护部门**: 数据架构组

---

## 🏗️ 数据库架构设计

### 1.1 整体数据架构

#### 1.1.1 多层数据架构

```mermaid
graph TB
    subgraph "应用层 Application Layer"
        A1[Agent管理服务]
        A2[任务调度服务]
        A3[Prompt管理服务]
        A4[监控治理服务]
    end
    
    subgraph "数据访问层 Data Access Layer"
        B1[ORM层 SQLAlchemy]
        B2[缓存层 Redis]
        B3[搜索层 Elasticsearch]
        B4[时序数据 InfluxDB]
    end
    
    subgraph "数据存储层 Data Storage Layer"
        C1[主数据库 PostgreSQL]
        C2[文档数据库 MongoDB]
        C3[图数据库 Neo4j]
        C4[对象存储 MinIO]
    end
    
    subgraph "数据治理层 Data Governance Layer"
        D1[数据质量监控]
        D2[数据血缘追踪]
        D3[数据安全审计]
        D4[数据备份恢复]
    end
    
    A1 --> B1
    A2 --> B2
    A3 --> B3
    A4 --> B4
    
    B1 --> C1
    B2 --> C1
    B3 --> C2
    B4 --> C3
    
    C1 --> D1
    C2 --> D2
    C3 --> D3
    C4 --> D4
```

#### 1.1.2 数据库选型原则

| 数据类型 | 数据库类型 | 选型标准 | 使用场景 |
|----------|------------|----------|----------|
| **结构化数据** | 关系型数据库 | ACID特性，强一致性 | Agent配置、用户信息、任务记录 |
| **半结构化数据** | 文档数据库 | 灵活模式，JSON支持 | Prompt模板、配置文件 |
| **时序数据** | 时序数据库 | 高写入性能，压缩存储 | 监控指标、日志数据 |
| **图关系数据** | 图数据库 | 关系查询，路径分析 | Agent依赖关系 |
| **缓存数据** | 内存数据库 | 高速读写，持久化 | 会话状态、临时数据 |
| **文件数据** | 对象存储 | 扩展性强，成本低 | 日志文件、备份数据 |

### 1.2 数据库技术栈

#### 1.2.1 主数据库 - PostgreSQL 15+

```yaml
版本要求: PostgreSQL 15.0+
推荐配置:
  max_connections: 200
  shared_buffers: 256MB
  effective_cache_size: 1GB
  work_mem: 16MB
  maintenance_work_mem: 256MB
  
扩展组件:
  - uuid-ossp: UUID生成支持
  - pg_stat_statements: SQL性能分析  
  - pg_trgm: 全文搜索支持
  - jsonb_plpython3u: JSON数据处理
  - pg_partman: 表分区管理
  - timescaledb: 时序数据扩展(可选)

连接池配置:
  工具: PgBouncer
  pool_mode: transaction
  max_client_conn: 1000
  default_pool_size: 100
```

#### 1.2.2 缓存数据库 - Redis 7.0+

```yaml
版本要求: Redis 7.0+
部署模式: Redis Cluster (3主3从)
内存配置:
  maxmemory: 2GB
  maxmemory-policy: allkeys-lru
  
持久化配置:
  save: "900 1 300 10 60 10000"
  appendonly: yes
  appendfsync: everysec
  
集群配置:
  cluster-enabled: yes
  cluster-config-file: nodes.conf
  cluster-node-timeout: 5000
```

#### 1.2.3 文档数据库 - MongoDB 6.0+

```yaml
版本要求: MongoDB 6.0+
部署模式: 副本集 (1主2从)
存储引擎: WiredTiger
配置参数:
  journal: true
  directoryPerDB: true
  
索引策略:
  - 复合索引优先
  - 稀疏索引用于可选字段
  - TTL索引用于自动过期
```

---

## 📊 数据模型设计标准

### 2.1 概念数据模型

#### 2.1.1 核心业务实体关系

```mermaid
erDiagram
    USER ||--o{ AGENT : manages
    AGENT ||--o{ TASK : executes
    AGENT ||--o{ CAPABILITY : has
    AGENT ||--o{ PROMPT_TEMPLATE : uses
    
    TASK ||--o{ TASK_EXECUTION : generates
    TASK_EXECUTION ||--o{ EXECUTION_LOG : produces
    
    PROMPT_TEMPLATE ||--o{ PROMPT_VERSION : versioned_by
    PROMPT_VERSION ||--o{ PROMPT_USAGE : used_in
    
    AGENT ||--o{ AGENT_METRIC : monitored_by
    TASK ||--o{ TASK_METRIC : measured_by
    
    USER {
        uuid user_id PK
        string username UK
        string email UK
        string password_hash
        jsonb roles
        timestamp created_at
        timestamp updated_at
        boolean is_active
    }
    
    AGENT {
        uuid agent_id PK
        string agent_type
        string version
        jsonb config
        enum status
        timestamp created_at
        timestamp updated_at
        uuid created_by FK
    }
    
    TASK {
        uuid task_id PK
        string task_type
        enum priority
        jsonb data
        enum status
        timestamp deadline
        timestamp created_at
        timestamp updated_at
        uuid assigned_agent FK
    }
    
    PROMPT_TEMPLATE {
        uuid template_id PK
        string template_name UK
        string agent_type
        string task_type
        text description
        timestamp created_at
        timestamp updated_at
        uuid created_by FK
    }
```

#### 2.1.2 领域模型约束

| 实体类型 | 主键约束 | 唯一性约束 | 外键约束 | 检查约束 |
|----------|----------|------------|----------|----------|
| **用户(User)** | UUID主键 | username, email唯一 | 无 | 邮箱格式验证 |
| **Agent** | UUID主键 | (agent_type, version)唯一 | created_by引用User | status枚举检查 |
| **任务(Task)** | UUID主键 | 无 | assigned_agent引用Agent | priority枚举检查 |
| **Prompt模板** | UUID主键 | template_name唯一 | created_by引用User | agent_type非空 |

### 2.2 逻辑数据模型

#### 2.2.1 分层模型设计

```mermaid
graph TB
    subgraph "业务层 Business Layer"
        B1[用户管理域]
        B2[Agent管理域]
        B3[任务调度域]
        B4[Prompt管理域]
        B5[监控治理域]
    end
    
    subgraph "数据层 Data Layer"
        D1[用户数据表]
        D2[Agent数据表]
        D3[任务数据表]
        D4[Prompt数据表]
        D5[监控数据表]
    end
    
    subgraph "存储层 Storage Layer"
        S1[主数据库分区]
        S2[缓存数据分片]
        S3[日志数据分区]
    end
    
    B1 --> D1
    B2 --> D2
    B3 --> D3
    B4 --> D4
    B5 --> D5
    
    D1 --> S1
    D2 --> S1
    D3 --> S1
    D4 --> S2
    D5 --> S3
```

#### 2.2.2 数据域定义

| 数据域 | 业务含义 | 表数量 | 预估数据量 | 增长率 |
|--------|----------|--------|------------|--------|
| **用户管理域** | 用户信息、角色权限 | 5个表 | 10万条/年 | 20%/年 |
| **Agent管理域** | Agent配置、状态管理 | 8个表 | 1万条/年 | 50%/年 |
| **任务调度域** | 任务执行、调度记录 | 6个表 | 100万条/月 | 100%/年 |
| **Prompt管理域** | Prompt模板、版本管理 | 4个表 | 1万条/年 | 30%/年 |
| **监控治理域** | 监控指标、日志数据 | 10个表 | 1000万条/月 | 200%/年 |

---

## 🗃️ 数据库设计原则

### 3.1 规范化设计原则

#### 3.1.1 范式要求

| 范式等级 | 适用场景 | 设计要求 | 例外情况 |
|----------|----------|----------|----------|
| **第一范式(1NF)** | 所有表 | 原子性数据，无重复列 | 无例外 |
| **第二范式(2NF)** | 业务主表 | 消除部分依赖 | 冗余字段优化除外 |
| **第三范式(3NF)** | 核心业务表 | 消除传递依赖 | 性能考虑可适当冗余 |
| **BCNF范式** | 关键主表 | 消除主属性依赖 | 复杂查询场景除外 |

#### 3.1.2 反规范化策略

```sql
-- 示例：Agent执行统计表(反规范化设计)
CREATE TABLE agent_performance_summary (
    agent_id UUID PRIMARY KEY,
    agent_type VARCHAR(50) NOT NULL,
    
    -- 冗余基础信息(减少JOIN)
    agent_name VARCHAR(100) NOT NULL,
    created_at TIMESTAMP NOT NULL,
    
    -- 汇总统计信息
    total_tasks_executed INTEGER DEFAULT 0,
    successful_tasks INTEGER DEFAULT 0,
    failed_tasks INTEGER DEFAULT 0,
    average_execution_time DECIMAL(10,3) DEFAULT 0,
    
    -- 最近更新时间
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    CONSTRAINT fk_agent_id FOREIGN KEY (agent_id) REFERENCES agents(agent_id)
);

-- 更新触发器维护统计数据
CREATE OR REPLACE FUNCTION update_agent_performance()
RETURNS TRIGGER AS $$
BEGIN
    INSERT INTO agent_performance_summary (agent_id, agent_type, agent_name)
    SELECT NEW.assigned_agent, a.agent_type, a.config->>'name'
    FROM agents a WHERE a.agent_id = NEW.assigned_agent
    ON CONFLICT (agent_id) DO UPDATE SET
        total_tasks_executed = agent_performance_summary.total_tasks_executed + 1,
        successful_tasks = CASE 
            WHEN NEW.status = 'completed' THEN agent_performance_summary.successful_tasks + 1 
            ELSE agent_performance_summary.successful_tasks 
        END,
        failed_tasks = CASE 
            WHEN NEW.status = 'failed' THEN agent_performance_summary.failed_tasks + 1 
            ELSE agent_performance_summary.failed_tasks 
        END,
        last_updated = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;
```

### 3.2 数据类型选择标准

#### 3.2.1 基础数据类型规范

| 业务类型 | PostgreSQL类型 | 长度限制 | 使用场景 | 注意事项 |
|----------|----------------|----------|----------|----------|
| **主键ID** | UUID | 固定36字符 | 所有主键 | 使用uuid_generate_v4() |
| **业务ID** | VARCHAR(50) | 最大50字符 | 业务标识符 | 必须建立唯一索引 |
| **名称类** | VARCHAR(100) | 最大100字符 | 名称、标题 | 考虑国际化长度 |
| **描述类** | TEXT | 无限制 | 长文本描述 | 避免过度使用 |
| **配置类** | JSONB | 无限制 | 灵活配置 | 建立GIN索引 |
| **状态类** | VARCHAR(20) | 最大20字符 | 枚举状态 | 使用CHECK约束 |
| **时间类** | TIMESTAMP | 固定8字节 | 时间戳 | 统一使用UTC时区 |
| **数值类** | DECIMAL(10,2) | 根据精度 | 金额、比例 | 避免浮点数 |
| **布尔类** | BOOLEAN | 1字节 | 是否标识 | 明确默认值 |

#### 3.2.2 JSON数据类型规范

```sql
-- JSONB字段设计示例
CREATE TABLE agents (
    agent_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    agent_type VARCHAR(50) NOT NULL,
    
    -- 结构化配置(推荐JSONB)
    config JSONB NOT NULL DEFAULT '{}',
    
    -- 配置JSON结构约束
    CONSTRAINT config_structure_check CHECK (
        config ? 'max_concurrent_tasks' AND
        config ? 'timeout_seconds' AND
        jsonb_typeof(config->'max_concurrent_tasks') = 'number' AND
        jsonb_typeof(config->'timeout_seconds') = 'number'
    ),
    
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- JSONB字段索引
CREATE INDEX idx_agents_config_gin ON agents USING GIN (config);
CREATE INDEX idx_agents_config_max_tasks ON agents USING BTREE ((config->>'max_concurrent_tasks'));
```

### 3.3 索引设计策略

#### 3.3.1 索引类型选择

| 索引类型 | 适用场景 | 创建语法 | 性能特点 |
|----------|----------|----------|----------|
| **B-Tree索引** | 等值查询、范围查询 | `CREATE INDEX idx_name ON table(column)` | 平衡查询和更新性能 |
| **Hash索引** | 等值查询(仅限) | `CREATE INDEX idx_name ON table USING HASH(column)` | 等值查询最快 |
| **GIN索引** | JSONB、全文搜索 | `CREATE INDEX idx_name ON table USING GIN(column)` | 复杂查询支持好 |
| **GiST索引** | 几何数据、范围类型 | `CREATE INDEX idx_name ON table USING GIST(column)` | 特殊数据类型 |
| **部分索引** | 条件过滤数据 | `CREATE INDEX idx_name ON table(column) WHERE condition` | 节省存储空间 |
| **复合索引** | 多字段查询 | `CREATE INDEX idx_name ON table(col1, col2, col3)` | 注意字段顺序 |

#### 3.3.2 索引设计规则

```sql
-- 1. 主键自动创建唯一索引
CREATE TABLE users (
    user_id UUID PRIMARY KEY,  -- 自动创建唯一索引
    username VARCHAR(50) UNIQUE,  -- 自动创建唯一索引
    email VARCHAR(100) UNIQUE     -- 自动创建唯一索引
);

-- 2. 外键字段建议创建索引
CREATE INDEX idx_tasks_assigned_agent ON tasks(assigned_agent);
CREATE INDEX idx_task_executions_task_id ON task_executions(task_id);

-- 3. 频繁查询字段创建索引
CREATE INDEX idx_tasks_status ON tasks(status);
CREATE INDEX idx_tasks_created_at ON tasks(created_at);
CREATE INDEX idx_agents_agent_type ON agents(agent_type);

-- 4. 复合索引考虑字段顺序(最左前缀原则)
CREATE INDEX idx_tasks_composite ON tasks(status, priority, created_at);

-- 5. 部分索引优化特定查询
CREATE INDEX idx_tasks_active ON tasks(assigned_agent) WHERE status IN ('pending', 'running');

-- 6. JSONB字段的索引策略
CREATE INDEX idx_agents_config_gin ON agents USING GIN (config);
CREATE INDEX idx_agents_capabilities ON agents USING GIN ((config->'capabilities'));
```

---

## 🔧 数据库设计模式

### 4.1 分区表设计

#### 4.1.1 分区策略选择

| 分区类型 | 适用场景 | 分区键选择 | 维护成本 |
|----------|----------|------------|----------|
| **时间分区** | 时序数据、日志表 | 时间字段 | 自动化程度高 |
| **哈希分区** | 均匀分布数据 | 哈希字段 | 维护简单 |
| **范围分区** | 有序数据 | 数值/时间范围 | 需要维护边界 |
| **列表分区** | 枚举类型数据 | 离散值 | 固定分区数量 |

#### 4.1.2 分区表实现示例

```sql
-- 任务执行日志按月分区
CREATE TABLE task_executions (
    execution_id UUID DEFAULT uuid_generate_v4(),
    task_id UUID NOT NULL,
    agent_id UUID NOT NULL,
    status VARCHAR(20) NOT NULL,
    execution_time DECIMAL(10,3),
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    
    PRIMARY KEY (execution_id, created_at)
) PARTITION BY RANGE (created_at);

-- 创建分区表
CREATE TABLE task_executions_2024_01 PARTITION OF task_executions
    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE task_executions_2024_02 PARTITION OF task_executions  
    FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

-- 创建分区维护函数
CREATE OR REPLACE FUNCTION create_monthly_partition(table_name text, start_date date)
RETURNS void AS $$
DECLARE
    partition_name text;
    end_date date;
BEGIN
    partition_name := table_name || '_' || to_char(start_date, 'YYYY_MM');
    end_date := start_date + interval '1 month';
    
    EXECUTE format('CREATE TABLE %I PARTITION OF %I FOR VALUES FROM (%L) TO (%L)',
                   partition_name, table_name, start_date, end_date);
                   
    EXECUTE format('CREATE INDEX %I ON %I (task_id)', 
                   'idx_' || partition_name || '_task_id', partition_name);
END;
$$ LANGUAGE plpgsql;
```

### 4.2 读写分离设计

#### 4.2.1 主从架构配置

```yaml
# 主数据库配置(写入)
master_database:
  host: postgres-master.local
  port: 5432
  max_connections: 200
  synchronous_commit: on
  
# 从数据库配置(读取)  
slave_databases:
  - host: postgres-slave-1.local
    port: 5432
    max_connections: 100
    hot_standby: on
  - host: postgres-slave-2.local
    port: 5432  
    max_connections: 100
    hot_standby: on

# 读写分离路由规则
routing_rules:
  - pattern: "SELECT|EXPLAIN|ANALYZE"
    target: slave
    load_balance: round_robin
  - pattern: "INSERT|UPDATE|DELETE|CREATE|DROP|ALTER"
    target: master
```

#### 4.2.2 数据库连接池配置

```python
# SQLAlchemy读写分离配置
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

class DatabaseConfig:
    """数据库配置管理"""
    
    def __init__(self):
        # 主库配置(写)
        self.master_url = "postgresql+asyncpg://user:pass@master:5432/db"
        self.master_engine = create_engine(
            self.master_url,
            poolclass=QueuePool,
            pool_size=20,
            max_overflow=30,
            pool_pre_ping=True,
            pool_recycle=3600
        )
        
        # 从库配置(读)
        self.slave_urls = [
            "postgresql+asyncpg://user:pass@slave1:5432/db",
            "postgresql+asyncpg://user:pass@slave2:5432/db"
        ]
        self.slave_engines = [
            create_engine(url, poolclass=QueuePool, pool_size=10, max_overflow=20)
            for url in self.slave_urls
        ]
        
    def get_read_engine(self):
        """获取读数据库引擎(负载均衡)"""
        import random
        return random.choice(self.slave_engines)
        
    def get_write_engine(self):
        """获取写数据库引擎"""
        return self.master_engine
```

### 4.3 缓存集成设计

#### 4.3.1 多级缓存架构

```mermaid
graph TB
    A[应用请求] --> B{L1缓存}
    B -->|命中| C[进程内存缓存]
    B -->|未命中| D{L2缓存}
    D -->|命中| E[Redis缓存]
    D -->|未命中| F{L3缓存}
    F -->|命中| G[数据库连接池]
    F -->|未命中| H[PostgreSQL数据库]
    
    C --> I[返回结果]
    E --> I
    G --> I
    H --> J[更新缓存]
    J --> I
```

#### 4.3.2 缓存策略实现

```python
import asyncio
import json
from typing import Any, Optional, Callable
from functools import wraps

class CacheManager:
    """多级缓存管理器"""
    
    def __init__(self, redis_client, local_cache_size=1000):
        self.redis = redis_client
        self.local_cache = {}  # 简化的本地缓存
        self.local_cache_size = local_cache_size
        
    async def get_or_set(self, 
                        key: str, 
                        factory: Callable,
                        ttl: int = 3600,
                        use_local: bool = True) -> Any:
        """获取或设置缓存"""
        
        # L1: 本地缓存
        if use_local and key in self.local_cache:
            return self.local_cache[key]['data']
            
        # L2: Redis缓存
        cached_value = await self.redis.get(key)
        if cached_value:
            data = json.loads(cached_value)
            if use_local:
                self._set_local_cache(key, data)
            return data
            
        # L3: 数据源
        data = await factory()
        
        # 更新缓存
        await self.redis.setex(key, ttl, json.dumps(data, default=str))
        if use_local:
            self._set_local_cache(key, data)
            
        return data
    
    def _set_local_cache(self, key: str, data: Any):
        """设置本地缓存"""
        if len(self.local_cache) >= self.local_cache_size:
            # 简单LRU淘汰
            oldest_key = next(iter(self.local_cache))
            del self.local_cache[oldest_key]
            
        self.local_cache[key] = {
            'data': data,
            'timestamp': asyncio.get_event_loop().time()
        }

def cached(ttl: int = 3600, key_prefix: str = ""):
    """缓存装饰器"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # 生成缓存键
            cache_key = f"{key_prefix}:{func.__name__}:{hash(str(args) + str(kwargs))}"
            
            cache_manager = get_cache_manager()  # 获取缓存管理器实例
            
            return await cache_manager.get_or_set(
                cache_key, 
                lambda: func(*args, **kwargs),
                ttl
            )
        return wrapper
    return decorator

# 使用示例
@cached(ttl=300, key_prefix="agent")
async def get_agent_config(agent_id: str):
    """获取Agent配置(带缓存)"""
    # 数据库查询逻辑
    pass
```

---

## 🔒 数据安全设计

### 5.1 数据加密策略

#### 5.1.1 字段级加密

```sql
-- 创建加密扩展
CREATE EXTENSION IF NOT EXISTS pgcrypto;

-- 敏感数据加密存储
CREATE TABLE users (
    user_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    username VARCHAR(50) UNIQUE NOT NULL,
    
    -- 密码哈希存储
    password_hash VARCHAR(255) NOT NULL,
    
    -- 敏感信息加密存储
    email_encrypted BYTEA,  -- 加密邮箱
    phone_encrypted BYTEA,  -- 加密手机号
    
    -- 用于搜索的哈希值
    email_hash VARCHAR(64),  -- 邮箱哈希(用于查询)
    phone_hash VARCHAR(64),  -- 手机号哈希
    
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- 创建加密/解密函数
CREATE OR REPLACE FUNCTION encrypt_pii(data TEXT, key TEXT)
RETURNS BYTEA AS $$
BEGIN
    RETURN pgp_sym_encrypt(data, key);
END;
$$ LANGUAGE plpgsql;

CREATE OR REPLACE FUNCTION decrypt_pii(encrypted_data BYTEA, key TEXT)
RETURNS TEXT AS $$
BEGIN
    RETURN pgp_sym_decrypt(encrypted_data, key);
END;
$$ LANGUAGE plpgsql;

-- 插入加密数据示例
INSERT INTO users (username, password_hash, email_encrypted, email_hash)
VALUES (
    'john_doe',
    crypt('user_password', gen_salt('bf', 12)),
    encrypt_pii('john@example.com', 'encryption_key'),  
    encode(digest('john@example.com', 'sha256'), 'hex')
);
```

#### 5.1.2 透明数据加密(TDE)

```yaml
# PostgreSQL TDE配置
postgresql_config:
  # 启用数据加密
  ssl: on
  ssl_cert_file: 'server.crt'
  ssl_key_file: 'server.key'
  ssl_ca_file: 'ca.crt'
  
  # 数据目录加密
  data_encryption: enabled
  encryption_key_command: '/usr/local/bin/get-encryption-key.sh'
  
  # WAL日志加密
  wal_encryption: on
  
# 备份加密配置
backup_encryption:
  tool: pg_dump
  encryption: 'gpg --cipher-algo AES256 --compress-algo 2'
  key_management: 'HashiCorp Vault'
```

### 5.2 访问控制设计

#### 5.2.1 行级安全策略(RLS)

```sql
-- 启用行级安全
ALTER TABLE agents ENABLE ROW LEVEL SECURITY;
ALTER TABLE tasks ENABLE ROW LEVEL SECURITY;

-- 创建安全策略：用户只能访问自己创建的Agent
CREATE POLICY agent_owner_policy ON agents
    FOR ALL
    TO application_user
    USING (created_by = current_setting('app.current_user_id')::UUID);

-- 创建安全策略：Agent只能访问分配给自己的任务
CREATE POLICY task_assignment_policy ON tasks
    FOR SELECT
    TO agent_user
    USING (assigned_agent = current_setting('app.current_agent_id')::UUID);

-- 创建安全策略：管理员可以访问所有数据
CREATE POLICY admin_full_access ON agents
    FOR ALL
    TO admin_user
    USING (true);

-- 设置应用级用户上下文
CREATE OR REPLACE FUNCTION set_user_context(user_id UUID, user_role TEXT)
RETURNS void AS $$
BEGIN
    PERFORM set_config('app.current_user_id', user_id::TEXT, false);
    PERFORM set_config('app.current_user_role', user_role, false);
END;
$$ LANGUAGE plpgsql;
```

#### 5.2.2 数据库角色权限设计

```sql
-- 创建数据库角色
CREATE ROLE readonly_user;
CREATE ROLE application_user;
CREATE ROLE agent_user;
CREATE ROLE admin_user;

-- 只读用户权限
GRANT CONNECT ON DATABASE digital_employee TO readonly_user;
GRANT USAGE ON SCHEMA public TO readonly_user;
GRANT SELECT ON ALL TABLES IN SCHEMA public TO readonly_user;

-- 应用用户权限
GRANT CONNECT ON DATABASE digital_employee TO application_user;
GRANT USAGE ON SCHEMA public TO application_user;
GRANT SELECT, INSERT, UPDATE ON users, agents, tasks TO application_user;
GRANT USAGE ON ALL SEQUENCES IN SCHEMA public TO application_user;

-- Agent用户权限(受限)
GRANT CONNECT ON DATABASE digital_employee TO agent_user;
GRANT USAGE ON SCHEMA public TO agent_user;
GRANT SELECT ON agents, tasks, prompt_templates TO agent_user;
GRANT INSERT, UPDATE ON task_executions, agent_metrics TO agent_user;

-- 管理员权限
GRANT ALL PRIVILEGES ON DATABASE digital_employee TO admin_user;

-- 创建具体用户并分配角色
CREATE USER app_server WITH PASSWORD 'secure_password';
GRANT application_user TO app_server;

CREATE USER agent_runner WITH PASSWORD 'agent_password';
GRANT agent_user TO agent_runner;
```

### 5.3 审计日志设计

#### 5.3.1 数据库审计表设计

```sql
-- 数据变更审计表
CREATE TABLE audit_log (
    audit_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    table_name VARCHAR(50) NOT NULL,
    record_id UUID NOT NULL,
    operation VARCHAR(10) NOT NULL,  -- INSERT, UPDATE, DELETE
    old_values JSONB,
    new_values JSONB,
    changed_by UUID,
    changed_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    ip_address INET,
    user_agent TEXT,
    
    -- 索引优化
    INDEX idx_audit_log_table_record (table_name, record_id),
    INDEX idx_audit_log_changed_at (changed_at),
    INDEX idx_audit_log_changed_by (changed_by)
) PARTITION BY RANGE (changed_at);

-- 创建审计触发器函数
CREATE OR REPLACE FUNCTION audit_trigger_function()
RETURNS TRIGGER AS $$
BEGIN
    INSERT INTO audit_log (
        table_name, 
        record_id, 
        operation, 
        old_values, 
        new_values, 
        changed_by,
        ip_address
    ) VALUES (
        TG_TABLE_NAME,
        COALESCE(NEW.id, OLD.id),
        TG_OP,
        CASE WHEN TG_OP = 'DELETE' THEN to_jsonb(OLD) ELSE NULL END,
        CASE WHEN TG_OP = 'INSERT' THEN to_jsonb(NEW) 
             WHEN TG_OP = 'UPDATE' THEN to_jsonb(NEW) 
             ELSE NULL END,
        current_setting('app.current_user_id', true)::UUID,
        inet_client_addr()
    );
    
    RETURN COALESCE(NEW, OLD);
END;
$$ LANGUAGE plpgsql;

-- 为关键表创建审计触发器
CREATE TRIGGER audit_users_trigger
    AFTER INSERT OR UPDATE OR DELETE ON users
    FOR EACH ROW EXECUTE FUNCTION audit_trigger_function();

CREATE TRIGGER audit_agents_trigger  
    AFTER INSERT OR UPDATE OR DELETE ON agents
    FOR EACH ROW EXECUTE FUNCTION audit_trigger_function();
```

---

## 📈 性能优化设计

### 6.1 查询优化策略

#### 6.1.1 慢查询监控

```sql
-- 启用慢查询日志
ALTER SYSTEM SET log_min_duration_statement = 1000;  -- 记录超过1秒的查询
ALTER SYSTEM SET log_statement = 'all';
ALTER SYSTEM SET log_duration = on;
SELECT pg_reload_conf();

-- 创建查询性能监控视图
CREATE VIEW slow_queries AS
SELECT 
    query,
    calls,
    total_time,
    mean_time,
    rows,
    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
FROM pg_stat_statements
WHERE mean_time > 100  -- 平均执行时间超过100ms
ORDER BY total_time DESC;

-- 创建索引使用率监控视图
CREATE VIEW index_usage AS
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_tup_read,
    idx_tup_fetch,
    idx_scan,
    CASE 
        WHEN idx_scan = 0 THEN 'UNUSED'
        WHEN idx_scan < 100 THEN 'LOW_USAGE'
        ELSE 'NORMAL'
    END as usage_status
FROM pg_stat_user_indexes
ORDER BY idx_scan ASC;
```

#### 6.1.2 查询计划优化

```sql
-- 示例：Agent任务查询优化
-- 原始查询(性能较差)
EXPLAIN ANALYZE
SELECT a.agent_id, a.agent_type, COUNT(t.task_id) as task_count
FROM agents a
LEFT JOIN tasks t ON a.agent_id = t.assigned_agent
WHERE a.status = 'active' 
  AND t.created_at >= CURRENT_DATE - INTERVAL '7 days'
GROUP BY a.agent_id, a.agent_type
ORDER BY task_count DESC;

-- 优化后查询
-- 1. 添加复合索引
CREATE INDEX idx_tasks_agent_created ON tasks(assigned_agent, created_at) 
WHERE created_at >= CURRENT_DATE - INTERVAL '30 days';

-- 2. 使用子查询优化
EXPLAIN ANALYZE
SELECT a.agent_id, a.agent_type, 
       COALESCE(task_stats.task_count, 0) as task_count
FROM agents a
LEFT JOIN (
    SELECT assigned_agent, COUNT(*) as task_count
    FROM tasks 
    WHERE created_at >= CURRENT_DATE - INTERVAL '7 days'
      AND assigned_agent IS NOT NULL
    GROUP BY assigned_agent
) task_stats ON a.agent_id = task_stats.assigned_agent
WHERE a.status = 'active'
ORDER BY task_count DESC;

-- 3. 物化视图进一步优化
CREATE MATERIALIZED VIEW agent_task_summary AS
SELECT 
    a.agent_id,
    a.agent_type,
    a.status,
    COUNT(CASE WHEN t.created_at >= CURRENT_DATE - INTERVAL '7 days' THEN t.task_id END) as recent_task_count,
    COUNT(t.task_id) as total_task_count,
    AVG(te.execution_time) as avg_execution_time
FROM agents a
LEFT JOIN tasks t ON a.agent_id = t.assigned_agent
LEFT JOIN task_executions te ON t.task_id = te.task_id
GROUP BY a.agent_id, a.agent_type, a.status;

-- 创建自动刷新任务
CREATE OR REPLACE FUNCTION refresh_agent_summary()
RETURNS void AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY agent_task_summary;
END;
$$ LANGUAGE plpgsql;

-- 定时刷新(需要pg_cron扩展)
SELECT cron.schedule('refresh-agent-summary', '*/15 * * * *', 'SELECT refresh_agent_summary();');
```

### 6.2 存储优化策略

#### 6.2.1 表空间设计

```sql
-- 创建不同类型数据的表空间
-- 1. 高频访问数据(SSD存储)
CREATE TABLESPACE hot_data LOCATION '/data/ssd/postgresql';

-- 2. 温数据(普通SSD存储)  
CREATE TABLESPACE warm_data LOCATION '/data/sata_ssd/postgresql';

-- 3. 冷数据(机械硬盘存储)
CREATE TABLESPACE cold_data LOCATION '/data/hdd/postgresql';

-- 4. 索引专用表空间(高速SSD)
CREATE TABLESPACE index_data LOCATION '/data/nvme/postgresql';

-- 将表分配到合适的表空间
-- 核心业务表 -> 热数据表空间
ALTER TABLE agents SET TABLESPACE hot_data;
ALTER TABLE tasks SET TABLESPACE hot_data;
ALTER TABLE users SET TABLESPACE hot_data;

-- 日志表 -> 温数据表空间
ALTER TABLE task_executions SET TABLESPACE warm_data;
ALTER TABLE audit_log SET TABLESPACE warm_data;

-- 历史数据 -> 冷数据表空间  
ALTER TABLE task_executions_archive SET TABLESPACE cold_data;

-- 索引 -> 索引表空间
ALTER INDEX idx_tasks_status SET TABLESPACE index_data;
ALTER INDEX idx_agents_type SET TABLESPACE index_data;
```

#### 6.2.2 数据归档策略

```sql
-- 创建数据归档函数
CREATE OR REPLACE FUNCTION archive_old_data(
    table_name TEXT,
    archive_table_name TEXT,
    date_column TEXT,
    retention_days INTEGER
) RETURNS INTEGER AS $$
DECLARE
    cutoff_date TIMESTAMP;
    archived_count INTEGER;
BEGIN
    cutoff_date := CURRENT_TIMESTAMP - (retention_days || ' days')::INTERVAL;
    
    -- 将旧数据移动到归档表
    EXECUTE format('
        WITH moved_rows AS (
            DELETE FROM %I 
            WHERE %I < %L
            RETURNING *
        )
        INSERT INTO %I SELECT * FROM moved_rows',
        table_name, date_column, cutoff_date, archive_table_name);
    
    GET DIAGNOSTICS archived_count = ROW_COUNT;
    
    -- 记录归档日志
    INSERT INTO archive_log (table_name, archived_count, archive_date)
    VALUES (table_name, archived_count, CURRENT_TIMESTAMP);
    
    RETURN archived_count;
END;
$$ LANGUAGE plpgsql;

-- 创建归档日志表
CREATE TABLE archive_log (
    log_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    table_name VARCHAR(50) NOT NULL,
    archived_count INTEGER NOT NULL,
    archive_date TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- 定期执行归档任务
SELECT cron.schedule('archive-task-executions', '0 2 * * *', 
    'SELECT archive_old_data(''task_executions'', ''task_executions_archive'', ''created_at'', 90);');
```

---

## 🔄 数据库运维规范

### 7.1 备份恢复策略

#### 7.1.1 备份策略设计

```yaml
# 备份策略配置
backup_strategy:
  # 全量备份
  full_backup:
    frequency: weekly
    schedule: "0 2 * * 0"  # 每周日凌晨2点
    retention: 4_weeks
    compression: gzip
    encryption: enabled
    
  # 增量备份
  incremental_backup:
    frequency: daily
    schedule: "0 3 * * 1-6"  # 周一到周六凌晨3点
    retention: 7_days
    
  # WAL备份
  wal_backup:
    continuous: true
    archive_command: 'test ! -f /backup/wal/%f && cp %p /backup/wal/%f'
    archive_timeout: 300
    
  # 备份验证
  backup_verification:
    schedule: "0 4 * * *"  # 每日凌晨4点验证
    test_restore: weekly
```

#### 7.1.2 备份脚本示例

```bash
#!/bin/bash
# 数据库备份脚本

set -euo pipefail

# 配置参数
DB_HOST="localhost"
DB_PORT="5432"
DB_NAME="digital_employee"
DB_USER="backup_user"
BACKUP_DIR="/backup/postgresql"
LOG_FILE="/var/log/postgresql_backup.log"
RETENTION_DAYS=30

# 日志函数
log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a "$LOG_FILE"
}

# 创建备份目录
mkdir -p "$BACKUP_DIR/$(date +%Y/%m)"

# 备份文件名
BACKUP_FILE="$BACKUP_DIR/$(date +%Y/%m)/db_backup_$(date +%Y%m%d_%H%M%S).sql.gz"

# 执行备份
log "开始数据库备份: $BACKUP_FILE"

if pg_dump -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" \
   --verbose --format=custom --compress=9 \
   | gzip > "$BACKUP_FILE"; then
    
    log "备份成功完成: $(du -h "$BACKUP_FILE" | cut -f1)"
    
    # 验证备份文件完整性
    if gzip -t "$BACKUP_FILE"; then
        log "备份文件完整性验证通过"
    else
        log "错误: 备份文件损坏"
        exit 1
    fi
    
    # 清理旧备份
    find "$BACKUP_DIR" -name "*.sql.gz" -mtime +$RETENTION_DAYS -delete
    log "已清理 $RETENTION_DAYS 天前的旧备份文件"
    
else
    log "错误: 备份失败"
    exit 1
fi

log "备份流程完成"
```

### 7.2 监控告警设计

#### 7.2.1 数据库监控指标

```yaml
# 数据库监控配置
monitoring_metrics:
  # 连接监控
  connections:
    - metric: active_connections
      threshold: 150
      severity: warning
    - metric: max_connections_usage
      threshold: 80%
      severity: critical
      
  # 性能监控
  performance:
    - metric: avg_query_time
      threshold: 2000ms
      severity: warning
    - metric: slow_query_count
      threshold: 10/min
      severity: warning
    - metric: lock_wait_time
      threshold: 5000ms
      severity: critical
      
  # 存储监控
  storage:
    - metric: disk_usage
      threshold: 85%
      severity: warning
    - metric: disk_usage
      threshold: 95%
      severity: critical
    - metric: table_bloat
      threshold: 30%
      severity: warning
      
  # 复制监控(如果使用主从)
  replication:
    - metric: replication_lag
      threshold: 60s
      severity: warning
    - metric: replication_lag  
      threshold: 300s
      severity: critical
```

#### 7.2.2 监控脚本示例

```python
#!/usr/bin/env python3
"""PostgreSQL监控脚本"""

import asyncio
import asyncpg
import logging
from typing import Dict, Any
import json
from datetime import datetime

class PostgreSQLMonitor:
    """PostgreSQL监控器"""
    
    def __init__(self, db_url: str):
        self.db_url = db_url
        self.logger = logging.getLogger(__name__)
        
    async def collect_metrics(self) -> Dict[str, Any]:
        """收集数据库指标"""
        conn = await asyncpg.connect(self.db_url)
        
        try:
            metrics = {}
            
            # 连接数监控
            metrics['connections'] = await self._check_connections(conn)
            
            # 性能监控
            metrics['performance'] = await self._check_performance(conn)
            
            # 存储监控
            metrics['storage'] = await self._check_storage(conn)
            
            # 复制监控
            metrics['replication'] = await self._check_replication(conn)
            
            return metrics
            
        finally:
            await conn.close()
    
    async def _check_connections(self, conn) -> Dict[str, Any]:
        """检查连接状态"""
        query = """
        SELECT 
            count(*) as total_connections,
            count(*) FILTER (WHERE state = 'active') as active_connections,
            count(*) FILTER (WHERE state = 'idle') as idle_connections,
            max_connections
        FROM pg_stat_activity, 
        (SELECT setting::int as max_connections FROM pg_settings WHERE name = 'max_connections') mc
        GROUP BY max_connections
        """
        
        result = await conn.fetchrow(query)
        return {
            'total_connections': result['total_connections'],
            'active_connections': result['active_connections'], 
            'idle_connections': result['idle_connections'],
            'max_connections': result['max_connections'],
            'connection_usage_percent': (result['total_connections'] / result['max_connections']) * 100
        }
    
    async def _check_performance(self, conn) -> Dict[str, Any]:
        """检查性能指标"""
        query = """
        SELECT 
            round(avg(mean_time)::numeric, 2) as avg_query_time,
            count(*) FILTER (WHERE mean_time > 1000) as slow_queries,
            sum(calls) as total_queries,
            round(avg(shared_blks_hit::numeric / nullif(shared_blks_hit + shared_blks_read, 0)) * 100, 2) as cache_hit_ratio
        FROM pg_stat_statements
        WHERE calls > 0
        """
        
        result = await conn.fetchrow(query)
        if result:
            return {
                'avg_query_time_ms': float(result['avg_query_time'] or 0),
                'slow_queries': result['slow_queries'],
                'total_queries': result['total_queries'],
                'cache_hit_ratio': float(result['cache_hit_ratio'] or 0)
            }
        return {}
    
    async def _check_storage(self, conn) -> Dict[str, Any]:
        """检查存储使用情况"""
        query = """
        SELECT 
            pg_database_size(current_database()) as db_size_bytes,
            pg_size_pretty(pg_database_size(current_database())) as db_size_pretty
        """
        
        result = await conn.fetchrow(query)
        
        # 检查表空间使用情况
        tablespace_query = """
        SELECT 
            spcname,
            pg_size_pretty(pg_tablespace_size(spcname)) as size
        FROM pg_tablespace
        """
        
        tablespaces = await conn.fetch(tablespace_query)
        
        return {
            'database_size_bytes': result['db_size_bytes'],
            'database_size_pretty': result['db_size_pretty'],
            'tablespaces': [dict(ts) for ts in tablespaces]
        }
    
    async def _check_replication(self, conn) -> Dict[str, Any]:
        """检查复制状态"""
        query = """
        SELECT 
            application_name,
            client_addr,
            state,
            pg_wal_lsn_diff(pg_current_wal_lsn(), flush_lsn) as replication_lag_bytes
        FROM pg_stat_replication
        """
        
        result = await conn.fetch(query)
        return {
            'replicas': [dict(row) for row in result],
            'replica_count': len(result)
        }

# 使用示例
async def main():
    monitor = PostgreSQLMonitor("postgresql://user:pass@localhost/db")
    metrics = await monitor.collect_metrics()
    
    print(json.dumps(metrics, indent=2, default=str))
    
    # 检查告警条件
    if metrics['connections']['connection_usage_percent'] > 80:
        print("告警: 数据库连接使用率过高")
        
    if metrics['performance']['avg_query_time_ms'] > 2000:
        print("告警: 平均查询时间过长")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## 📋 数据库变更管理

### 8.1 数据库版本控制

#### 8.1.1 迁移脚本规范

```sql
-- 迁移脚本模板: V001__Create_agents_table.sql
-- 版本: 1.0.0
-- 描述: 创建Agent基础表
-- 作者: Database Team
-- 日期: 2024-01-24

-- 向前迁移
BEGIN;

-- 创建agents表
CREATE TABLE IF NOT EXISTS agents (
    agent_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    agent_type VARCHAR(50) NOT NULL,
    version VARCHAR(20) NOT NULL DEFAULT '1.0.0',
    config JSONB NOT NULL DEFAULT '{}',
    status VARCHAR(20) NOT NULL DEFAULT 'inactive',
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    created_by UUID,
    
    -- 约束
    CONSTRAINT agents_status_check CHECK (status IN ('active', 'inactive', 'suspended', 'deleted')),
    CONSTRAINT agents_config_check CHECK (jsonb_typeof(config) = 'object')
);

-- 创建索引
CREATE INDEX IF NOT EXISTS idx_agents_type ON agents(agent_type);
CREATE INDEX IF NOT EXISTS idx_agents_status ON agents(status);
CREATE INDEX IF NOT EXISTS idx_agents_created_at ON agents(created_at);

-- 创建触发器
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER agents_updated_at 
    BEFORE UPDATE ON agents 
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

-- 插入初始数据
INSERT INTO agents (agent_type, config) VALUES 
('system_admin', '{"max_concurrent_tasks": 100, "timeout_seconds": 3600}'),
('default_agent', '{"max_concurrent_tasks": 10, "timeout_seconds": 300}')
ON CONFLICT DO NOTHING;

-- 记录迁移版本
INSERT INTO schema_migrations (version, applied_at) VALUES ('V001', CURRENT_TIMESTAMP);

COMMIT;
```

#### 8.1.2 回滚脚本规范

```sql
-- 回滚脚本: V001__Create_agents_table_rollback.sql
-- 版本: 1.0.0 
-- 描述: 回滚Agent基础表创建
-- 作者: Database Team
-- 日期: 2024-01-24

BEGIN;

-- 删除触发器
DROP TRIGGER IF EXISTS agents_updated_at ON agents;
DROP FUNCTION IF EXISTS update_updated_at_column();

-- 删除索引
DROP INDEX IF EXISTS idx_agents_type;
DROP INDEX IF EXISTS idx_agents_status;
DROP INDEX IF EXISTS idx_agents_created_at;

-- 删除表
DROP TABLE IF EXISTS agents;

-- 删除迁移记录
DELETE FROM schema_migrations WHERE version = 'V001';

COMMIT;
```

### 8.2 数据库CI/CD集成

#### 8.2.1 数据库部署流水线

```yaml
# .gitlab-ci.yml 数据库部署部分
stages:
  - validate
  - test
  - deploy

variables:
  FLYWAY_URL: "jdbc:postgresql://localhost:5432/digital_employee"
  FLYWAY_USER: "flyway_user"
  FLYWAY_SCHEMAS: "public"

# 数据库脚本验证
db_validate:
  stage: validate
  image: flyway/flyway:latest
  script:
    - flyway validate -url=$FLYWAY_URL -user=$FLYWAY_USER -password=$FLYWAY_PASSWORD
  rules:
    - changes:
      - database/migrations/**/*

# 数据库测试
db_test:
  stage: test
  services:
    - postgres:15
  variables:
    POSTGRES_DB: test_db
    POSTGRES_USER: test_user
    POSTGRES_PASSWORD: test_password
  script:
    - flyway migrate -url=jdbc:postgresql://postgres:5432/test_db -user=test_user -password=test_password
    - python -m pytest tests/database/
  rules:
    - changes:
      - database/migrations/**/*

# 部署到开发环境
deploy_dev:
  stage: deploy
  script:
    - flyway info -url=$DEV_DB_URL -user=$DEV_DB_USER -password=$DEV_DB_PASSWORD
    - flyway migrate -url=$DEV_DB_URL -user=$DEV_DB_USER -password=$DEV_DB_PASSWORD
  environment:
    name: development
  rules:
    - if: $CI_COMMIT_BRANCH == "develop"

# 部署到生产环境
deploy_prod:
  stage: deploy
  script:
    - flyway info -url=$PROD_DB_URL -user=$PROD_DB_USER -password=$PROD_DB_PASSWORD
    - flyway migrate -url=$PROD_DB_URL -user=$PROD_DB_USER -password=$PROD_DB_PASSWORD
  environment:
    name: production
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
      when: manual
  before_script:
    - echo "即将在生产环境执行数据库迁移，请确认所有变更已经过充分测试"
```

---

## 📚 附录

### A.1 数据库检查清单

#### A.1.1 设计评审检查清单

- [ ] **表结构设计**
  - [ ] 主键设计合理(推荐UUID)
  - [ ] 外键关系正确定义
  - [ ] 字段类型选择恰当 
  - [ ] 必要的约束条件已添加
  - [ ] 索引设计符合查询需求

- [ ] **性能设计**
  - [ ] 查询路径已优化
  - [ ] 必要的索引已创建
  - [ ] 分区策略已考虑
  - [ ] 缓存策略已设计

- [ ] **安全设计**
  - [ ] 敏感数据已加密
  - [ ] 访问权限已限制
  - [ ] 审计日志已启用
  - [ ] 行级安全已考虑

- [ ] **运维设计**
  - [ ] 备份策略已制定
  - [ ] 监控指标已定义
  - [ ] 归档策略已考虑
  - [ ] 灾难恢复已规划

#### A.1.2 部署前检查清单

- [ ] **环境准备**
  - [ ] 数据库服务器配置正确
  - [ ] 网络连接已测试
  - [ ] 防火墙规则已配置
  - [ ] SSL证书已安装

- [ ] **数据迁移**
  - [ ] 迁移脚本已测试
  - [ ] 回滚脚本已准备
  - [ ] 数据备份已完成
  - [ ] 迁移时间窗口已安排

- [ ] **监控告警**
  - [ ] 监控系统已配置
  - [ ] 告警规则已设置
  - [ ] 通知渠道已测试
  - [ ] 运维手册已准备

### A.2 常用SQL工具函数

```sql
-- 工具函数集合

-- 1. 生成随机字符串
CREATE OR REPLACE FUNCTION generate_random_string(length INTEGER)
RETURNS TEXT AS $$
BEGIN
    RETURN array_to_string(
        ARRAY(
            SELECT chr((97 + round(random() * 25))::integer)
            FROM generate_series(1, length)
        ), 
        ''
    );
END;
$$ LANGUAGE plpgsql;

-- 2. JSON字段合并
CREATE OR REPLACE FUNCTION jsonb_merge(json1 JSONB, json2 JSONB)
RETURNS JSONB AS $$
BEGIN
    RETURN json1 || json2;
END;
$$ LANGUAGE plpgsql;

-- 3. 分页查询辅助函数
CREATE OR REPLACE FUNCTION paginate_query(
    base_query TEXT,
    page_size INTEGER DEFAULT 20,
    page_number INTEGER DEFAULT 1
) RETURNS TEXT AS $$
BEGIN
    RETURN base_query || 
           ' LIMIT ' || page_size || 
           ' OFFSET ' || ((page_number - 1) * page_size);
END;
$$ LANGUAGE plpgsql;

-- 4. 数据库统计信息函数
CREATE OR REPLACE FUNCTION get_table_stats(table_name TEXT)
RETURNS TABLE(
    total_rows BIGINT,
    table_size TEXT,
    index_size TEXT,
    total_size TEXT
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        (SELECT reltuples::BIGINT FROM pg_class WHERE relname = table_name),
        pg_size_pretty(pg_total_relation_size(table_name::regclass)),
        pg_size_pretty(pg_indexes_size(table_name::regclass)),
        pg_size_pretty(pg_total_relation_size(table_name::regclass) + pg_indexes_size(table_name::regclass));
END;
$$ LANGUAGE plpgsql;
```

### A.3 参考资料

- [PostgreSQL官方文档](https://www.postgresql.org/docs/)
- [数据库设计最佳实践](https://wiki.postgresql.org/wiki/Don%27t_Do_This)
- [PostgreSQL性能调优指南](https://wiki.postgresql.org/wiki/Performance_Optimization)
- [数据库安全最佳实践](https://www.postgresql.org/docs/current/security.html)

---

**文档状态**: 正式发布  
**最后更新**: 2024-01-24  
**下次评审**: 2024-04-24  
**批准人**: 数据架构委员会