# æ•°å­—å‘˜å·¥ç³»ç»Ÿéƒ¨ç½²å’Œè¿ç»´è§„èŒƒ

## ğŸ¯ è¿ç»´ç†å¿µ

### æ ¸å¿ƒåŸåˆ™
- **æ¸è¿›å¼éƒ¨ç½²**ï¼šä»æœ¬åœ°å¼€å‘åˆ°ç”Ÿäº§ç¯å¢ƒçš„å¹³æ»‘è¿‡æ¸¡
- **ç›‘æ§ä¼˜å…ˆ**ï¼šæ¯ä¸ªç»„ä»¶éƒ½æœ‰å®Œæ•´çš„ç›‘æ§è¦†ç›–
- **è‡ªåŠ¨åŒ–è¿ç»´**ï¼šå‡å°‘äººå·¥æ“ä½œï¼Œæé«˜å¯é æ€§
- **æ•…éšœè‡ªæ„ˆ**ï¼šç³»ç»Ÿå…·å¤‡è‡ªåŠ¨æ¢å¤èƒ½åŠ›
- **æˆæœ¬ä¼˜åŒ–**ï¼šåœ¨æ€§èƒ½å’Œæˆæœ¬ä¹‹é—´æ‰¾åˆ°æœ€ä½³å¹³è¡¡

### AIç³»ç»Ÿè¿ç»´ç‰¹ç‚¹
- **å¤–éƒ¨ä¾èµ–é‡**ï¼šé«˜åº¦ä¾èµ–ç¬¬ä¸‰æ–¹AIæœåŠ¡
- **æ€§èƒ½æ³¢åŠ¨å¤§**ï¼šAIå“åº”æ—¶é—´ä¸ç¨³å®š
- **æˆæœ¬æ•æ„Ÿ**ï¼šTokenä½¿ç”¨ç›´æ¥å½±å“è¿è¥æˆæœ¬
- **è´¨é‡ç›‘æ§å¤æ‚**ï¼šéœ€è¦ç›‘æ§AIå“åº”è´¨é‡

---

## ğŸ—ï¸ éƒ¨ç½²æ¶æ„è®¾è®¡

### ç¯å¢ƒåˆ†å±‚ç­–ç•¥
```
å¼€å‘ç¯å¢ƒ (Development) â†’ æµ‹è¯•ç¯å¢ƒ (Testing) â†’ é¢„å‘ç¯å¢ƒ (Staging) â†’ ç”Ÿäº§ç¯å¢ƒ (Production)
```

#### ç¯å¢ƒé…ç½®å¯¹æ¯”
| ç»„ä»¶ | å¼€å‘ç¯å¢ƒ | æµ‹è¯•ç¯å¢ƒ | é¢„å‘ç¯å¢ƒ | ç”Ÿäº§ç¯å¢ƒ |
|------|----------|----------|----------|----------|
| **AIæœåŠ¡** | Mock/Sandbox | Sandbox | ç”Ÿäº§API | ç”Ÿäº§API |
| **æ•°æ®åº“** | SQLite | PostgreSQL | PostgreSQL | PostgreSQLé›†ç¾¤ |
| **ç¼“å­˜** | å†…å­˜ | Rediså•å®ä¾‹ | Rediså•å®ä¾‹ | Redisé›†ç¾¤ |
| **è´Ÿè½½å‡è¡¡** | æ—  | æ—  | Nginx | Nginx+Keepalived |
| **ç›‘æ§** | åŸºç¡€æ—¥å¿— | åŸºç¡€ç›‘æ§ | å®Œæ•´ç›‘æ§ | å®Œæ•´ç›‘æ§+å‘Šè­¦ |
| **å®¹å™¨åŒ–** | Docker | Docker | Docker | Kubernetes |

### éƒ¨ç½²æ¶æ„å›¾
```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              è´Ÿè½½å‡è¡¡å±‚              â”‚
                    â”‚     Nginx + SSLç»ˆç«¯ + é™æµ         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              åº”ç”¨å±‚                 â”‚
                    â”‚  FastAPIæœåŠ¡é›†ç¾¤ (3ä¸ªå®ä¾‹)         â”‚
                    â”‚  â”œâ”€ Agent Runtime                   â”‚
                    â”‚  â”œâ”€ AI Service Layer                â”‚
                    â”‚  â””â”€ Task Orchestrator               â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                 â”‚                                 â”‚
â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”
â”‚      æ•°æ®å±‚          â”‚    â”‚    ç¼“å­˜å±‚       â”‚    â”‚      å¤–éƒ¨æœåŠ¡å±‚       â”‚
â”‚  PostgreSQLä¸»ä»      â”‚    â”‚  Redisé›†ç¾¤      â”‚    â”‚  OpenAI/Claude API   â”‚
â”‚  â”œâ”€ ä¸»åº“(å†™)        â”‚    â”‚  â”œâ”€ ä¼šè¯ç¼“å­˜    â”‚    â”‚  â”œâ”€ å¤šå‚å•†å¤‡ä»½        â”‚
â”‚  â”œâ”€ ä»åº“(è¯»)        â”‚    â”‚  â”œâ”€ ç»“æœç¼“å­˜    â”‚    â”‚  â”œâ”€ é™æµå’Œé‡è¯•        â”‚
â”‚  â””â”€ å¤‡ä»½å­˜å‚¨        â”‚    â”‚  â””â”€ é˜Ÿåˆ—å­˜å‚¨    â”‚    â”‚  â””â”€ å¥åº·æ£€æŸ¥          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ³ å®¹å™¨åŒ–éƒ¨ç½²è§„èŒƒ

### Dockerfileè§„èŒƒ
```dockerfile
# ç”Ÿäº§ç¯å¢ƒDockerfile
FROM python:3.11-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    curl \
    vim \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
COPY requirements-prod.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements-prod.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºérootç”¨æˆ·
RUN groupadd -r appuser && useradd -r -g appuser appuser
RUN chown -R appuser:appuser /app
USER appuser

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["gunicorn", "run_server:app", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "-b", "0.0.0.0:8000"]
```

### Docker Composeé…ç½®
```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  app:
    build:
      context: .
      dockerfile: Dockerfile.prod
    ports:
      - "8000:8000"
    environment:
      - ENV=production
      - DATABASE_URL=postgresql://user:${DB_PASSWORD}@db:5432/digital_employee
      - REDIS_URL=redis://redis:6379/0
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LOG_LEVEL=INFO
    depends_on:
      - db
      - redis
    restart: unless-stopped
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
      - ./nginx/logs:/var/log/nginx
    depends_on:
      - app
    restart: unless-stopped

  db:
    image: postgres:15
    environment:
      POSTGRES_DB: digital_employee
      POSTGRES_USER: user
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backups:/backups
    restart: unless-stopped
    ports:
      - "5432:5432"
    command: >
      postgres
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100

  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    restart: unless-stopped
    ports:
      - "6379:6379"

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:

networks:
  default:
    driver: bridge
```

---

## â˜¸ï¸ Kuberneteséƒ¨ç½²è§„èŒƒ

### Namespaceé…ç½®
```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: digital-employee
  labels:
    app.kubernetes.io/name: digital-employee
    app.kubernetes.io/version: "1.0.0"
```

### ConfigMapé…ç½®
```yaml
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: digital-employee-config
  namespace: digital-employee
data:
  APP_ENV: "production"
  LOG_LEVEL: "INFO"
  DB_HOST: "postgresql-service"
  DB_PORT: "5432"
  DB_NAME: "digital_employee"
  REDIS_HOST: "redis-service"
  REDIS_PORT: "6379"
  
  # AIæœåŠ¡é…ç½®
  AI_SERVICE_TIMEOUT: "30"
  AI_SERVICE_RETRY_COUNT: "3"
  AI_SERVICE_RETRY_DELAY: "2"
  
  # ç›‘æ§é…ç½®
  PROMETHEUS_METRICS_PORT: "9000"
  HEALTH_CHECK_INTERVAL: "30"
```

### Secreté…ç½®
```yaml
# k8s/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: digital-employee-secrets
  namespace: digital-employee
type: Opaque
data:
  DB_PASSWORD: <base64-encoded-password>
  OPENAI_API_KEY: <base64-encoded-api-key>
  CLAUDE_API_KEY: <base64-encoded-api-key>
  JWT_SECRET_KEY: <base64-encoded-jwt-secret>
```

### Deploymenté…ç½®
```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: digital-employee-app
  namespace: digital-employee
  labels:
    app: digital-employee
    version: "1.0.0"
spec:
  replicas: 3
  selector:
    matchLabels:
      app: digital-employee
  template:
    metadata:
      labels:
        app: digital-employee
        version: "1.0.0"
    spec:
      containers:
      - name: app
        image: digital-employee:1.0.0
        ports:
        - containerPort: 8000
          name: http
        - containerPort: 9000
          name: metrics
        
        env:
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: digital-employee-secrets
              key: DB_PASSWORD
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: digital-employee-secrets
              key: OPENAI_API_KEY
        
        envFrom:
        - configMapRef:
            name: digital-employee-config
        
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 2Gi
        
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 15"]
      
      terminationGracePeriodSeconds: 30
      
      # åäº²å’Œæ€§ç¡®ä¿Podåˆ†å¸ƒåœ¨ä¸åŒèŠ‚ç‚¹
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - digital-employee
              topologyKey: kubernetes.io/hostname
```

### Serviceé…ç½®
```yaml
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: digital-employee-service
  namespace: digital-employee
  labels:
    app: digital-employee
spec:
  selector:
    app: digital-employee
  ports:
  - name: http
    port: 80
    targetPort: 8000
  - name: metrics
    port: 9000
    targetPort: 9000
  type: ClusterIP
```

### Ingressé…ç½®
```yaml
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: digital-employee-ingress
  namespace: digital-employee
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/rate-limit-window: "1m"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
spec:
  tls:
  - hosts:
    - api.digital-employee.example.com
    secretName: digital-employee-tls
  rules:
  - host: api.digital-employee.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: digital-employee-service
            port:
              number: 80
```

---

## ğŸ“Š ç›‘æ§å’Œå¯è§‚æµ‹æ€§

### Prometheusç›‘æ§é…ç½®
```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'digital-employee'
    static_configs:
      - targets: ['app:9000']
    metrics_path: /metrics
    scrape_interval: 30s
    
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres_exporter:9187']
    
  - job_name: 'redis'
    static_configs:
      - targets: ['redis_exporter:9121']
    
  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx_exporter:9113']

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

### å‘Šè­¦è§„åˆ™é…ç½®
```yaml
# monitoring/alert_rules.yml
groups:
- name: digital_employee_alerts
  rules:
  
  # æœåŠ¡å¯ç”¨æ€§å‘Šè­¦
  - alert: ServiceDown
    expr: up{job="digital-employee"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "æ•°å­—å‘˜å·¥æœåŠ¡å®•æœº"
      description: "{{ $labels.instance }} æœåŠ¡å·²å®•æœºè¶…è¿‡1åˆ†é’Ÿ"
  
  # å“åº”æ—¶é—´å‘Šè­¦
  - alert: HighResponseTime
    expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 10
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "å“åº”æ—¶é—´è¿‡é«˜"
      description: "95%è¯·æ±‚å“åº”æ—¶é—´è¶…è¿‡10ç§’ï¼Œå½“å‰å€¼: {{ $value }}ç§’"
  
  # AIæœåŠ¡é”™è¯¯ç‡å‘Šè­¦
  - alert: HighAIServiceErrorRate
    expr: rate(ai_requests_total{status="error"}[5m]) / rate(ai_requests_total[5m]) > 0.05
    for: 3m
    labels:
      severity: warning
    annotations:
      summary: "AIæœåŠ¡é”™è¯¯ç‡è¿‡é«˜"
      description: "AIæœåŠ¡é”™è¯¯ç‡è¶…è¿‡5%ï¼Œå½“å‰å€¼: {{ $value | humanizePercentage }}"
  
  # Tokenä½¿ç”¨é‡å‘Šè­¦
  - alert: HighTokenUsage
    expr: increase(ai_tokens_used_total[1h]) > 100000
    for: 0m
    labels:
      severity: warning
    annotations:
      summary: "Tokenä½¿ç”¨é‡è¿‡é«˜"
      description: "è¿‡å»1å°æ—¶Tokenä½¿ç”¨é‡: {{ $value }}"
  
  # æ•°æ®åº“è¿æ¥å‘Šè­¦
  - alert: DatabaseConnectionsHigh
    expr: postgres_stat_database_numbackends > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "æ•°æ®åº“è¿æ¥æ•°è¿‡é«˜"
      description: "æ•°æ®åº“è¿æ¥æ•°: {{ $value }}/100"
  
  # å†…å­˜ä½¿ç”¨å‘Šè­¦
  - alert: HighMemoryUsage
    expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) > 0.8
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
      description: "å®¹å™¨å†…å­˜ä½¿ç”¨ç‡: {{ $value | humanizePercentage }}"
```

### Grafanaä»ªè¡¨æ¿é…ç½®
```json
{
  "dashboard": {
    "id": null,
    "title": "æ•°å­—å‘˜å·¥ç³»ç»Ÿç›‘æ§",
    "tags": ["digital-employee"],
    "timezone": "browser",
    "panels": [
      {
        "title": "æœåŠ¡æ¦‚è§ˆ",
        "type": "stat",
        "targets": [
          {
            "expr": "up{job=\"digital-employee\"}",
            "legendFormat": "æœåŠ¡çŠ¶æ€"
          }
        ]
      },
      {
        "title": "è¯·æ±‚QPS",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{endpoint}}"
          }
        ]
      },
      {
        "title": "å“åº”æ—¶é—´åˆ†å¸ƒ",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "P50"
          },
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "P95"
          },
          {
            "expr": "histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "P99"
          }
        ]
      },
      {
        "title": "AIæœåŠ¡è°ƒç”¨",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(ai_requests_total[5m])",
            "legendFormat": "{{service}} {{status}}"
          }
        ]
      },
      {
        "title": "Tokenä½¿ç”¨é‡",
        "type": "graph",
        "targets": [
          {
            "expr": "increase(ai_tokens_used_total[1h])",
            "legendFormat": "{{service}} {{model}}"
          }
        ]
      }
    ]
  }
}
```

---

## ğŸ” å®‰å…¨é…ç½®è§„èŒƒ

### SSL/TLSé…ç½®
```nginx
# nginx/ssl.conf
server {
    listen 443 ssl http2;
    server_name api.digital-employee.example.com;
    
    # SSLé…ç½®
    ssl_certificate /etc/nginx/ssl/cert.pem;
    ssl_certificate_key /etc/nginx/ssl/key.pem;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384;
    ssl_prefer_server_ciphers off;
    
    # å®‰å…¨å¤´
    add_header Strict-Transport-Security "max-age=63072000" always;
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Referrer-Policy "strict-origin-when-cross-origin";
    
    # é™æµé…ç½®
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req zone=api burst=20 nodelay;
    
    # ä»£ç†é…ç½®
    location / {
        proxy_pass http://app:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # è¶…æ—¶é…ç½®
        proxy_connect_timeout 30s;
        proxy_send_timeout 30s;
        proxy_read_timeout 30s;
    }
}
```

### åº”ç”¨å®‰å…¨é…ç½®
```python
# config/security.py
from functools import wraps
import jwt
import time
from typing import Optional

class SecurityConfig:
    """å®‰å…¨é…ç½®ç±»"""
    
    def __init__(self):
        self.jwt_secret = os.getenv("JWT_SECRET_KEY")
        self.jwt_algorithm = "HS256"
        self.jwt_expiration = 3600  # 1å°æ—¶
        self.rate_limit_requests = 100
        self.rate_limit_window = 60  # 1åˆ†é’Ÿ
        
        # APIå¯†é’¥ç™½åå•
        self.api_key_whitelist = set(os.getenv("API_KEY_WHITELIST", "").split(","))
        
        # æ•æ„Ÿä¿¡æ¯è¿‡æ»¤
        self.sensitive_patterns = [
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',  # Email
            r'\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b',  # ä¿¡ç”¨å¡å·
            r'\b\d{3}-\d{2}-\d{4}\b',  # SSN
        ]

def require_api_key(f):
    """APIå¯†é’¥éªŒè¯è£…é¥°å™¨"""
    @wraps(f)
    async def wrapper(*args, **kwargs):
        request = kwargs.get('request') or args[0]
        api_key = request.headers.get('X-API-Key')
        
        if not api_key or api_key not in security_config.api_key_whitelist:
            raise HTTPException(status_code=401, detail="Invalid API key")
        
        return await f(*args, **kwargs)
    return wrapper

def rate_limit(max_requests: int = 100, window_seconds: int = 60):
    """é¢‘ç‡é™åˆ¶è£…é¥°å™¨"""
    def decorator(f):
        @wraps(f)
        async def wrapper(*args, **kwargs):
            request = kwargs.get('request') or args[0]
            client_ip = request.client.host
            
            # æ£€æŸ¥é¢‘ç‡é™åˆ¶
            if not check_rate_limit(client_ip, max_requests, window_seconds):
                raise HTTPException(status_code=429, detail="Rate limit exceeded")
            
            return await f(*args, **kwargs)
        return wrapper
    return decorator

def check_rate_limit(client_ip: str, max_requests: int, window_seconds: int) -> bool:
    """æ£€æŸ¥é¢‘ç‡é™åˆ¶"""
    current_time = int(time.time())
    window_start = current_time - window_seconds
    
    # ä½¿ç”¨Rediså®ç°æ»‘åŠ¨çª—å£é¢‘ç‡é™åˆ¶
    redis_key = f"rate_limit:{client_ip}"
    
    # æ¸…ç†è¿‡æœŸè®°å½•
    redis_client.zremrangebyscore(redis_key, 0, window_start)
    
    # æ£€æŸ¥å½“å‰è¯·æ±‚æ•°
    current_requests = redis_client.zcard(redis_key)
    
    if current_requests >= max_requests:
        return False
    
    # è®°å½•å½“å‰è¯·æ±‚
    redis_client.zadd(redis_key, {str(current_time): current_time})
    redis_client.expire(redis_key, window_seconds)
    
    return True
```

---

## ğŸ—„ï¸ æ•°æ®å¤‡ä»½å’Œæ¢å¤

### æ•°æ®åº“å¤‡ä»½ç­–ç•¥
```bash
#!/bin/bash
# scripts/backup_database.sh

set -e

# é…ç½®
DB_HOST=${DB_HOST:-localhost}
DB_PORT=${DB_PORT:-5432}
DB_NAME=${DB_NAME:-digital_employee}
DB_USER=${DB_USER:-user}
BACKUP_DIR=${BACKUP_DIR:-/backups}
RETENTION_DAYS=${RETENTION_DAYS:-7}

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="$BACKUP_DIR/db_backup_$TIMESTAMP.sql.gz"

echo "å¼€å§‹æ•°æ®åº“å¤‡ä»½: $BACKUP_FILE"

# æ‰§è¡Œå¤‡ä»½
PGPASSWORD=$DB_PASSWORD pg_dump \
  -h $DB_HOST \
  -p $DB_PORT \
  -U $DB_USER \
  -d $DB_NAME \
  --no-password \
  --format=custom \
  --compress=9 \
  --verbose \
  | gzip > $BACKUP_FILE

# éªŒè¯å¤‡ä»½æ–‡ä»¶
if [ -f "$BACKUP_FILE" ] && [ -s "$BACKUP_FILE" ]; then
    echo "æ•°æ®åº“å¤‡ä»½æˆåŠŸ: $BACKUP_FILE"
    
    # è®°å½•å¤‡ä»½æ—¥å¿—
    echo "$(date): Database backup completed - $BACKUP_FILE" >> $BACKUP_DIR/backup.log
    
    # æ¸…ç†è¿‡æœŸå¤‡ä»½
    find $BACKUP_DIR -name "db_backup_*.sql.gz" -mtime +$RETENTION_DAYS -delete
    echo "æ¸…ç†è¶…è¿‡ $RETENTION_DAYS å¤©çš„æ—§å¤‡ä»½"
    
else
    echo "æ•°æ®åº“å¤‡ä»½å¤±è´¥!"
    exit 1
fi

# å¯é€‰ï¼šä¸Šä¼ åˆ°äº‘å­˜å‚¨
if [ -n "$AWS_S3_BUCKET" ]; then
    aws s3 cp $BACKUP_FILE s3://$AWS_S3_BUCKET/backups/database/
    echo "å¤‡ä»½å·²ä¸Šä¼ åˆ°S3: s3://$AWS_S3_BUCKET/backups/database/"
fi
```

### æ•°æ®æ¢å¤è„šæœ¬
```bash
#!/bin/bash
# scripts/restore_database.sh

set -e

BACKUP_FILE=$1
DB_HOST=${DB_HOST:-localhost}
DB_PORT=${DB_PORT:-5432}
DB_NAME=${DB_NAME:-digital_employee}
DB_USER=${DB_USER:-user}

if [ -z "$BACKUP_FILE" ]; then
    echo "ç”¨æ³•: $0 <backup_file>"
    echo "å¯ç”¨å¤‡ä»½æ–‡ä»¶:"
    ls -la /backups/db_backup_*.sql.gz
    exit 1
fi

if [ ! -f "$BACKUP_FILE" ]; then
    echo "å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: $BACKUP_FILE"
    exit 1
fi

echo "è­¦å‘Š: æ­¤æ“ä½œå°†è¦†ç›–æ•°æ®åº“ $DB_NAME çš„æ‰€æœ‰æ•°æ®!"
read -p "ç¡®è®¤ç»§ç»­? (y/N): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "æ“ä½œå·²å–æ¶ˆ"
    exit 1
fi

echo "å¼€å§‹æ•°æ®åº“æ¢å¤: $BACKUP_FILE"

# åœæ­¢åº”ç”¨æœåŠ¡
echo "åœæ­¢åº”ç”¨æœåŠ¡..."
docker-compose stop app

# æ¢å¤æ•°æ®åº“
echo "æ¢å¤æ•°æ®åº“..."
gunzip -c $BACKUP_FILE | PGPASSWORD=$DB_PASSWORD psql \
  -h $DB_HOST \
  -p $DB_PORT \
  -U $DB_USER \
  -d $DB_NAME

echo "æ•°æ®åº“æ¢å¤å®Œæˆ"

# é‡å¯åº”ç”¨æœåŠ¡
echo "é‡å¯åº”ç”¨æœåŠ¡..."
docker-compose start app

echo "æ•°æ®æ¢å¤å®Œæˆ: $BACKUP_FILE"
```

### è‡ªåŠ¨å¤‡ä»½Cronä»»åŠ¡
```bash
# crontabé…ç½®
# æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œæ•°æ®åº“å¤‡ä»½
0 2 * * * /opt/digital-employee/scripts/backup_database.sh >> /var/log/backup.log 2>&1

# æ¯å‘¨æ—¥å‡Œæ™¨1ç‚¹æ‰§è¡Œå®Œæ•´ç³»ç»Ÿå¤‡ä»½
0 1 * * 0 /opt/digital-employee/scripts/full_system_backup.sh >> /var/log/backup.log 2>&1

# æ¯å°æ—¶æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€
0 * * * * /opt/digital-employee/scripts/health_check.sh >> /var/log/health.log 2>&1
```

---

## ğŸš€ è‡ªåŠ¨åŒ–éƒ¨ç½²æµç¨‹

### CI/CD Pipelineé…ç½®
```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [main]
    tags: ['v*']

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: Run tests
      run: |
        pytest tests/ -v --cov=digital_employee --cov-report=xml
    
    - name: Security scan
      run: |
        bandit -r digital_employee/
        safety check
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
    
    - name: Login to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ghcr.io
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Build and push
      uses: docker/build-push-action@v4
      with:
        context: .
        push: true
        tags: |
          ghcr.io/${{ github.repository }}:latest
          ghcr.io/${{ github.repository }}:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment..."
        # è¿™é‡Œæ·»åŠ stagingéƒ¨ç½²é€»è¾‘
    
    - name: Run integration tests
      run: |
        echo "Running integration tests on staging..."
        # è¿™é‡Œæ·»åŠ é›†æˆæµ‹è¯•é€»è¾‘
    
    - name: Deploy to production
      if: success()
      run: |
        echo "Deploying to production environment..."
        # è¿™é‡Œæ·»åŠ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²é€»è¾‘
```

### è“ç»¿éƒ¨ç½²è„šæœ¬
```bash
#!/bin/bash
# scripts/blue_green_deploy.sh

set -e

NEW_VERSION=$1
CURRENT_ENV=$(kubectl get service digital-employee-service -o jsonpath='{.spec.selector.version}')

if [ -z "$NEW_VERSION" ]; then
    echo "ç”¨æ³•: $0 <new_version>"
    exit 1
fi

echo "å½“å‰ç‰ˆæœ¬: $CURRENT_ENV"
echo "æ–°ç‰ˆæœ¬: $NEW_VERSION"

# ç¡®å®šæ–°ç¯å¢ƒé¢œè‰²
if [ "$CURRENT_ENV" = "blue" ]; then
    NEW_ENV="green"
else
    NEW_ENV="blue"
fi

echo "éƒ¨ç½²åˆ° $NEW_ENV ç¯å¢ƒ..."

# æ›´æ–°éƒ¨ç½²é…ç½®
kubectl set image deployment/digital-employee-$NEW_ENV-deployment \
    app=digital-employee:$NEW_VERSION \
    --namespace=digital-employee

# ç­‰å¾…éƒ¨ç½²å®Œæˆ
kubectl rollout status deployment/digital-employee-$NEW_ENV-deployment \
    --namespace=digital-employee \
    --timeout=600s

# å¥åº·æ£€æŸ¥
echo "æ‰§è¡Œå¥åº·æ£€æŸ¥..."
for i in {1..30}; do
    if kubectl exec -n digital-employee \
        deployment/digital-employee-$NEW_ENV-deployment \
        -- curl -f http://localhost:8000/health; then
        echo "å¥åº·æ£€æŸ¥é€šè¿‡"
        break
    fi
    
    if [ $i -eq 30 ]; then
        echo "å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œå›æ»šéƒ¨ç½²"
        kubectl rollout undo deployment/digital-employee-$NEW_ENV-deployment \
            --namespace=digital-employee
        exit 1
    fi
    
    echo "ç­‰å¾…æœåŠ¡å°±ç»ª... ($i/30)"
    sleep 10
done

# æ‰§è¡ŒçƒŸé›¾æµ‹è¯•
echo "æ‰§è¡ŒçƒŸé›¾æµ‹è¯•..."
if ! ./scripts/smoke_test.sh $NEW_ENV; then
    echo "çƒŸé›¾æµ‹è¯•å¤±è´¥ï¼Œå›æ»šéƒ¨ç½²"
    kubectl rollout undo deployment/digital-employee-$NEW_ENV-deployment \
        --namespace=digital-employee
    exit 1
fi

# åˆ‡æ¢æµé‡
echo "åˆ‡æ¢æµé‡åˆ° $NEW_ENV ç¯å¢ƒ..."
kubectl patch service digital-employee-service \
    -p '{"spec":{"selector":{"version":"'$NEW_ENV'"}}}' \
    --namespace=digital-employee

echo "éƒ¨ç½²å®Œæˆï¼Œæµé‡å·²åˆ‡æ¢åˆ°ç‰ˆæœ¬ $NEW_VERSION ($NEW_ENV ç¯å¢ƒ)"

# å¯é€‰ï¼šä¿ç•™æ—§ç‰ˆæœ¬ä¸€æ®µæ—¶é—´åæ¸…ç†
echo "æ—§ç‰ˆæœ¬ ($CURRENT_ENV) å°†ä¿ç•™24å°æ—¶åè‡ªåŠ¨æ¸…ç†"
```

---

## ğŸ”§ è¿ç»´å·¥å…·å’Œè„šæœ¬

### ç³»ç»Ÿå¥åº·æ£€æŸ¥è„šæœ¬
```python
#!/usr/bin/env python3
# scripts/health_check.py

import asyncio
import aiohttp
import psutil
import redis
import psycopg2
from datetime import datetime
import json
import logging

class SystemHealthChecker:
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥å™¨"""
    
    def __init__(self):
        self.results = {}
        self.logger = logging.getLogger(__name__)
    
    async def check_all(self):
        """æ‰§è¡Œæ‰€æœ‰å¥åº·æ£€æŸ¥"""
        checks = [
            self.check_application_health(),
            self.check_database_health(),
            self.check_redis_health(),
            self.check_ai_service_health(),
            self.check_system_resources(),
            self.check_disk_space(),
        ]
        
        results = await asyncio.gather(*checks, return_exceptions=True)
        
        # æ±‡æ€»ç»“æœ
        health_status = {
            "timestamp": datetime.now().isoformat(),
            "overall_status": "healthy",
            "checks": {}
        }
        
        check_names = [
            "application", "database", "redis", 
            "ai_service", "system_resources", "disk_space"
        ]
        
        for i, result in enumerate(results):
            check_name = check_names[i]
            if isinstance(result, Exception):
                health_status["checks"][check_name] = {
                    "status": "error",
                    "error": str(result)
                }
                health_status["overall_status"] = "unhealthy"
            else:
                health_status["checks"][check_name] = result
                if not result.get("healthy", False):
                    health_status["overall_status"] = "unhealthy"
        
        return health_status
    
    async def check_application_health(self):
        """æ£€æŸ¥åº”ç”¨ç¨‹åºå¥åº·çŠ¶æ€"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get('http://localhost:8000/health', timeout=10) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        return {
                            "healthy": True,
                            "response_time": resp.headers.get('X-Response-Time', 'unknown'),
                            "version": data.get('version', 'unknown')
                        }
                    else:
                        return {
                            "healthy": False,
                            "error": f"HTTP {resp.status}"
                        }
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e)
            }
    
    async def check_database_health(self):
        """æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶æ€"""
        try:
            conn = psycopg2.connect(
                host="localhost",
                port=5432,
                database="digital_employee",
                user="user",
                password=os.getenv("DB_PASSWORD")
            )
            
            cursor = conn.cursor()
            cursor.execute("SELECT 1")
            result = cursor.fetchone()
            
            cursor.execute("SELECT count(*) FROM pg_stat_activity WHERE state = 'active'")
            active_connections = cursor.fetchone()[0]
            
            cursor.close()
            conn.close()
            
            return {
                "healthy": True,
                "active_connections": active_connections,
                "max_connections": 100  # ä»é…ç½®è·å–
            }
            
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e)
            }
    
    async def check_redis_health(self):
        """æ£€æŸ¥Rediså¥åº·çŠ¶æ€"""
        try:
            r = redis.Redis(host='localhost', port=6379, db=0)
            info = r.info()
            
            return {
                "healthy": True,
                "used_memory": info['used_memory_human'],
                "connected_clients": info['connected_clients'],
                "uptime": info['uptime_in_seconds']
            }
            
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e)
            }
    
    async def check_ai_service_health(self):
        """æ£€æŸ¥AIæœåŠ¡å¥åº·çŠ¶æ€"""
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ å¯¹OpenAI APIçš„ç®€å•æµ‹è¯•
            async with aiohttp.ClientSession() as session:
                headers = {
                    "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}",
                    "Content-Type": "application/json"
                }
                
                # å‘é€ç®€å•çš„APIè¯·æ±‚æµ‹è¯•è¿é€šæ€§
                data = {
                    "model": "gpt-3.5-turbo",
                    "messages": [{"role": "user", "content": "test"}],
                    "max_tokens": 5
                }
                
                async with session.post(
                    'https://api.openai.com/v1/chat/completions',
                    headers=headers,
                    json=data,
                    timeout=10
                ) as resp:
                    if resp.status in [200, 429]:  # 429æ˜¯é¢‘ç‡é™åˆ¶ï¼Œä½†è¯´æ˜æœåŠ¡å¯ç”¨
                        return {
                            "healthy": True,
                            "api_status": "available"
                        }
                    else:
                        return {
                            "healthy": False,
                            "error": f"API returned {resp.status}"
                        }
                        
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e)
            }
    
    async def check_system_resources(self):
        """æ£€æŸ¥ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            load_avg = psutil.getloadavg()
            
            # å¥åº·é˜ˆå€¼
            cpu_threshold = 80
            memory_threshold = 80
            
            is_healthy = (
                cpu_percent < cpu_threshold and 
                memory.percent < memory_threshold
            )
            
            return {
                "healthy": is_healthy,
                "cpu_percent": cpu_percent,
                "memory_percent": memory.percent,
                "load_average": load_avg,
                "warnings": [
                    f"High CPU usage: {cpu_percent}%" if cpu_percent >= cpu_threshold else None,
                    f"High memory usage: {memory.percent}%" if memory.percent >= memory_threshold else None
                ]
            }
            
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e)
            }
    
    async def check_disk_space(self):
        """æ£€æŸ¥ç£ç›˜ç©ºé—´"""
        try:
            disk_usage = psutil.disk_usage('/')
            used_percent = (disk_usage.used / disk_usage.total) * 100
            
            # ç£ç›˜ç©ºé—´é˜ˆå€¼
            disk_threshold = 85
            
            return {
                "healthy": used_percent < disk_threshold,
                "used_percent": used_percent,
                "free_space_gb": disk_usage.free / (1024**3),
                "total_space_gb": disk_usage.total / (1024**3)
            }
            
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e)
            }

async def main():
    """ä¸»å‡½æ•°"""
    checker = SystemHealthChecker()
    health_status = await checker.check_all()
    
    # è¾“å‡ºç»“æœ
    print(json.dumps(health_status, indent=2))
    
    # å¦‚æœä¸å¥åº·ï¼Œé€€å‡ºç ä¸º1
    if health_status["overall_status"] != "healthy":
        exit(1)

if __name__ == "__main__":
    asyncio.run(main())
```

### æ—¥å¿—åˆ†æè„šæœ¬
```python
#!/usr/bin/env python3
# scripts/log_analyzer.py

import re
import json
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import argparse

class LogAnalyzer:
    """æ—¥å¿—åˆ†æå·¥å…·"""
    
    def __init__(self, log_file):
        self.log_file = log_file
        self.patterns = {
            'timestamp': r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})',
            'level': r'"level":\s*"(\w+)"',
            'message': r'"message":\s*"([^"]+)"',
            'response_time': r'"response_time":\s*([0-9.]+)',
            'status_code': r'"status_code":\s*(\d+)',
            'ai_tokens': r'"ai_tokens_used":\s*(\d+)',
            'error': r'"error":\s*"([^"]+)"'
        }
    
    def analyze_logs(self, hours=24):
        """åˆ†ææ—¥å¿—æ–‡ä»¶"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        stats = {
            'total_requests': 0,
            'error_count': 0,
            'response_times': [],
            'status_codes': Counter(),
            'error_messages': Counter(),
            'ai_token_usage': 0,
            'hourly_distribution': defaultdict(int),
            'slow_requests': []
        }
        
        with open(self.log_file, 'r') as f:
            for line in f:
                try:
                    # è§£ææ—¶é—´æˆ³
                    timestamp_match = re.search(self.patterns['timestamp'], line)
                    if not timestamp_match:
                        continue
                    
                    log_time = datetime.fromisoformat(timestamp_match.group(1))
                    if log_time < cutoff_time:
                        continue
                    
                    # ç»Ÿè®¡æ¯å°æ—¶åˆ†å¸ƒ
                    hour_key = log_time.strftime('%Y-%m-%d %H:00')
                    stats['hourly_distribution'][hour_key] += 1
                    
                    # è§£æå…¶ä»–å­—æ®µ
                    level_match = re.search(self.patterns['level'], line)
                    if level_match and level_match.group(1) in ['INFO', 'ERROR', 'WARNING']:
                        stats['total_requests'] += 1
                        
                        # é”™è¯¯ç»Ÿè®¡
                        if level_match.group(1) == 'ERROR':
                            stats['error_count'] += 1
                            
                            error_match = re.search(self.patterns['error'], line)
                            if error_match:
                                stats['error_messages'][error_match.group(1)] += 1
                        
                        # å“åº”æ—¶é—´ç»Ÿè®¡
                        response_time_match = re.search(self.patterns['response_time'], line)
                        if response_time_match:
                            response_time = float(response_time_match.group(1))
                            stats['response_times'].append(response_time)
                            
                            # è®°å½•æ…¢è¯·æ±‚
                            if response_time > 5.0:
                                message_match = re.search(self.patterns['message'], line)
                                stats['slow_requests'].append({
                                    'timestamp': log_time.isoformat(),
                                    'response_time': response_time,
                                    'message': message_match.group(1) if message_match else 'unknown'
                                })
                        
                        # çŠ¶æ€ç ç»Ÿè®¡
                        status_match = re.search(self.patterns['status_code'], line)
                        if status_match:
                            stats['status_codes'][status_match.group(1)] += 1
                        
                        # AI Tokenä½¿ç”¨ç»Ÿè®¡
                        tokens_match = re.search(self.patterns['ai_tokens'], line)
                        if tokens_match:
                            stats['ai_token_usage'] += int(tokens_match.group(1))
                            
                except Exception as e:
                    print(f"è§£ææ—¥å¿—è¡Œæ—¶å‡ºé”™: {e}")
                    continue
        
        return self.generate_report(stats)
    
    def generate_report(self, stats):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report = {
            'analysis_time': datetime.now().isoformat(),
            'summary': {
                'total_requests': stats['total_requests'],
                'error_rate': stats['error_count'] / max(stats['total_requests'], 1) * 100,
                'total_ai_tokens': stats['ai_token_usage']
            },
            'performance': {},
            'errors': {
                'count': stats['error_count'],
                'top_errors': dict(stats['error_messages'].most_common(10))
            },
            'traffic': {
                'hourly_distribution': dict(stats['hourly_distribution'])
            },
            'slow_requests': stats['slow_requests'][-10:]  # æœ€è¿‘10ä¸ªæ…¢è¯·æ±‚
        }
        
        # å“åº”æ—¶é—´ç»Ÿè®¡
        if stats['response_times']:
            response_times = sorted(stats['response_times'])
            count = len(response_times)
            
            report['performance'] = {
                'avg_response_time': sum(response_times) / count,
                'p50_response_time': response_times[int(count * 0.5)],
                'p95_response_time': response_times[int(count * 0.95)],
                'p99_response_time': response_times[int(count * 0.99)],
                'max_response_time': max(response_times)
            }
        
        return report

def main():
    parser = argparse.ArgumentParser(description='æ—¥å¿—åˆ†æå·¥å…·')
    parser.add_argument('--log-file', required=True, help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--hours', type=int, default=24, help='åˆ†ææœ€è¿‘Nå°æ—¶çš„æ—¥å¿—')
    parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    analyzer = LogAnalyzer(args.log_file)
    report = analyzer.analyze_logs(args.hours)
    
    if args.output:
        with open(args.output, 'w') as f:
            json.dump(report, f, indent=2)
        print(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
    else:
        print(json.dumps(report, indent=2))

if __name__ == "__main__":
    main()
```

---

## ğŸ“‹ è¿ç»´æ£€æŸ¥æ¸…å•

### éƒ¨ç½²å‰æ£€æŸ¥æ¸…å•
- [ ] **ä»£ç è´¨é‡**ï¼šé€šè¿‡æ‰€æœ‰æµ‹è¯•ï¼Œä»£ç å®¡æŸ¥å®Œæˆ
- [ ] **å®‰å…¨æ£€æŸ¥**ï¼šé€šè¿‡å®‰å…¨æ‰«æï¼Œæ— é«˜å±æ¼æ´
- [ ] **é…ç½®éªŒè¯**ï¼šç¯å¢ƒå˜é‡å’Œé…ç½®æ–‡ä»¶æ­£ç¡®
- [ ] **æ•°æ®åº“è¿ç§»**ï¼šæ•°æ®åº“è„šæœ¬æµ‹è¯•é€šè¿‡
- [ ] **ä¾èµ–æ£€æŸ¥**ï¼šæ‰€æœ‰å¤–éƒ¨æœåŠ¡å¯ç”¨
- [ ] **å¤‡ä»½ç¡®è®¤**ï¼šå½“å‰æ•°æ®å·²å¤‡ä»½
- [ ] **å›æ»šæ–¹æ¡ˆ**ï¼šå›æ»šæ­¥éª¤æ˜ç¡®ï¼Œå·²æµ‹è¯•
- [ ] **ç›‘æ§å‡†å¤‡**ï¼šç›‘æ§å’Œå‘Šè­¦é…ç½®å°±ç»ª
- [ ] **æ–‡æ¡£æ›´æ–°**ï¼šéƒ¨ç½²æ–‡æ¡£å’Œè¿ç»´æ‰‹å†Œæ›´æ–°

### æ—¥å¸¸è¿ç»´æ£€æŸ¥æ¸…å•
- [ ] **ç³»ç»Ÿèµ„æº**ï¼šCPUã€å†…å­˜ã€ç£ç›˜ä½¿ç”¨æ­£å¸¸
- [ ] **æœåŠ¡çŠ¶æ€**ï¼šæ‰€æœ‰æœåŠ¡è¿è¡Œæ­£å¸¸ï¼Œæ— å¼‚å¸¸é‡å¯
- [ ] **å“åº”æ—¶é—´**ï¼šAPIå“åº”æ—¶é—´åœ¨æ­£å¸¸èŒƒå›´å†…
- [ ] **é”™è¯¯ç‡**ï¼šé”™è¯¯ç‡ä½äºå‘Šè­¦é˜ˆå€¼
- [ ] **AIæœåŠ¡**ï¼šAI APIè°ƒç”¨æˆåŠŸç‡æ­£å¸¸
- [ ] **æ•°æ®åº“**ï¼šæ•°æ®åº“è¿æ¥æ± æ­£å¸¸ï¼Œæ…¢æŸ¥è¯¢æ£€æŸ¥
- [ ] **å¤‡ä»½çŠ¶æ€**ï¼šè‡ªåŠ¨å¤‡ä»½æ‰§è¡ŒæˆåŠŸ
- [ ] **ç£ç›˜ç©ºé—´**ï¼šæ—¥å¿—æ–‡ä»¶å’Œæ•°æ®åº“ç©ºé—´å……è¶³
- [ ] **å®‰å…¨æ—¥å¿—**ï¼šæ£€æŸ¥å¼‚å¸¸è®¿é—®å’Œå®‰å…¨äº‹ä»¶

### æ•…éšœå“åº”æ£€æŸ¥æ¸…å•
- [ ] **é—®é¢˜ç¡®è®¤**ï¼šç¡®è®¤é—®é¢˜èŒƒå›´å’Œå½±å“ç¨‹åº¦
- [ ] **å‘Šè­¦é€šçŸ¥**ï¼šç›¸å…³äººå‘˜å·²æ”¶åˆ°é€šçŸ¥
- [ ] **ç”¨æˆ·é€šä¿¡**ï¼šå¦‚éœ€è¦ï¼Œå·²é€šçŸ¥ç”¨æˆ·
- [ ] **ä¸´æ—¶æªæ–½**ï¼šå®æ–½ä¸´æ—¶ç¼“è§£æªæ–½
- [ ] **æ ¹å› åˆ†æ**ï¼šåˆ†æé—®é¢˜æ ¹æœ¬åŸå› 
- [ ] **ä¿®å¤æ–¹æ¡ˆ**ï¼šåˆ¶å®šå¹¶å®æ–½ä¿®å¤æ–¹æ¡ˆ
- [ ] **éªŒè¯æ¢å¤**ï¼šç¡®è®¤ç³»ç»Ÿå®Œå…¨æ¢å¤
- [ ] **æ€»ç»“æŠ¥å‘Š**ï¼šç¼–å†™æ•…éšœæ€»ç»“æŠ¥å‘Š
- [ ] **é¢„é˜²æªæ–½**ï¼šåˆ¶å®šé¢„é˜²ç±»ä¼¼é—®é¢˜çš„æªæ–½

---

*éƒ¨ç½²å’Œè¿ç»´è§„èŒƒæ–‡æ¡£ç‰ˆæœ¬ï¼šv1.0*  
*åˆ›å»ºæ—¶é—´ï¼š2025-07-31*  
*ç»´æŠ¤è€…ï¼šæ•°å­—å‘˜å·¥ç³»ç»Ÿè¿ç»´å›¢é˜Ÿ*