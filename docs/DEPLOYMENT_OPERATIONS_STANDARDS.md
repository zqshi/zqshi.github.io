# Êï∞Â≠óÂëòÂ∑•Á≥ªÁªüÈÉ®ÁΩ≤ÂíåËøêÁª¥ËßÑËåÉ

## üéØ ËøêÁª¥ÁêÜÂøµ

### Ê†∏ÂøÉÂéüÂàô
- **Ê∏êËøõÂºèÈÉ®ÁΩ≤**Ôºö‰ªéÊú¨Âú∞ÂºÄÂèëÂà∞Áîü‰∫ßÁéØÂ¢ÉÁöÑÂπ≥ÊªëËøáÊ∏°
- **ÁõëÊéß‰ºòÂÖà**ÔºöÊØè‰∏™ÁªÑ‰ª∂ÈÉΩÊúâÂÆåÊï¥ÁöÑÁõëÊéßË¶ÜÁõñ
- **Ëá™Âä®ÂåñËøêÁª¥**ÔºöÂáèÂ∞ë‰∫∫Â∑•Êìç‰ΩúÔºåÊèêÈ´òÂèØÈù†ÊÄß
- **ÊïÖÈöúËá™ÊÑà**ÔºöÁ≥ªÁªüÂÖ∑Â§áËá™Âä®ÊÅ¢Â§çËÉΩÂäõ
- **ÊàêÊú¨‰ºòÂåñ**ÔºöÂú®ÊÄßËÉΩÂíåÊàêÊú¨‰πãÈó¥ÊâæÂà∞ÊúÄ‰Ω≥Âπ≥Ë°°

### AIÁ≥ªÁªüËøêÁª¥ÁâπÁÇπ
- **Â§ñÈÉ®‰æùËµñÈáç**ÔºöÈ´òÂ∫¶‰æùËµñÁ¨¨‰∏âÊñπAIÊúçÂä°
- **ÊÄßËÉΩÊ≥¢Âä®Â§ß**ÔºöAIÂìçÂ∫îÊó∂Èó¥‰∏çÁ®≥ÂÆö
- **ÊàêÊú¨ÊïèÊÑü**ÔºöToken‰ΩøÁî®Áõ¥Êé•ÂΩ±ÂìçËøêËê•ÊàêÊú¨
- **Ë¥®ÈáèÁõëÊéßÂ§çÊùÇ**ÔºöÈúÄË¶ÅÁõëÊéßAIÂìçÂ∫îË¥®Èáè

---

## üèóÔ∏è ÈÉ®ÁΩ≤Êû∂ÊûÑËÆæËÆ°

### ÁéØÂ¢ÉÂàÜÂ±ÇÁ≠ñÁï•
```
ÂºÄÂèëÁéØÂ¢É (Development) ‚Üí ÊµãËØïÁéØÂ¢É (Testing) ‚Üí È¢ÑÂèëÁéØÂ¢É (Staging) ‚Üí Áîü‰∫ßÁéØÂ¢É (Production)
```

#### ÁéØÂ¢ÉÈÖçÁΩÆÂØπÊØî
| ÁªÑ‰ª∂ | ÂºÄÂèëÁéØÂ¢É | ÊµãËØïÁéØÂ¢É | È¢ÑÂèëÁéØÂ¢É | Áîü‰∫ßÁéØÂ¢É |
|------|----------|----------|----------|----------|
| **AIÊúçÂä°** | Mock/Sandbox | Sandbox | Áîü‰∫ßAPI | Áîü‰∫ßAPI |
| **Êï∞ÊçÆÂ∫ì** | SQLite | PostgreSQL | PostgreSQL | PostgreSQLÈõÜÁæ§ |
| **ÁºìÂ≠ò** | ÂÜÖÂ≠ò | RedisÂçïÂÆû‰æã | RedisÂçïÂÆû‰æã | RedisÈõÜÁæ§ |
| **Ë¥üËΩΩÂùáË°°** | Êó† | Êó† | Nginx | Nginx+Keepalived |
| **ÁõëÊéß** | Âü∫Á°ÄÊó•Âøó | Âü∫Á°ÄÁõëÊéß | ÂÆåÊï¥ÁõëÊéß | ÂÆåÊï¥ÁõëÊéß+ÂëäË≠¶ |
| **ÂÆπÂô®Âåñ** | Docker | Docker | Docker | Kubernetes |

### ÈÉ®ÁΩ≤Êû∂ÊûÑÂõæ
```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ              Ë¥üËΩΩÂùáË°°Â±Ç              ‚îÇ
                    ‚îÇ     Nginx + SSLÁªàÁ´Ø + ÈôêÊµÅ         ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ              Â∫îÁî®Â±Ç                 ‚îÇ
                    ‚îÇ  FastAPIÊúçÂä°ÈõÜÁæ§ (3‰∏™ÂÆû‰æã)         ‚îÇ
                    ‚îÇ  ‚îú‚îÄ Agent Runtime                   ‚îÇ
                    ‚îÇ  ‚îú‚îÄ AI Service Layer                ‚îÇ
                    ‚îÇ  ‚îî‚îÄ Task Orchestrator               ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                                 ‚îÇ                                 ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Êï∞ÊçÆÂ±Ç          ‚îÇ    ‚îÇ    ÁºìÂ≠òÂ±Ç       ‚îÇ    ‚îÇ      Â§ñÈÉ®ÊúçÂä°Â±Ç       ‚îÇ
‚îÇ  PostgreSQL‰∏ª‰ªé      ‚îÇ    ‚îÇ  RedisÈõÜÁæ§      ‚îÇ    ‚îÇ  OpenAI/Claude API   ‚îÇ
‚îÇ  ‚îú‚îÄ ‰∏ªÂ∫ì(ÂÜô)        ‚îÇ    ‚îÇ  ‚îú‚îÄ ‰ºöËØùÁºìÂ≠ò    ‚îÇ    ‚îÇ  ‚îú‚îÄ Â§öÂéÇÂïÜÂ§á‰ªΩ        ‚îÇ
‚îÇ  ‚îú‚îÄ ‰ªéÂ∫ì(ËØª)        ‚îÇ    ‚îÇ  ‚îú‚îÄ ÁªìÊûúÁºìÂ≠ò    ‚îÇ    ‚îÇ  ‚îú‚îÄ ÈôêÊµÅÂíåÈáçËØï        ‚îÇ
‚îÇ  ‚îî‚îÄ Â§á‰ªΩÂ≠òÂÇ®        ‚îÇ    ‚îÇ  ‚îî‚îÄ ÈòüÂàóÂ≠òÂÇ®    ‚îÇ    ‚îÇ  ‚îî‚îÄ ÂÅ•Â∫∑Ê£ÄÊü•          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üê≥ ÂÆπÂô®ÂåñÈÉ®ÁΩ≤ËßÑËåÉ

### DockerfileËßÑËåÉ
```dockerfile
# Áîü‰∫ßÁéØÂ¢ÉDockerfile
FROM python:3.11-slim

# ËÆæÁΩÆÂ∑•‰ΩúÁõÆÂΩï
WORKDIR /app

# ÂÆâË£ÖÁ≥ªÁªü‰æùËµñ
RUN apt-get update && apt-get install -y \
    curl \
    vim \
    && rm -rf /var/lib/apt/lists/*

# Â§çÂà∂‰æùËµñÊñá‰ª∂
COPY requirements.txt .
COPY requirements-prod.txt .

# ÂÆâË£ÖPython‰æùËµñ
RUN pip install --no-cache-dir -r requirements-prod.txt

# Â§çÂà∂Â∫îÁî®‰ª£Á†Å
COPY . .

# ÂàõÂª∫ÈùûrootÁî®Êà∑
RUN groupadd -r appuser && useradd -r -g appuser appuser
RUN chown -R appuser:appuser /app
USER appuser

# ÂÅ•Â∫∑Ê£ÄÊü•
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Êö¥Èú≤Á´ØÂè£
EXPOSE 8000

# ÂêØÂä®ÂëΩ‰ª§
CMD ["gunicorn", "run_server:app", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "-b", "0.0.0.0:8000"]
```

### Docker ComposeÈÖçÁΩÆ
```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  app:
    build:
      context: .
      dockerfile: Dockerfile.prod
    ports:
      - "8000:8000"
    environment:
      - ENV=production
      - DATABASE_URL=postgresql://user:${DB_PASSWORD}@db:5432/digital_employee
      - REDIS_URL=redis://redis:6379/0
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LOG_LEVEL=INFO
    depends_on:
      - db
      - redis
    restart: unless-stopped
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
      - ./nginx/logs:/var/log/nginx
    depends_on:
      - app
    restart: unless-stopped

  db:
    image: postgres:15
    environment:
      POSTGRES_DB: digital_employee
      POSTGRES_USER: user
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backups:/backups
    restart: unless-stopped
    ports:
      - "5432:5432"
    command: >
      postgres
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100

  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    restart: unless-stopped
    ports:
      - "6379:6379"

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:

networks:
  default:
    driver: bridge
```

---

## ‚ò∏Ô∏è KubernetesÈÉ®ÁΩ≤ËßÑËåÉ

### NamespaceÈÖçÁΩÆ
```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: digital-employee
  labels:
    app.kubernetes.io/name: digital-employee
    app.kubernetes.io/version: "1.0.0"
```

### ConfigMapÈÖçÁΩÆ
```yaml
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: digital-employee-config
  namespace: digital-employee
data:
  APP_ENV: "production"
  LOG_LEVEL: "INFO"
  DB_HOST: "postgresql-service"
  DB_PORT: "5432"
  DB_NAME: "digital_employee"
  REDIS_HOST: "redis-service"
  REDIS_PORT: "6379"
  
  # AIÊúçÂä°ÈÖçÁΩÆ
  AI_SERVICE_TIMEOUT: "30"
  AI_SERVICE_RETRY_COUNT: "3"
  AI_SERVICE_RETRY_DELAY: "2"
  
  # ÁõëÊéßÈÖçÁΩÆ
  PROMETHEUS_METRICS_PORT: "9000"
  HEALTH_CHECK_INTERVAL: "30"
```

### SecretÈÖçÁΩÆ
```yaml
# k8s/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: digital-employee-secrets
  namespace: digital-employee
type: Opaque
data:
  DB_PASSWORD: <base64-encoded-password>
  OPENAI_API_KEY: <base64-encoded-api-key>
  CLAUDE_API_KEY: <base64-encoded-api-key>
  JWT_SECRET_KEY: <base64-encoded-jwt-secret>
```

### DeploymentÈÖçÁΩÆ
```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: digital-employee-app
  namespace: digital-employee
  labels:
    app: digital-employee
    version: "1.0.0"
spec:
  replicas: 3
  selector:
    matchLabels:
      app: digital-employee
  template:
    metadata:
      labels:
        app: digital-employee
        version: "1.0.0"
    spec:
      containers:
      - name: app
        image: digital-employee:1.0.0
        ports:
        - containerPort: 8000
          name: http
        - containerPort: 9000
          name: metrics
        
        env:
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: digital-employee-secrets
              key: DB_PASSWORD
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: digital-employee-secrets
              key: OPENAI_API_KEY
        
        envFrom:
        - configMapRef:
            name: digital-employee-config
        
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 2Gi
        
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 15"]
      
      terminationGracePeriodSeconds: 30
      
      # Âèç‰∫≤ÂíåÊÄßÁ°Æ‰øùPodÂàÜÂ∏ÉÂú®‰∏çÂêåËäÇÁÇπ
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - digital-employee
              topologyKey: kubernetes.io/hostname
```

### ServiceÈÖçÁΩÆ
```yaml
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: digital-employee-service
  namespace: digital-employee
  labels:
    app: digital-employee
spec:
  selector:
    app: digital-employee
  ports:
  - name: http
    port: 80
    targetPort: 8000
  - name: metrics
    port: 9000
    targetPort: 9000
  type: ClusterIP
```

### IngressÈÖçÁΩÆ
```yaml
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: digital-employee-ingress
  namespace: digital-employee
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/rate-limit-window: "1m"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
spec:
  tls:
  - hosts:
    - api.digital-employee.example.com
    secretName: digital-employee-tls
  rules:
  - host: api.digital-employee.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: digital-employee-service
            port:
              number: 80
```

---

## üìä ÁõëÊéßÂíåÂèØËßÇÊµãÊÄß

### PrometheusÁõëÊéßÈÖçÁΩÆ
```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'digital-employee'
    static_configs:
      - targets: ['app:9000']
    metrics_path: /metrics
    scrape_interval: 30s
    
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres_exporter:9187']
    
  - job_name: 'redis'
    static_configs:
      - targets: ['redis_exporter:9121']
    
  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx_exporter:9113']

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

### ÂëäË≠¶ËßÑÂàôÈÖçÁΩÆ
```yaml
# monitoring/alert_rules.yml
groups:
- name: digital_employee_alerts
  rules:
  
  # ÊúçÂä°ÂèØÁî®ÊÄßÂëäË≠¶
  - alert: ServiceDown
    expr: up{job="digital-employee"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Êï∞Â≠óÂëòÂ∑•ÊúçÂä°ÂÆïÊú∫"
      description: "{{ $labels.instance }} ÊúçÂä°Â∑≤ÂÆïÊú∫Ë∂ÖËøá1ÂàÜÈíü"
  
  # ÂìçÂ∫îÊó∂Èó¥ÂëäË≠¶
  - alert: HighResponseTime
    expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 10
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "ÂìçÂ∫îÊó∂Èó¥ËøáÈ´ò"
      description: "95%ËØ∑Ê±ÇÂìçÂ∫îÊó∂Èó¥Ë∂ÖËøá10ÁßíÔºåÂΩìÂâçÂÄº: {{ $value }}Áßí"
  
  # AIÊúçÂä°ÈîôËØØÁéáÂëäË≠¶
  - alert: HighAIServiceErrorRate
    expr: rate(ai_requests_total{status="error"}[5m]) / rate(ai_requests_total[5m]) > 0.05
    for: 3m
    labels:
      severity: warning
    annotations:
      summary: "AIÊúçÂä°ÈîôËØØÁéáËøáÈ´ò"
      description: "AIÊúçÂä°ÈîôËØØÁéáË∂ÖËøá5%ÔºåÂΩìÂâçÂÄº: {{ $value | humanizePercentage }}"
  
  # Token‰ΩøÁî®ÈáèÂëäË≠¶
  - alert: HighTokenUsage
    expr: increase(ai_tokens_used_total[1h]) > 100000
    for: 0m
    labels:
      severity: warning
    annotations:
      summary: "Token‰ΩøÁî®ÈáèËøáÈ´ò"
      description: "ËøáÂéª1Â∞èÊó∂Token‰ΩøÁî®Èáè: {{ $value }}"
  
  # Êï∞ÊçÆÂ∫ìËøûÊé•ÂëäË≠¶
  - alert: DatabaseConnectionsHigh
    expr: postgres_stat_database_numbackends > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Êï∞ÊçÆÂ∫ìËøûÊé•Êï∞ËøáÈ´ò"
      description: "Êï∞ÊçÆÂ∫ìËøûÊé•Êï∞: {{ $value }}/100"
  
  # ÂÜÖÂ≠ò‰ΩøÁî®ÂëäË≠¶
  - alert: HighMemoryUsage
    expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) > 0.8
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "ÂÜÖÂ≠ò‰ΩøÁî®ÁéáËøáÈ´ò"
      description: "ÂÆπÂô®ÂÜÖÂ≠ò‰ΩøÁî®Áéá: {{ $value | humanizePercentage }}"
```

### Grafana‰ª™Ë°®ÊùøÈÖçÁΩÆ
```json
{
  "dashboard": {
    "id": null,
    "title": "Êï∞Â≠óÂëòÂ∑•Á≥ªÁªüÁõëÊéß",
    "tags": ["digital-employee"],
    "timezone": "browser",
    "panels": [
      {
        "title": "ÊúçÂä°Ê¶ÇËßà",
        "type": "stat",
        "targets": [
          {
            "expr": "up{job=\"digital-employee\"}",
            "legendFormat": "ÊúçÂä°Áä∂ÊÄÅ"
          }
        ]
      },
      {
        "title": "ËØ∑Ê±ÇQPS",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{endpoint}}"
          }
        ]
      },
      {
        "title": "ÂìçÂ∫îÊó∂Èó¥ÂàÜÂ∏É",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "P50"
          },
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "P95"
          },
          {
            "expr": "histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "P99"
          }
        ]
      },
      {
        "title": "AIÊúçÂä°Ë∞ÉÁî®",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(ai_requests_total[5m])",
            "legendFormat": "{{service}} {{status}}"
          }
        ]
      },
      {
        "title": "Token‰ΩøÁî®Èáè",
        "type": "graph",
        "targets": [
          {
            "expr": "increase(ai_tokens_used_total[1h])",
            "legendFormat": "{{service}} {{model}}"
          }
        ]
      }
    ]
  }
}
```

---

## üîê ÂÆâÂÖ®ÈÖçÁΩÆËßÑËåÉ

### SSL/TLSÈÖçÁΩÆ
```nginx
# nginx/ssl.conf
server {
    listen 443 ssl http2;
    server_name api.digital-employee.example.com;
    
    # SSLÈÖçÁΩÆ
    ssl_certificate /etc/nginx/ssl/cert.pem;
    ssl_certificate_key /etc/nginx/ssl/key.pem;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384;
    ssl_prefer_server_ciphers off;
    
    # ÂÆâÂÖ®Â§¥
    add_header Strict-Transport-Security "max-age=63072000" always;
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Referrer-Policy "strict-origin-when-cross-origin";
    
    # ÈôêÊµÅÈÖçÁΩÆ
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req zone=api burst=20 nodelay;
    
    # ‰ª£ÁêÜÈÖçÁΩÆ
    location / {
        proxy_pass http://app:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Ë∂ÖÊó∂ÈÖçÁΩÆ
        proxy_connect_timeout 30s;
        proxy_send_timeout 30s;
        proxy_read_timeout 30s;
    }
}
```

### Â∫îÁî®ÂÆâÂÖ®ÈÖçÁΩÆ
```python
# config/security.py
from functools import wraps
import jwt
import time
from typing import Optional

class SecurityConfig:
    """ÂÆâÂÖ®ÈÖçÁΩÆÁ±ª"""
    
    def __init__(self):
        self.jwt_secret = os.getenv("JWT_SECRET_KEY")
        self.jwt_algorithm = "HS256"
        self.jwt_expiration = 3600  # 1Â∞èÊó∂
        self.rate_limit_requests = 100
        self.rate_limit_window = 60  # 1ÂàÜÈíü
        
        # APIÂØÜÈí•ÁôΩÂêçÂçï
        self.api_key_whitelist = set(os.getenv("API_KEY_WHITELIST", "").split(","))
        
        # ÊïèÊÑü‰ø°ÊÅØËøáÊª§
        self.sensitive_patterns = [
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',  # Email
            r'\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b',  # ‰ø°Áî®Âç°Âè∑
            r'\b\d{3}-\d{2}-\d{4}\b',  # SSN
        ]

def require_api_key(f):
    """APIÂØÜÈí•È™åËØÅË£ÖÈ•∞Âô®"""
    @wraps(f)
    async def wrapper(*args, **kwargs):
        request = kwargs.get('request') or args[0]
        api_key = request.headers.get('X-API-Key')
        
        if not api_key or api_key not in security_config.api_key_whitelist:
            raise HTTPException(status_code=401, detail="Invalid API key")
        
        return await f(*args, **kwargs)
    return wrapper

def rate_limit(max_requests: int = 100, window_seconds: int = 60):
    """È¢ëÁéáÈôêÂà∂Ë£ÖÈ•∞Âô®"""
    def decorator(f):
        @wraps(f)
        async def wrapper(*args, **kwargs):
            request = kwargs.get('request') or args[0]
            client_ip = request.client.host
            
            # Ê£ÄÊü•È¢ëÁéáÈôêÂà∂
            if not check_rate_limit(client_ip, max_requests, window_seconds):
                raise HTTPException(status_code=429, detail="Rate limit exceeded")
            
            return await f(*args, **kwargs)
        return wrapper
    return decorator

def check_rate_limit(client_ip: str, max_requests: int, window_seconds: int) -> bool:
    """Ê£ÄÊü•È¢ëÁéáÈôêÂà∂"""
    current_time = int(time.time())
    window_start = current_time - window_seconds
    
    # ‰ΩøÁî®RedisÂÆûÁé∞ÊªëÂä®Á™óÂè£È¢ëÁéáÈôêÂà∂
    redis_key = f"rate_limit:{client_ip}"
    
    # Ê∏ÖÁêÜËøáÊúüËÆ∞ÂΩï
    redis_client.zremrangebyscore(redis_key, 0, window_start)
    
    # Ê£ÄÊü•ÂΩìÂâçËØ∑Ê±ÇÊï∞
    current_requests = redis_client.zcard(redis_key)
    
    if current_requests >= max_requests:
        return False
    
    # ËÆ∞ÂΩïÂΩìÂâçËØ∑Ê±Ç
    redis_client.zadd(redis_key, {str(current_time): current_time})
    redis_client.expire(redis_key, window_seconds)
    
    return True
```

---

## üóÑÔ∏è Êï∞ÊçÆÂ§á‰ªΩÂíåÊÅ¢Â§ç

### Êï∞ÊçÆÂ∫ìÂ§á‰ªΩÁ≠ñÁï•
```bash
#!/bin/bash
# scripts/backup_database.sh

set -e

# ÈÖçÁΩÆ
DB_HOST=${DB_HOST:-localhost}
DB_PORT=${DB_PORT:-5432}
DB_NAME=${DB_NAME:-digital_employee}
DB_USER=${DB_USER:-user}
BACKUP_DIR=${BACKUP_DIR:-/backups}
RETENTION_DAYS=${RETENTION_DAYS:-7}

# ÂàõÂª∫Â§á‰ªΩÁõÆÂΩï
mkdir -p $BACKUP_DIR

# ÁîüÊàêÂ§á‰ªΩÊñá‰ª∂Âêç
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="$BACKUP_DIR/db_backup_$TIMESTAMP.sql.gz"

echo "ÂºÄÂßãÊï∞ÊçÆÂ∫ìÂ§á‰ªΩ: $BACKUP_FILE"

# ÊâßË°åÂ§á‰ªΩ
PGPASSWORD=$DB_PASSWORD pg_dump \
  -h $DB_HOST \
  -p $DB_PORT \
  -U $DB_USER \
  -d $DB_NAME \
  --no-password \
  --format=custom \
  --compress=9 \
  --verbose \
  | gzip > $BACKUP_FILE

# È™åËØÅÂ§á‰ªΩÊñá‰ª∂
if [ -f "$BACKUP_FILE" ] && [ -s "$BACKUP_FILE" ]; then
    echo "Êï∞ÊçÆÂ∫ìÂ§á‰ªΩÊàêÂäü: $BACKUP_FILE"
    
    # ËÆ∞ÂΩïÂ§á‰ªΩÊó•Âøó
    echo "$(date): Database backup completed - $BACKUP_FILE" >> $BACKUP_DIR/backup.log
    
    # Ê∏ÖÁêÜËøáÊúüÂ§á‰ªΩ
    find $BACKUP_DIR -name "db_backup_*.sql.gz" -mtime +$RETENTION_DAYS -delete
    echo "Ê∏ÖÁêÜË∂ÖËøá $RETENTION_DAYS Â§©ÁöÑÊóßÂ§á‰ªΩ"
    
else
    echo "Êï∞ÊçÆÂ∫ìÂ§á‰ªΩÂ§±Ë¥•!"
    exit 1
fi

# ÂèØÈÄâÔºö‰∏ä‰º†Âà∞‰∫ëÂ≠òÂÇ®
if [ -n "$AWS_S3_BUCKET" ]; then
    aws s3 cp $BACKUP_FILE s3://$AWS_S3_BUCKET/backups/database/
    echo "Â§á‰ªΩÂ∑≤‰∏ä‰º†Âà∞S3: s3://$AWS_S3_BUCKET/backups/database/"
fi
```

### Êï∞ÊçÆÊÅ¢Â§çËÑöÊú¨
```bash
#!/bin/bash
# scripts/restore_database.sh

set -e

BACKUP_FILE=$1
DB_HOST=${DB_HOST:-localhost}
DB_PORT=${DB_PORT:-5432}
DB_NAME=${DB_NAME:-digital_employee}
DB_USER=${DB_USER:-user}

if [ -z "$BACKUP_FILE" ]; then
    echo "Áî®Ê≥ï: $0 <backup_file>"
    echo "ÂèØÁî®Â§á‰ªΩÊñá‰ª∂:"
    ls -la /backups/db_backup_*.sql.gz
    exit 1
fi

if [ ! -f "$BACKUP_FILE" ]; then
    echo "Â§á‰ªΩÊñá‰ª∂‰∏çÂ≠òÂú®: $BACKUP_FILE"
    exit 1
fi

echo "Ë≠¶Âëä: Ê≠§Êìç‰ΩúÂ∞ÜË¶ÜÁõñÊï∞ÊçÆÂ∫ì $DB_NAME ÁöÑÊâÄÊúâÊï∞ÊçÆ!"
read -p "Á°ÆËÆ§ÁªßÁª≠? (y/N): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "Êìç‰ΩúÂ∑≤ÂèñÊ∂à"
    exit 1
fi

echo "ÂºÄÂßãÊï∞ÊçÆÂ∫ìÊÅ¢Â§ç: $BACKUP_FILE"

# ÂÅúÊ≠¢Â∫îÁî®ÊúçÂä°
echo "ÂÅúÊ≠¢Â∫îÁî®ÊúçÂä°..."
docker-compose stop app

# ÊÅ¢Â§çÊï∞ÊçÆÂ∫ì
echo "ÊÅ¢Â§çÊï∞ÊçÆÂ∫ì..."
gunzip -c $BACKUP_FILE | PGPASSWORD=$DB_PASSWORD psql \
  -h $DB_HOST \
  -p $DB_PORT \
  -U $DB_USER \
  -d $DB_NAME

echo "Êï∞ÊçÆÂ∫ìÊÅ¢Â§çÂÆåÊàê"

# ÈáçÂêØÂ∫îÁî®ÊúçÂä°
echo "ÈáçÂêØÂ∫îÁî®ÊúçÂä°..."
docker-compose start app

echo "Êï∞ÊçÆÊÅ¢Â§çÂÆåÊàê: $BACKUP_FILE"
```

### Ëá™Âä®Â§á‰ªΩCron‰ªªÂä°
```bash
# crontabÈÖçÁΩÆ
# ÊØèÂ§©ÂáåÊô®2ÁÇπÊâßË°åÊï∞ÊçÆÂ∫ìÂ§á‰ªΩ
0 2 * * * /opt/digital-employee/scripts/backup_database.sh >> /var/log/backup.log 2>&1

# ÊØèÂë®Êó•ÂáåÊô®1ÁÇπÊâßË°åÂÆåÊï¥Á≥ªÁªüÂ§á‰ªΩ
0 1 * * 0 /opt/digital-employee/scripts/full_system_backup.sh >> /var/log/backup.log 2>&1

# ÊØèÂ∞èÊó∂Ê£ÄÊü•Á≥ªÁªüÂÅ•Â∫∑Áä∂ÊÄÅ
0 * * * * /opt/digital-employee/scripts/health_check.sh >> /var/log/health.log 2>&1
```

---

## üöÄ Ëá™Âä®ÂåñÈÉ®ÁΩ≤ÊµÅÁ®ã

### CI/CD PipelineÈÖçÁΩÆ
```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [main]
    tags: ['v*']

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: Run tests
      run: |
        pytest tests/ -v --cov=digital_employee --cov-report=xml
    
    - name: Security scan
      run: |
        bandit -r digital_employee/
        safety check
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
    
    - name: Login to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ghcr.io
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Build and push
      uses: docker/build-push-action@v4
      with:
        context: .
        push: true
        tags: |
          ghcr.io/${{ github.repository }}:latest
          ghcr.io/${{ github.repository }}:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment..."
        # ËøôÈáåÊ∑ªÂä†stagingÈÉ®ÁΩ≤ÈÄªËæë
    
    - name: Run integration tests
      run: |
        echo "Running integration tests on staging..."
        # ËøôÈáåÊ∑ªÂä†ÈõÜÊàêÊµãËØïÈÄªËæë
    
    - name: Deploy to production
      if: success()
      run: |
        echo "Deploying to production environment..."
        # ËøôÈáåÊ∑ªÂä†Áîü‰∫ßÁéØÂ¢ÉÈÉ®ÁΩ≤ÈÄªËæë
```

### ËìùÁªøÈÉ®ÁΩ≤ËÑöÊú¨
```bash
#!/bin/bash
# scripts/blue_green_deploy.sh

set -e

NEW_VERSION=$1
CURRENT_ENV=$(kubectl get service digital-employee-service -o jsonpath='{.spec.selector.version}')

if [ -z "$NEW_VERSION" ]; then
    echo "Áî®Ê≥ï: $0 <new_version>"
    exit 1
fi

echo "ÂΩìÂâçÁâàÊú¨: $CURRENT_ENV"
echo "Êñ∞ÁâàÊú¨: $NEW_VERSION"

# Á°ÆÂÆöÊñ∞ÁéØÂ¢ÉÈ¢úËâ≤
if [ "$CURRENT_ENV" = "blue" ]; then
    NEW_ENV="green"
else
    NEW_ENV="blue"
fi

echo "ÈÉ®ÁΩ≤Âà∞ $NEW_ENV ÁéØÂ¢É..."

# Êõ¥Êñ∞ÈÉ®ÁΩ≤ÈÖçÁΩÆ
kubectl set image deployment/digital-employee-$NEW_ENV-deployment \
    app=digital-employee:$NEW_VERSION \
    --namespace=digital-employee

# Á≠âÂæÖÈÉ®ÁΩ≤ÂÆåÊàê
kubectl rollout status deployment/digital-employee-$NEW_ENV-deployment \
    --namespace=digital-employee \
    --timeout=600s

# ÂÅ•Â∫∑Ê£ÄÊü•
echo "ÊâßË°åÂÅ•Â∫∑Ê£ÄÊü•..."
for i in {1..30}; do
    if kubectl exec -n digital-employee \
        deployment/digital-employee-$NEW_ENV-deployment \
        -- curl -f http://localhost:8000/health; then
        echo "ÂÅ•Â∫∑Ê£ÄÊü•ÈÄöËøá"
        break
    fi
    
    if [ $i -eq 30 ]; then
        echo "ÂÅ•Â∫∑Ê£ÄÊü•Â§±Ë¥•ÔºåÂõûÊªöÈÉ®ÁΩ≤"
        kubectl rollout undo deployment/digital-employee-$NEW_ENV-deployment \
            --namespace=digital-employee
        exit 1
    fi
    
    echo "Á≠âÂæÖÊúçÂä°Â∞±Áª™... ($i/30)"
    sleep 10
done

# ÊâßË°åÁÉüÈõæÊµãËØï
echo "ÊâßË°åÁÉüÈõæÊµãËØï..."
if ! ./scripts/smoke_test.sh $NEW_ENV; then
    echo "ÁÉüÈõæÊµãËØïÂ§±Ë¥•ÔºåÂõûÊªöÈÉ®ÁΩ≤"
    kubectl rollout undo deployment/digital-employee-$NEW_ENV-deployment \
        --namespace=digital-employee
    exit 1
fi

# ÂàáÊç¢ÊµÅÈáè
echo "ÂàáÊç¢ÊµÅÈáèÂà∞ $NEW_ENV ÁéØÂ¢É..."
kubectl patch service digital-employee-service \
    -p '{"spec":{"selector":{"version":"'$NEW_ENV'"}}}' \
    --namespace=digital-employee

echo "ÈÉ®ÁΩ≤ÂÆåÊàêÔºåÊµÅÈáèÂ∑≤ÂàáÊç¢Âà∞ÁâàÊú¨ $NEW_VERSION ($NEW_ENV ÁéØÂ¢É)"

# ÂèØÈÄâÔºö‰øùÁïôÊóßÁâàÊú¨‰∏ÄÊÆµÊó∂Èó¥ÂêéÊ∏ÖÁêÜ
echo "ÊóßÁâàÊú¨ ($CURRENT_ENV) Â∞Ü‰øùÁïô24Â∞èÊó∂ÂêéËá™Âä®Ê∏ÖÁêÜ"
```

---

## üîß ËøêÁª¥Â∑•ÂÖ∑ÂíåËÑöÊú¨

### Á≥ªÁªüÂÅ•Â∫∑Ê£ÄÊü•ËÑöÊú¨
```python
#!/usr/bin/env python3
# scripts/health_check.py

import asyncio
import aiohttp
import psutil
import redis
import psycopg2
from datetime import datetime
import json
import logging

class SystemHealthChecker:
    """Á≥ªÁªüÂÅ•Â∫∑Ê£ÄÊü•Âô®"""
    
    def __init__(self):
        self.results = {}
        self.logger = logging.getLogger(__name__)
    
    async def check_all(self):
        """ÊâßË°åÊâÄÊúâÂÅ•Â∫∑Ê£ÄÊü•"""
        checks = [
            self.check_application_health(),
            self.check_database_health(),
            self.check_redis_health(),
            self.check_ai_service_health(),
            self.check_system_resources(),
            self.check_disk_space(),
        ]
        
        results = await asyncio.gather(*checks, return_exceptions=True)
        
        # Ê±áÊÄªÁªìÊûú
        health_status = {
            "timestamp": datetime.now().isoformat(),
            "overall_status": "healthy",
            "checks": {}
        }
        
        check_names = [
            "application", "database", "redis", 
            "ai_service", "system_resources", "disk_space"
        ]
        
        for i, result in enumerate(results):
            check_name = check_names[i]
            if isinstance(result, Exception):
                health_status["checks"][check_name] = {
                    "status": "error",
                    "error": str(result)
                }
                health_status["overall_status"] = "unhealthy"
            else:
                health_status["checks"][check_name] = result
                if not result.get("healthy", False):
                    health_status["overall_status"] = "unhealthy"
        
        return health_status
    
    async def check_application_health(self):
        """Ê£ÄÊü•Â∫îÁî®Á®ãÂ∫èÂÅ•Â∫∑Áä∂ÊÄÅ"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get('http://localhost:8000/health', timeout=10) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        return {
                            "healthy": True,
                            "response_time": resp.headers.get('X-Response-Time', 'unknown'),
                            "version": data.get('version', 'unknown')
                        }
                    else:
                        return {
                            "healthy": False,
                            "error": f"HTTP {resp.status}"
                        }
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e)
            }
    
    async def check_database_health(self):
        """Ê£ÄÊü•Êï∞ÊçÆÂ∫ìÂÅ•Â∫∑Áä∂ÊÄÅ"""
        try:
            conn = psycopg2.connect(
                host="localhost",
                port=5432,
                database="digital_employee",
                user="user",
                password=os.getenv("DB_PASSWORD")
            )
            
            cursor = conn.cursor()
            cursor.execute("SELECT 1")
            result = cursor.fetchone()
            
            cursor.execute("SELECT count(*) FROM pg_stat_activity WHERE state = 'active'")
            active_connections = cursor.fetchone()[0]
            
            cursor.close()
            conn.close()
            
            return {
                "healthy": True,
                "active_connections": active_connections,
                "max_connections": 100  # ‰ªéÈÖçÁΩÆËé∑Âèñ
            }
            
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e)
            }
    
    async def check_redis_health(self):
        """Ê£ÄÊü•RedisÂÅ•Â∫∑Áä∂ÊÄÅ"""
        try:
            r = redis.Redis(host='localhost', port=6379, db=0)
            info = r.info()
            
            return {
                "healthy": True,
                "used_memory": info['used_memory_human'],
                "connected_clients": info['connected_clients'],
                "uptime": info['uptime_in_seconds']
            }
            
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e)
            }
    
    async def check_ai_service_health(self):
        """Ê£ÄÊü•AIÊúçÂä°ÂÅ•Â∫∑Áä∂ÊÄÅ"""
        try:
            # ËøôÈáåÂèØ‰ª•Ê∑ªÂä†ÂØπOpenAI APIÁöÑÁÆÄÂçïÊµãËØï
            async with aiohttp.ClientSession() as session:
                headers = {
                    "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}",
                    "Content-Type": "application/json"
                }
                
                # ÂèëÈÄÅÁÆÄÂçïÁöÑAPIËØ∑Ê±ÇÊµãËØïËøûÈÄöÊÄß
                data = {
                    "model": "gpt-3.5-turbo",
                    "messages": [{"role": "user", "content": "test"}],
                    "max_tokens": 5
                }
                
                async with session.post(
                    'https://api.openai.com/v1/chat/completions',
                    headers=headers,
                    json=data,
                    timeout=10
                ) as resp:
                    if resp.status in [200, 429]:  # 429ÊòØÈ¢ëÁéáÈôêÂà∂Ôºå‰ΩÜËØ¥ÊòéÊúçÂä°ÂèØÁî®
                        return {
                            "healthy": True,
                            "api_status": "available"
                        }
                    else:
                        return {
                            "healthy": False,
                            "error": f"API returned {resp.status}"
                        }
                        
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e)
            }
    
    async def check_system_resources(self):
        """Ê£ÄÊü•Á≥ªÁªüËµÑÊ∫ê‰ΩøÁî®ÊÉÖÂÜµ"""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            load_avg = psutil.getloadavg()
            
            # ÂÅ•Â∫∑ÈòàÂÄº
            cpu_threshold = 80
            memory_threshold = 80
            
            is_healthy = (
                cpu_percent < cpu_threshold and 
                memory.percent < memory_threshold
            )
            
            return {
                "healthy": is_healthy,
                "cpu_percent": cpu_percent,
                "memory_percent": memory.percent,
                "load_average": load_avg,
                "warnings": [
                    f"High CPU usage: {cpu_percent}%" if cpu_percent >= cpu_threshold else None,
                    f"High memory usage: {memory.percent}%" if memory.percent >= memory_threshold else None
                ]
            }
            
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e)
            }
    
    async def check_disk_space(self):
        """Ê£ÄÊü•Á£ÅÁõòÁ©∫Èó¥"""
        try:
            disk_usage = psutil.disk_usage('/')
            used_percent = (disk_usage.used / disk_usage.total) * 100
            
            # Á£ÅÁõòÁ©∫Èó¥ÈòàÂÄº
            disk_threshold = 85
            
            return {
                "healthy": used_percent < disk_threshold,
                "used_percent": used_percent,
                "free_space_gb": disk_usage.free / (1024**3),
                "total_space_gb": disk_usage.total / (1024**3)
            }
            
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e)
            }

async def main():
    """‰∏ªÂáΩÊï∞"""
    checker = SystemHealthChecker()
    health_status = await checker.check_all()
    
    # ËæìÂá∫ÁªìÊûú
    print(json.dumps(health_status, indent=2))
    
    # Â¶ÇÊûú‰∏çÂÅ•Â∫∑ÔºåÈÄÄÂá∫Á†Å‰∏∫1
    if health_status["overall_status"] != "healthy":
        exit(1)

if __name__ == "__main__":
    asyncio.run(main())
```

### Êó•ÂøóÂàÜÊûêËÑöÊú¨
```python
#!/usr/bin/env python3
# scripts/log_analyzer.py

import re
import json
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import argparse

class LogAnalyzer:
    """Êó•ÂøóÂàÜÊûêÂ∑•ÂÖ∑"""
    
    def __init__(self, log_file):
        self.log_file = log_file
        self.patterns = {
            'timestamp': r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})',
            'level': r'"level":\s*"(\w+)"',
            'message': r'"message":\s*"([^"]+)"',
            'response_time': r'"response_time":\s*([0-9.]+)',
            'status_code': r'"status_code":\s*(\d+)',
            'ai_tokens': r'"ai_tokens_used":\s*(\d+)',
            'error': r'"error":\s*"([^"]+)"'
        }
    
    def analyze_logs(self, hours=24):
        """ÂàÜÊûêÊó•ÂøóÊñá‰ª∂"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        stats = {
            'total_requests': 0,
            'error_count': 0,
            'response_times': [],
            'status_codes': Counter(),
            'error_messages': Counter(),
            'ai_token_usage': 0,
            'hourly_distribution': defaultdict(int),
            'slow_requests': []
        }
        
        with open(self.log_file, 'r') as f:
            for line in f:
                try:
                    # Ëß£ÊûêÊó∂Èó¥Êà≥
                    timestamp_match = re.search(self.patterns['timestamp'], line)
                    if not timestamp_match:
                        continue
                    
                    log_time = datetime.fromisoformat(timestamp_match.group(1))
                    if log_time < cutoff_time:
                        continue
                    
                    # ÁªüËÆ°ÊØèÂ∞èÊó∂ÂàÜÂ∏É
                    hour_key = log_time.strftime('%Y-%m-%d %H:00')
                    stats['hourly_distribution'][hour_key] += 1
                    
                    # Ëß£ÊûêÂÖ∂‰ªñÂ≠óÊÆµ
                    level_match = re.search(self.patterns['level'], line)
                    if level_match and level_match.group(1) in ['INFO', 'ERROR', 'WARNING']:
                        stats['total_requests'] += 1
                        
                        # ÈîôËØØÁªüËÆ°
                        if level_match.group(1) == 'ERROR':
                            stats['error_count'] += 1
                            
                            error_match = re.search(self.patterns['error'], line)
                            if error_match:
                                stats['error_messages'][error_match.group(1)] += 1
                        
                        # ÂìçÂ∫îÊó∂Èó¥ÁªüËÆ°
                        response_time_match = re.search(self.patterns['response_time'], line)
                        if response_time_match:
                            response_time = float(response_time_match.group(1))
                            stats['response_times'].append(response_time)
                            
                            # ËÆ∞ÂΩïÊÖ¢ËØ∑Ê±Ç
                            if response_time > 5.0:
                                message_match = re.search(self.patterns['message'], line)
                                stats['slow_requests'].append({
                                    'timestamp': log_time.isoformat(),
                                    'response_time': response_time,
                                    'message': message_match.group(1) if message_match else 'unknown'
                                })
                        
                        # Áä∂ÊÄÅÁ†ÅÁªüËÆ°
                        status_match = re.search(self.patterns['status_code'], line)
                        if status_match:
                            stats['status_codes'][status_match.group(1)] += 1
                        
                        # AI Token‰ΩøÁî®ÁªüËÆ°
                        tokens_match = re.search(self.patterns['ai_tokens'], line)
                        if tokens_match:
                            stats['ai_token_usage'] += int(tokens_match.group(1))
                            
                except Exception as e:
                    print(f"Ëß£ÊûêÊó•ÂøóË°åÊó∂Âá∫Èîô: {e}")
                    continue
        
        return self.generate_report(stats)
    
    def generate_report(self, stats):
        """ÁîüÊàêÂàÜÊûêÊä•Âëä"""
        report = {
            'analysis_time': datetime.now().isoformat(),
            'summary': {
                'total_requests': stats['total_requests'],
                'error_rate': stats['error_count'] / max(stats['total_requests'], 1) * 100,
                'total_ai_tokens': stats['ai_token_usage']
            },
            'performance': {},
            'errors': {
                'count': stats['error_count'],
                'top_errors': dict(stats['error_messages'].most_common(10))
            },
            'traffic': {
                'hourly_distribution': dict(stats['hourly_distribution'])
            },
            'slow_requests': stats['slow_requests'][-10:]  # ÊúÄËøë10‰∏™ÊÖ¢ËØ∑Ê±Ç
        }
        
        # ÂìçÂ∫îÊó∂Èó¥ÁªüËÆ°
        if stats['response_times']:
            response_times = sorted(stats['response_times'])
            count = len(response_times)
            
            report['performance'] = {
                'avg_response_time': sum(response_times) / count,
                'p50_response_time': response_times[int(count * 0.5)],
                'p95_response_time': response_times[int(count * 0.95)],
                'p99_response_time': response_times[int(count * 0.99)],
                'max_response_time': max(response_times)
            }
        
        return report

def main():
    parser = argparse.ArgumentParser(description='Êó•ÂøóÂàÜÊûêÂ∑•ÂÖ∑')
    parser.add_argument('--log-file', required=True, help='Êó•ÂøóÊñá‰ª∂Ë∑ØÂæÑ')
    parser.add_argument('--hours', type=int, default=24, help='ÂàÜÊûêÊúÄËøëNÂ∞èÊó∂ÁöÑÊó•Âøó')
    parser.add_argument('--output', help='ËæìÂá∫Êñá‰ª∂Ë∑ØÂæÑ')
    
    args = parser.parse_args()
    
    analyzer = LogAnalyzer(args.log_file)
    report = analyzer.analyze_logs(args.hours)
    
    if args.output:
        with open(args.output, 'w') as f:
            json.dump(report, f, indent=2)
        print(f"ÂàÜÊûêÊä•ÂëäÂ∑≤‰øùÂ≠òÂà∞: {args.output}")
    else:
        print(json.dumps(report, indent=2))

if __name__ == "__main__":
    main()
```

---

## üìã ËøêÁª¥Ê£ÄÊü•Ê∏ÖÂçï

### ÈÉ®ÁΩ≤ÂâçÊ£ÄÊü•Ê∏ÖÂçï
- [ ] **‰ª£Á†ÅË¥®Èáè**ÔºöÈÄöËøáÊâÄÊúâÊµãËØïÔºå‰ª£Á†ÅÂÆ°Êü•ÂÆåÊàê
- [ ] **ÂÆâÂÖ®Ê£ÄÊü•**ÔºöÈÄöËøáÂÆâÂÖ®Êâ´ÊèèÔºåÊó†È´òÂç±ÊºèÊ¥û
- [ ] **ÈÖçÁΩÆÈ™åËØÅ**ÔºöÁéØÂ¢ÉÂèòÈáèÂíåÈÖçÁΩÆÊñá‰ª∂Ê≠£Á°Æ
- [ ] **Êï∞ÊçÆÂ∫ìËøÅÁßª**ÔºöÊï∞ÊçÆÂ∫ìËÑöÊú¨ÊµãËØïÈÄöËøá
- [ ] **‰æùËµñÊ£ÄÊü•**ÔºöÊâÄÊúâÂ§ñÈÉ®ÊúçÂä°ÂèØÁî®
- [ ] **Â§á‰ªΩÁ°ÆËÆ§**ÔºöÂΩìÂâçÊï∞ÊçÆÂ∑≤Â§á‰ªΩ
- [ ] **ÂõûÊªöÊñπÊ°à**ÔºöÂõûÊªöÊ≠•È™§ÊòéÁ°ÆÔºåÂ∑≤ÊµãËØï
- [ ] **ÁõëÊéßÂáÜÂ§á**ÔºöÁõëÊéßÂíåÂëäË≠¶ÈÖçÁΩÆÂ∞±Áª™
- [ ] **ÊñáÊ°£Êõ¥Êñ∞**ÔºöÈÉ®ÁΩ≤ÊñáÊ°£ÂíåËøêÁª¥ÊâãÂÜåÊõ¥Êñ∞

### Êó•Â∏∏ËøêÁª¥Ê£ÄÊü•Ê∏ÖÂçï
- [ ] **Á≥ªÁªüËµÑÊ∫ê**ÔºöCPU„ÄÅÂÜÖÂ≠ò„ÄÅÁ£ÅÁõò‰ΩøÁî®Ê≠£Â∏∏
- [ ] **ÊúçÂä°Áä∂ÊÄÅ**ÔºöÊâÄÊúâÊúçÂä°ËøêË°åÊ≠£Â∏∏ÔºåÊó†ÂºÇÂ∏∏ÈáçÂêØ
- [ ] **ÂìçÂ∫îÊó∂Èó¥**ÔºöAPIÂìçÂ∫îÊó∂Èó¥Âú®Ê≠£Â∏∏ËåÉÂõ¥ÂÜÖ
- [ ] **ÈîôËØØÁéá**ÔºöÈîôËØØÁéá‰Ωé‰∫éÂëäË≠¶ÈòàÂÄº
- [ ] **AIÊúçÂä°**ÔºöAI APIË∞ÉÁî®ÊàêÂäüÁéáÊ≠£Â∏∏
- [ ] **Êï∞ÊçÆÂ∫ì**ÔºöÊï∞ÊçÆÂ∫ìËøûÊé•Ê±†Ê≠£Â∏∏ÔºåÊÖ¢Êü•ËØ¢Ê£ÄÊü•
- [ ] **Â§á‰ªΩÁä∂ÊÄÅ**ÔºöËá™Âä®Â§á‰ªΩÊâßË°åÊàêÂäü
- [ ] **Á£ÅÁõòÁ©∫Èó¥**ÔºöÊó•ÂøóÊñá‰ª∂ÂíåÊï∞ÊçÆÂ∫ìÁ©∫Èó¥ÂÖÖË∂≥
- [ ] **ÂÆâÂÖ®Êó•Âøó**ÔºöÊ£ÄÊü•ÂºÇÂ∏∏ËÆøÈóÆÂíåÂÆâÂÖ®‰∫ã‰ª∂

### ÊïÖÈöúÂìçÂ∫îÊ£ÄÊü•Ê∏ÖÂçï
- [ ] **ÈóÆÈ¢òÁ°ÆËÆ§**ÔºöÁ°ÆËÆ§ÈóÆÈ¢òËåÉÂõ¥ÂíåÂΩ±ÂìçÁ®ãÂ∫¶
- [ ] **ÂëäË≠¶ÈÄöÁü•**ÔºöÁõ∏ÂÖ≥‰∫∫ÂëòÂ∑≤Êî∂Âà∞ÈÄöÁü•
- [ ] **Áî®Êà∑ÈÄö‰ø°**ÔºöÂ¶ÇÈúÄË¶ÅÔºåÂ∑≤ÈÄöÁü•Áî®Êà∑
- [ ] **‰∏¥Êó∂Êé™ÊñΩ**ÔºöÂÆûÊñΩ‰∏¥Êó∂ÁºìËß£Êé™ÊñΩ
- [ ] **Ê†πÂõ†ÂàÜÊûê**ÔºöÂàÜÊûêÈóÆÈ¢òÊ†πÊú¨ÂéüÂõ†
- [ ] **‰øÆÂ§çÊñπÊ°à**ÔºöÂà∂ÂÆöÂπ∂ÂÆûÊñΩ‰øÆÂ§çÊñπÊ°à
- [ ] **È™åËØÅÊÅ¢Â§ç**ÔºöÁ°ÆËÆ§Á≥ªÁªüÂÆåÂÖ®ÊÅ¢Â§ç
- [ ] **ÊÄªÁªìÊä•Âëä**ÔºöÁºñÂÜôÊïÖÈöúÊÄªÁªìÊä•Âëä
- [ ] **È¢ÑÈò≤Êé™ÊñΩ**ÔºöÂà∂ÂÆöÈ¢ÑÈò≤Á±ª‰ººÈóÆÈ¢òÁöÑÊé™ÊñΩ

---

*ÈÉ®ÁΩ≤ÂíåËøêÁª¥ËßÑËåÉÊñáÊ°£ÁâàÊú¨Ôºöv1.0*  
*ÂàõÂª∫Êó∂Èó¥Ôºö2025-07-31*  
*Áª¥Êä§ËÄÖÔºöÊï∞Â≠óÂëòÂ∑•Á≥ªÁªüËøêÁª¥Âõ¢Èòü*