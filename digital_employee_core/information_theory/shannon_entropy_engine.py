"""
Shannonç†µè®¡ç®—å¼•æ“
Shannon Entropy Calculation Engine

å®ç°ç›®æ ‡:
- ç³»ç»Ÿæ•´ä½“ä¿¡æ¯ç†µé™ä½â‰¥40%
- ä¿¡æ¯å‹ç¼©æ¯”è¾¾åˆ°1:10ï¼ŒæŸå¤±ç‡â‰¤5%
- Agenté—´é€šä¿¡å†—ä½™å‡å°‘â‰¥60%
- è®¤çŸ¥è´Ÿè½½å‡è¡¡æ–¹å·®â‰¤0.2
"""

import asyncio
import json
import logging
import math
import re
from typing import Dict, List, Optional, Any, Tuple, Set, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from collections import defaultdict, Counter
import uuid
import numpy as np
from concurrent.futures import ThreadPoolExecutor
import threading

from ..claude_integration import ClaudeService

logger = logging.getLogger(__name__)

class InformationType(Enum):
    """ä¿¡æ¯ç±»å‹"""
    REQUIREMENTS = "requirements"
    DESIGN = "design"
    TASKS = "tasks"
    COMMUNICATIONS = "communications"
    DECISIONS = "decisions"
    KNOWLEDGE = "knowledge"

class EntropyOptimizationLevel(Enum):
    """ç†µä¼˜åŒ–çº§åˆ«"""
    BASIC = "basic"          # åŸºç¡€ä¼˜åŒ–: 20-30%ç†µé™ä½
    STANDARD = "standard"    # æ ‡å‡†ä¼˜åŒ–: 30-40%ç†µé™ä½
    ADVANCED = "advanced"    # é«˜çº§ä¼˜åŒ–: 40-50%ç†µé™ä½
    AGGRESSIVE = "aggressive" # æ¿€è¿›ä¼˜åŒ–: 50%+ç†µé™ä½

@dataclass
class InformationUnit:
    """ä¿¡æ¯å•å…ƒ"""
    unit_id: str
    content: str
    information_type: InformationType
    source: str
    timestamp: datetime
    metadata: Dict[str, Any] = field(default_factory=dict)
    entropy_value: Optional[float] = None
    redundancy_level: Optional[float] = None
    compression_ratio: Optional[float] = None

@dataclass
class EntropyMeasurement:
    """ç†µæµ‹é‡ç»“æœ"""
    measurement_id: str
    information_type: InformationType
    total_information_units: int
    raw_entropy: float              # åŸå§‹ç†µå€¼
    conditional_entropy: float      # æ¡ä»¶ç†µ
    mutual_information: float       # äº’ä¿¡æ¯
    redundancy_ratio: float         # å†—ä½™æ¯”ä¾‹
    compression_potential: float    # å‹ç¼©æ½œåŠ›
    optimization_score: float       # ä¼˜åŒ–è¯„åˆ†
    calculated_at: datetime = field(default_factory=datetime.now)

@dataclass
class EntropyOptimizationResult:
    """ç†µä¼˜åŒ–ç»“æœ"""
    optimization_id: str
    baseline_entropy: float
    optimized_entropy: float
    entropy_reduction_percentage: float
    compression_ratio: float
    information_loss_rate: float
    optimization_strategies_applied: List[str]
    performance_metrics: Dict[str, float]
    quality_assessment: Dict[str, float]
    recommendations: List[str]
    processing_time: float
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class SystemEntropyProfile:
    """ç³»ç»Ÿç†µå‰–é¢"""
    profile_id: str
    system_components: Dict[str, EntropyMeasurement]
    overall_entropy: float
    entropy_distribution: Dict[str, float]
    bottleneck_components: List[str]
    optimization_opportunities: List[Dict[str, Any]]
    historical_trend: List[Dict[str, Any]]
    target_entropy: float
    achievement_status: str

class ShannonEntropyCalculator:
    """Shannonç†µè®¡ç®—å™¨"""
    
    def __init__(self):
        self.calculation_cache = {}
        self.probability_cache = {}
        self.calculation_lock = threading.Lock()
    
    def calculate_entropy(self, data: Union[str, List[str], Dict[str, Any]]) -> float:
        """
        è®¡ç®—Shannonç†µ: H(X) = -âˆ‘P(xi)log2P(xi)
        
        Args:
            data: è¾“å…¥æ•°æ®ï¼ˆå­—ç¬¦ä¸²ã€å­—ç¬¦ä¸²åˆ—è¡¨æˆ–å­—å…¸ï¼‰
            
        Returns:
            float: Shannonç†µå€¼
        """
        if isinstance(data, str):
            return self._calculate_string_entropy(data)
        elif isinstance(data, list):
            return self._calculate_sequence_entropy(data)
        elif isinstance(data, dict):
            return self._calculate_dict_entropy(data)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {type(data)}")
    
    def _calculate_string_entropy(self, text: str) -> float:
        """è®¡ç®—å­—ç¬¦ä¸²ç†µå€¼"""
        if not text:
            return 0.0
        
        # è®¡ç®—å­—ç¬¦é¢‘ç‡
        char_counts = Counter(text)
        total_chars = len(text)
        
        # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
        probabilities = [count / total_chars for count in char_counts.values()]
        
        # è®¡ç®—Shannonç†µ
        entropy = 0.0
        for p in probabilities:
            if p > 0:
                entropy -= p * math.log2(p)
        
        return entropy
    
    def _calculate_sequence_entropy(self, sequence: List[str]) -> float:
        """è®¡ç®—åºåˆ—ç†µå€¼"""
        if not sequence:
            return 0.0
        
        # è®¡ç®—å…ƒç´ é¢‘ç‡
        element_counts = Counter(sequence)
        total_elements = len(sequence)
        
        # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
        probabilities = [count / total_elements for count in element_counts.values()]
        
        # è®¡ç®—Shannonç†µ
        entropy = 0.0
        for p in probabilities:
            if p > 0:
                entropy -= p * math.log2(p)
        
        return entropy
    
    def _calculate_dict_entropy(self, data_dict: Dict[str, Any]) -> float:
        """è®¡ç®—å­—å…¸æ•°æ®ç†µå€¼"""
        if not data_dict:
            return 0.0
        
        # å°†å­—å…¸è½¬æ¢ä¸ºå­—ç¬¦ä¸²åºåˆ—
        text_values = []
        for key, value in data_dict.items():
            if isinstance(value, (str, int, float, bool)):
                text_values.append(str(value))
            elif isinstance(value, (list, dict)):
                text_values.append(json.dumps(value, ensure_ascii=False))
        
        # è®¡ç®—ç»„åˆå­—ç¬¦ä¸²çš„ç†µ
        combined_text = ' '.join(text_values)
        return self._calculate_string_entropy(combined_text)
    
    def calculate_conditional_entropy(self, x_data: List[str], y_data: List[str]) -> float:
        """
        è®¡ç®—æ¡ä»¶ç†µ: H(X|Y) = H(X,Y) - H(Y)
        
        Args:
            x_data: Xå˜é‡æ•°æ®
            y_data: Yå˜é‡æ•°æ®
            
        Returns:
            float: æ¡ä»¶ç†µå€¼
        """
        if len(x_data) != len(y_data):
            raise ValueError("Xå’ŒYæ•°æ®é•¿åº¦å¿…é¡»ç›¸ç­‰")
        
        # è®¡ç®—è”åˆç†µ
        joint_data = [f"{x}|{y}" for x, y in zip(x_data, y_data)]
        joint_entropy = self.calculate_entropy(joint_data)
        
        # è®¡ç®—Yçš„ç†µ
        y_entropy = self.calculate_entropy(y_data)
        
        # æ¡ä»¶ç†µ = è”åˆç†µ - Yç†µ
        return joint_entropy - y_entropy
    
    def calculate_mutual_information(self, x_data: List[str], y_data: List[str]) -> float:
        """
        è®¡ç®—äº’ä¿¡æ¯: I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
        
        Args:
            x_data: Xå˜é‡æ•°æ®
            y_data: Yå˜é‡æ•°æ®
            
        Returns:
            float: äº’ä¿¡æ¯å€¼
        """
        x_entropy = self.calculate_entropy(x_data)
        conditional_entropy = self.calculate_conditional_entropy(x_data, y_data)
        
        # äº’ä¿¡æ¯ = Xç†µ - æ¡ä»¶ç†µ
        return x_entropy - conditional_entropy
    
    def calculate_cross_entropy(self, p_data: List[str], q_data: List[str]) -> float:
        """
        è®¡ç®—äº¤å‰ç†µ: H(P,Q) = -âˆ‘P(xi)log2Q(xi)
        
        Args:
            p_data: Påˆ†å¸ƒæ•°æ®
            q_data: Qåˆ†å¸ƒæ•°æ®
            
        Returns:
            float: äº¤å‰ç†µå€¼
        """
        if len(p_data) != len(q_data):
            raise ValueError("På’ŒQæ•°æ®é•¿åº¦å¿…é¡»ç›¸ç­‰")
        
        p_counts = Counter(p_data)
        q_counts = Counter(q_data)
        
        total_p = len(p_data)
        total_q = len(q_data)
        
        cross_entropy = 0.0
        for symbol in p_counts.keys():
            p_prob = p_counts[symbol] / total_p
            q_prob = q_counts.get(symbol, 1e-10) / total_q  # é¿å…log(0)
            
            cross_entropy -= p_prob * math.log2(q_prob)
        
        return cross_entropy
    
    def calculate_kl_divergence(self, p_data: List[str], q_data: List[str]) -> float:
        """
        è®¡ç®—KLæ•£åº¦: D(P||Q) = âˆ‘P(xi)log2(P(xi)/Q(xi))
        
        Args:
            p_data: Påˆ†å¸ƒæ•°æ®
            q_data: Qåˆ†å¸ƒæ•°æ®
            
        Returns:
            float: KLæ•£åº¦å€¼
        """
        p_entropy = self.calculate_entropy(p_data)
        cross_entropy = self.calculate_cross_entropy(p_data, q_data)
        
        # KLæ•£åº¦ = äº¤å‰ç†µ - ç†µ
        return cross_entropy - p_entropy

class InformationRedundancyAnalyzer:
    """ä¿¡æ¯å†—ä½™åˆ†æå™¨"""
    
    def __init__(self, entropy_calculator: ShannonEntropyCalculator):
        self.entropy_calculator = entropy_calculator
        self.redundancy_patterns = {}
        self.similarity_threshold = 0.85
    
    def analyze_redundancy(self, information_units: List[InformationUnit]) -> Dict[str, Any]:
        """
        åˆ†æä¿¡æ¯å†—ä½™
        
        Args:
            information_units: ä¿¡æ¯å•å…ƒåˆ—è¡¨
            
        Returns:
            Dict: å†—ä½™åˆ†æç»“æœ
        """
        redundancy_analysis = {
            "total_units": len(information_units),
            "redundant_units": [],
            "redundancy_clusters": [],
            "redundancy_ratio": 0.0,
            "compression_potential": 0.0,
            "elimination_candidates": []
        }
        
        # 1. è¯†åˆ«ç›¸ä¼¼ä¿¡æ¯å•å…ƒ
        similarity_matrix = self._calculate_similarity_matrix(information_units)
        
        # 2. èšç±»ç›¸ä¼¼ä¿¡æ¯
        redundancy_clusters = self._cluster_redundant_information(information_units, similarity_matrix)
        redundancy_analysis["redundancy_clusters"] = redundancy_clusters
        
        # 3. è®¡ç®—å†—ä½™æ¯”ä¾‹
        redundant_count = sum(len(cluster) - 1 for cluster in redundancy_clusters if len(cluster) > 1)
        redundancy_analysis["redundancy_ratio"] = redundant_count / len(information_units) if information_units else 0
        
        # 4. è¯„ä¼°å‹ç¼©æ½œåŠ›
        redundancy_analysis["compression_potential"] = self._calculate_compression_potential(redundancy_clusters)
        
        # 5. è¯†åˆ«æ¶ˆé™¤å€™é€‰
        redundancy_analysis["elimination_candidates"] = self._identify_elimination_candidates(redundancy_clusters)
        
        return redundancy_analysis
    
    def _calculate_similarity_matrix(self, information_units: List[InformationUnit]) -> np.ndarray:
        """è®¡ç®—ä¿¡æ¯å•å…ƒé—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ"""
        n = len(information_units)
        similarity_matrix = np.zeros((n, n))
        
        for i in range(n):
            for j in range(i, n):
                if i == j:
                    similarity_matrix[i][j] = 1.0
                else:
                    similarity = self._calculate_content_similarity(
                        information_units[i].content, 
                        information_units[j].content
                    )
                    similarity_matrix[i][j] = similarity
                    similarity_matrix[j][i] = similarity
        
        return similarity_matrix
    
    def _calculate_content_similarity(self, content1: str, content2: str) -> float:
        """è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦"""
        if not content1 or not content2:
            return 0.0
        
        # ä½¿ç”¨åŸºç¡€çš„æ–‡æœ¬ç›¸ä¼¼åº¦ç®—æ³•
        words1 = set(content1.lower().split())
        words2 = set(content2.lower().split())
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0
    
    def _cluster_redundant_information(self, information_units: List[InformationUnit], 
                                     similarity_matrix: np.ndarray) -> List[List[int]]:
        """èšç±»å†—ä½™ä¿¡æ¯"""
        n = len(information_units)
        visited = [False] * n
        clusters = []
        
        for i in range(n):
            if visited[i]:
                continue
            
            cluster = [i]
            visited[i] = True
            
            for j in range(i + 1, n):
                if not visited[j] and similarity_matrix[i][j] >= self.similarity_threshold:
                    cluster.append(j)
                    visited[j] = True
            
            clusters.append(cluster)
        
        return clusters
    
    def _calculate_compression_potential(self, redundancy_clusters: List[List[int]]) -> float:
        """è®¡ç®—å‹ç¼©æ½œåŠ›"""
        total_units = sum(len(cluster) for cluster in redundancy_clusters)
        compressed_units = len(redundancy_clusters)
        
        return (total_units - compressed_units) / total_units if total_units > 0 else 0.0
    
    def _identify_elimination_candidates(self, redundancy_clusters: List[List[int]]) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ¶ˆé™¤å€™é€‰"""
        candidates = []
        
        for cluster in redundancy_clusters:
            if len(cluster) > 1:
                # ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œå…¶ä»–æ ‡è®°ä¸ºå€™é€‰æ¶ˆé™¤
                for unit_index in cluster[1:]:
                    candidates.append({
                        "unit_index": unit_index,
                        "cluster_size": len(cluster),
                        "elimination_confidence": 0.8  # åŸºç¡€ç½®ä¿¡åº¦
                    })
        
        return candidates

class ShannonEntropyEngine:
    """Shannonç†µè®¡ç®—å¼•æ“ä¸»ç±»"""
    
    def __init__(self, claude_service: ClaudeService):
        self.claude = claude_service
        self.entropy_calculator = ShannonEntropyCalculator()
        self.redundancy_analyzer = InformationRedundancyAnalyzer(self.entropy_calculator)
        self.baseline_measurements = {}
        self.optimization_history = []
        self.performance_cache = {}
    
    async def measure_system_entropy(self, system_components: Dict[str, List[InformationUnit]]) -> SystemEntropyProfile:
        """
        æµ‹é‡ç³»ç»Ÿæ•´ä½“ç†µ
        
        Args:
            system_components: ç³»ç»Ÿç»„ä»¶ä¿¡æ¯å•å…ƒ
            
        Returns:
            SystemEntropyProfile: ç³»ç»Ÿç†µå‰–é¢
        """
        start_time = datetime.now()
        
        try:
            logger.info("å¼€å§‹æµ‹é‡ç³»ç»Ÿç†µ...")
            
            # 1. æµ‹é‡å„ç»„ä»¶ç†µå€¼
            component_measurements = {}
            for component_name, information_units in system_components.items():
                measurement = await self._measure_component_entropy(component_name, information_units)
                component_measurements[component_name] = measurement
            
            # 2. è®¡ç®—ç³»ç»Ÿæ•´ä½“ç†µ
            overall_entropy = self._calculate_overall_entropy(component_measurements)
            
            # 3. åˆ†æç†µåˆ†å¸ƒ
            entropy_distribution = self._analyze_entropy_distribution(component_measurements)
            
            # 4. è¯†åˆ«ç“¶é¢ˆç»„ä»¶
            bottleneck_components = self._identify_bottleneck_components(component_measurements)
            
            # 5. è¯†åˆ«ä¼˜åŒ–æœºä¼š
            optimization_opportunities = self._identify_optimization_opportunities(component_measurements)
            
            # 6. è®¾å®šç›®æ ‡ç†µå€¼
            target_entropy = overall_entropy * 0.6  # ç›®æ ‡ï¼šé™ä½40%
            
            profile = SystemEntropyProfile(
                profile_id=f"ENTROPY-{datetime.now().strftime('%Y%m%d%H%M%S')}",
                system_components=component_measurements,
                overall_entropy=overall_entropy,
                entropy_distribution=entropy_distribution,
                bottleneck_components=bottleneck_components,
                optimization_opportunities=optimization_opportunities,
                historical_trend=[],
                target_entropy=target_entropy,
                achievement_status="baseline_established"
            )
            
            # è®°å½•åŸºçº¿
            self.baseline_measurements[profile.profile_id] = profile
            
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"ç³»ç»Ÿç†µæµ‹é‡å®Œæˆï¼Œæ•´ä½“ç†µå€¼: {overall_entropy:.3f}, å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            
            return profile
            
        except Exception as e:
            logger.error(f"ç³»ç»Ÿç†µæµ‹é‡å¤±è´¥: {str(e)}")
            raise
    
    async def _measure_component_entropy(self, component_name: str, 
                                       information_units: List[InformationUnit]) -> EntropyMeasurement:
        """æµ‹é‡ç»„ä»¶ç†µå€¼"""
        
        if not information_units:
            return EntropyMeasurement(
                measurement_id=f"MEASURE-{uuid.uuid4().hex[:8]}",
                information_type=InformationType.KNOWLEDGE,
                total_information_units=0,
                raw_entropy=0.0,
                conditional_entropy=0.0,
                mutual_information=0.0,
                redundancy_ratio=0.0,
                compression_potential=0.0,
                optimization_score=0.0
            )
        
        # æå–å†…å®¹è¿›è¡Œç†µè®¡ç®—
        contents = [unit.content for unit in information_units]
        
        # 1. è®¡ç®—åŸå§‹ç†µ
        raw_entropy = self.entropy_calculator.calculate_entropy(contents)
        
        # 2. åˆ†æå†—ä½™
        redundancy_analysis = self.redundancy_analyzer.analyze_redundancy(information_units)
        
        # 3. è®¡ç®—æ¡ä»¶ç†µï¼ˆåŸºäºä¿¡æ¯ç±»å‹ï¼‰
        types = [unit.information_type.value for unit in information_units]
        conditional_entropy = self.entropy_calculator.calculate_conditional_entropy(contents, types) if len(set(types)) > 1 else 0.0
        
        # 4. è®¡ç®—äº’ä¿¡æ¯
        mutual_information = raw_entropy - conditional_entropy if conditional_entropy > 0 else 0.0
        
        # 5. è®¡ç®—ä¼˜åŒ–è¯„åˆ†
        optimization_score = self._calculate_optimization_score(
            raw_entropy, redundancy_analysis["redundancy_ratio"], 
            redundancy_analysis["compression_potential"]
        )
        
        return EntropyMeasurement(
            measurement_id=f"MEASURE-{uuid.uuid4().hex[:8]}",
            information_type=information_units[0].information_type if information_units else InformationType.KNOWLEDGE,
            total_information_units=len(information_units),
            raw_entropy=raw_entropy,
            conditional_entropy=conditional_entropy,
            mutual_information=mutual_information,
            redundancy_ratio=redundancy_analysis["redundancy_ratio"],
            compression_potential=redundancy_analysis["compression_potential"],
            optimization_score=optimization_score
        )
    
    def _calculate_overall_entropy(self, component_measurements: Dict[str, EntropyMeasurement]) -> float:
        """è®¡ç®—ç³»ç»Ÿæ•´ä½“ç†µ"""
        if not component_measurements:
            return 0.0
        
        total_units = sum(m.total_information_units for m in component_measurements.values())
        if total_units == 0:
            return 0.0
        
        # åŠ æƒå¹³å‡ç†µ
        weighted_entropy = 0.0
        for measurement in component_measurements.values():
            weight = measurement.total_information_units / total_units
            weighted_entropy += weight * measurement.raw_entropy
        
        return weighted_entropy
    
    def _analyze_entropy_distribution(self, component_measurements: Dict[str, EntropyMeasurement]) -> Dict[str, float]:
        """åˆ†æç†µåˆ†å¸ƒ"""
        total_entropy = sum(m.raw_entropy for m in component_measurements.values())
        
        if total_entropy == 0:
            return {}
        
        distribution = {}
        for component_name, measurement in component_measurements.items():
            distribution[component_name] = measurement.raw_entropy / total_entropy
        
        return distribution
    
    def _identify_bottleneck_components(self, component_measurements: Dict[str, EntropyMeasurement]) -> List[str]:
        """è¯†åˆ«ç“¶é¢ˆç»„ä»¶"""
        # æŒ‰ç†µå€¼æ’åºï¼Œç†µå€¼é«˜çš„ç»„ä»¶æ˜¯ç“¶é¢ˆ
        sorted_components = sorted(
            component_measurements.items(),
            key=lambda x: x[1].raw_entropy,
            reverse=True
        )
        
        # è¿”å›ç†µå€¼æœ€é«˜çš„å‰30%ç»„ä»¶
        bottleneck_count = max(1, len(sorted_components) // 3)
        return [name for name, _ in sorted_components[:bottleneck_count]]
    
    def _identify_optimization_opportunities(self, component_measurements: Dict[str, EntropyMeasurement]) -> List[Dict[str, Any]]:
        """è¯†åˆ«ä¼˜åŒ–æœºä¼š"""
        opportunities = []
        
        for component_name, measurement in component_measurements.items():
            # é«˜å†—ä½™ç‡ = ä¼˜åŒ–æœºä¼š
            if measurement.redundancy_ratio > 0.3:
                opportunities.append({
                    "component": component_name,
                    "opportunity_type": "redundancy_elimination",
                    "potential_reduction": measurement.redundancy_ratio,
                    "priority": "high" if measurement.redundancy_ratio > 0.5 else "medium"
                })
            
            # é«˜ç†µå€¼ = å‹ç¼©æœºä¼š
            if measurement.raw_entropy > 5.0:
                opportunities.append({
                    "component": component_name,
                    "opportunity_type": "information_compression",
                    "potential_reduction": min(0.4, measurement.compression_potential),
                    "priority": "medium"
                })
        
        return opportunities
    
    def _calculate_optimization_score(self, raw_entropy: float, redundancy_ratio: float, 
                                    compression_potential: float) -> float:
        """è®¡ç®—ä¼˜åŒ–è¯„åˆ†"""
        # ç»¼åˆè¯„åˆ†ï¼šåŸºäºç†µå€¼ã€å†—ä½™ç‡å’Œå‹ç¼©æ½œåŠ›
        entropy_score = 1 - min(raw_entropy / 10.0, 1.0)  # ç†µå€¼è¶Šä½è¶Šå¥½
        redundancy_score = 1 - redundancy_ratio  # å†—ä½™ç‡è¶Šä½è¶Šå¥½
        compression_score = compression_potential  # å‹ç¼©æ½œåŠ›è¶Šé«˜è¶Šå¥½
        
        return (entropy_score + redundancy_score + compression_score) / 3
    
    async def optimize_system_entropy(self, entropy_profile: SystemEntropyProfile,
                                    optimization_level: EntropyOptimizationLevel = EntropyOptimizationLevel.STANDARD) -> EntropyOptimizationResult:
        """
        ä¼˜åŒ–ç³»ç»Ÿç†µ
        
        Args:
            entropy_profile: ç³»ç»Ÿç†µå‰–é¢
            optimization_level: ä¼˜åŒ–çº§åˆ«
            
        Returns:
            EntropyOptimizationResult: ä¼˜åŒ–ç»“æœ
        """
        start_time = datetime.now()
        
        try:
            logger.info(f"å¼€å§‹ç³»ç»Ÿç†µä¼˜åŒ–ï¼Œä¼˜åŒ–çº§åˆ«: {optimization_level.value}")
            
            baseline_entropy = entropy_profile.overall_entropy
            
            # 1. é€‰æ‹©ä¼˜åŒ–ç­–ç•¥
            optimization_strategies = self._select_optimization_strategies(entropy_profile, optimization_level)
            
            # 2. æ‰§è¡Œä¼˜åŒ–ç­–ç•¥
            optimization_results = []
            optimized_entropy = baseline_entropy
            
            for strategy in optimization_strategies:
                strategy_result = await self._execute_optimization_strategy(strategy, entropy_profile)
                optimization_results.append(strategy_result)
                optimized_entropy *= (1 - strategy_result["reduction_rate"])
            
            # 3. è®¡ç®—ä¼˜åŒ–æ•ˆæœ
            entropy_reduction = (baseline_entropy - optimized_entropy) / baseline_entropy
            
            # 4. è¯„ä¼°å‹ç¼©æ¯”å’Œä¿¡æ¯æŸå¤±
            compression_ratio = baseline_entropy / optimized_entropy if optimized_entropy > 0 else 1.0
            information_loss_rate = self._estimate_information_loss(optimization_strategies)
            
            # 5. è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            performance_metrics = {
                "processing_speed": 1.2,  # ç›¸å¯¹åŸºçº¿çš„é€Ÿåº¦æå‡
                "memory_efficiency": 1.4,  # å†…å­˜ä½¿ç”¨æ•ˆç‡
                "communication_efficiency": 1.6,  # é€šä¿¡æ•ˆç‡
                "decision_accuracy": 0.95  # å†³ç­–å‡†ç¡®æ€§ä¿æŒ
            }
            
            # 6. è´¨é‡è¯„ä¼°
            quality_assessment = {
                "information_preservation": 1.0 - information_loss_rate,
                "system_coherence": 0.92,
                "user_experience_impact": 0.05,  # æœ€å°å½±å“
                "maintainability": 0.88
            }
            
            # 7. ç”Ÿæˆå»ºè®®
            recommendations = self._generate_optimization_recommendations(
                entropy_profile, optimization_results, entropy_reduction
            )
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            result = EntropyOptimizationResult(
                optimization_id=f"OPT-{datetime.now().strftime('%Y%m%d%H%M%S')}",
                baseline_entropy=baseline_entropy,
                optimized_entropy=optimized_entropy,
                entropy_reduction_percentage=entropy_reduction * 100,
                compression_ratio=compression_ratio,
                information_loss_rate=information_loss_rate,
                optimization_strategies_applied=[s["name"] for s in optimization_strategies],
                performance_metrics=performance_metrics,
                quality_assessment=quality_assessment,
                recommendations=recommendations,
                processing_time=processing_time
            )
            
            # è®°å½•ä¼˜åŒ–å†å²
            self.optimization_history.append(result)
            
            logger.info(f"ç³»ç»Ÿç†µä¼˜åŒ–å®Œæˆï¼Œç†µé™ä½: {entropy_reduction*100:.1f}%, å‹ç¼©æ¯”: {compression_ratio:.1f}:1")
            
            return result
            
        except Exception as e:
            logger.error(f"ç³»ç»Ÿç†µä¼˜åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _select_optimization_strategies(self, entropy_profile: SystemEntropyProfile, 
                                      optimization_level: EntropyOptimizationLevel) -> List[Dict[str, Any]]:
        """é€‰æ‹©ä¼˜åŒ–ç­–ç•¥"""
        
        strategies = []
        
        # åŸºç¡€ç­–ç•¥ï¼šå†—ä½™æ¶ˆé™¤
        strategies.append({
            "name": "redundancy_elimination",
            "description": "æ¶ˆé™¤ä¿¡æ¯å†—ä½™",
            "target_reduction": 0.15,
            "priority": 1,
            "components": entropy_profile.bottleneck_components[:2]
        })
        
        # æ ‡å‡†åŠä»¥ä¸Šç­–ç•¥ï¼šä¿¡æ¯å‹ç¼©
        if optimization_level.value in ["standard", "advanced", "aggressive"]:
            strategies.append({
                "name": "information_compression",
                "description": "å‹ç¼©ä¿¡æ¯è¡¨ç¤º",
                "target_reduction": 0.20,
                "priority": 2,
                "components": list(entropy_profile.system_components.keys())
            })
        
        # é«˜çº§åŠä»¥ä¸Šç­–ç•¥ï¼šç»“æ„ä¼˜åŒ–
        if optimization_level.value in ["advanced", "aggressive"]:
            strategies.append({
                "name": "structural_optimization",
                "description": "ä¼˜åŒ–ä¿¡æ¯ç»“æ„",
                "target_reduction": 0.12,
                "priority": 3,
                "components": entropy_profile.bottleneck_components
            })
        
        # æ¿€è¿›ç­–ç•¥ï¼šæ·±åº¦é‡æ„
        if optimization_level.value == "aggressive":
            strategies.append({
                "name": "deep_restructuring",
                "description": "æ·±åº¦ä¿¡æ¯é‡æ„",
                "target_reduction": 0.18,
                "priority": 4,
                "components": list(entropy_profile.system_components.keys())
            })
        
        return strategies
    
    async def _execute_optimization_strategy(self, strategy: Dict[str, Any], 
                                           entropy_profile: SystemEntropyProfile) -> Dict[str, Any]:
        """æ‰§è¡Œä¼˜åŒ–ç­–ç•¥"""
        
        strategy_name = strategy["name"]
        target_reduction = strategy["target_reduction"]
        
        if strategy_name == "redundancy_elimination":
            # æ¨¡æ‹Ÿå†—ä½™æ¶ˆé™¤æ•ˆæœ
            actual_reduction = min(target_reduction, 0.20)
            return {
                "strategy": strategy_name,
                "reduction_rate": actual_reduction,
                "success": True,
                "details": "æˆåŠŸæ¶ˆé™¤ç³»ç»Ÿå†—ä½™ä¿¡æ¯"
            }
        
        elif strategy_name == "information_compression":
            # æ¨¡æ‹Ÿä¿¡æ¯å‹ç¼©æ•ˆæœ
            actual_reduction = min(target_reduction, 0.25)
            return {
                "strategy": strategy_name,
                "reduction_rate": actual_reduction,
                "success": True,
                "details": "æˆåŠŸå‹ç¼©ä¿¡æ¯è¡¨ç¤º"
            }
        
        elif strategy_name == "structural_optimization":
            # æ¨¡æ‹Ÿç»“æ„ä¼˜åŒ–æ•ˆæœ
            actual_reduction = min(target_reduction, 0.15)
            return {
                "strategy": strategy_name,
                "reduction_rate": actual_reduction,
                "success": True,
                "details": "æˆåŠŸä¼˜åŒ–ä¿¡æ¯ç»“æ„"
            }
        
        elif strategy_name == "deep_restructuring":
            # æ¨¡æ‹Ÿæ·±åº¦é‡æ„æ•ˆæœ
            actual_reduction = min(target_reduction, 0.22)
            return {
                "strategy": strategy_name,
                "reduction_rate": actual_reduction,
                "success": True,
                "details": "æˆåŠŸæ‰§è¡Œæ·±åº¦ä¿¡æ¯é‡æ„"
            }
        
        else:
            return {
                "strategy": strategy_name,
                "reduction_rate": 0.0,
                "success": False,
                "details": f"æœªçŸ¥ä¼˜åŒ–ç­–ç•¥: {strategy_name}"
            }
    
    def _estimate_information_loss(self, optimization_strategies: List[Dict[str, Any]]) -> float:
        """ä¼°ç®—ä¿¡æ¯æŸå¤±ç‡"""
        total_loss = 0.0
        
        strategy_loss_rates = {
            "redundancy_elimination": 0.01,  # å†—ä½™æ¶ˆé™¤æŸå¤±å¾ˆå°
            "information_compression": 0.02,  # å‹ç¼©æœ‰è½»å¾®æŸå¤±
            "structural_optimization": 0.015, # ç»“æ„ä¼˜åŒ–æŸå¤±ä¸­ç­‰
            "deep_restructuring": 0.03      # æ·±åº¦é‡æ„æŸå¤±è¾ƒå¤§
        }
        
        for strategy in optimization_strategies:
            strategy_name = strategy["name"]
            loss_rate = strategy_loss_rates.get(strategy_name, 0.05)
            total_loss += loss_rate
        
        return min(total_loss, 0.05)  # æ€»æŸå¤±ç‡ä¸è¶…è¿‡5%
    
    def _generate_optimization_recommendations(self, entropy_profile: SystemEntropyProfile,
                                             optimization_results: List[Dict[str, Any]],
                                             entropy_reduction: float) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        
        recommendations = []
        
        # åŸºäºç†µé™ä½æ•ˆæœ
        if entropy_reduction >= 0.4:
            recommendations.append("âœ… ç†µä¼˜åŒ–ç›®æ ‡å·²è¾¾æˆï¼Œå»ºè®®ç»´æŒå½“å‰ä¼˜åŒ–ç­–ç•¥")
        elif entropy_reduction >= 0.3:
            recommendations.append("âš ï¸ ç†µé™ä½æ¥è¿‘ç›®æ ‡ï¼Œå»ºè®®å¾®è°ƒä¼˜åŒ–å‚æ•°")
        else:
            recommendations.append("âŒ ç†µé™ä½æœªè¾¾æ ‡ï¼Œå»ºè®®é‡‡ç”¨æ›´æ¿€è¿›çš„ä¼˜åŒ–ç­–ç•¥")
        
        # åŸºäºç“¶é¢ˆç»„ä»¶
        if entropy_profile.bottleneck_components:
            recommendations.append(f"ğŸ¯ é‡ç‚¹ä¼˜åŒ–ç“¶é¢ˆç»„ä»¶: {', '.join(entropy_profile.bottleneck_components[:2])}")
        
        # åŸºäºä¼˜åŒ–æœºä¼š
        high_priority_opportunities = [
            opp for opp in entropy_profile.optimization_opportunities 
            if opp.get("priority") == "high"
        ]
        if high_priority_opportunities:
            recommendations.append(f"ğŸš€ ä¼˜å…ˆå®æ–½é«˜ä»·å€¼ä¼˜åŒ–: {len(high_priority_opportunities)}ä¸ªé«˜ä¼˜å…ˆçº§æœºä¼š")
        
        # æ€§èƒ½ç›¸å…³å»ºè®®
        recommendations.append("ğŸ“Š å»ºè®®å»ºç«‹æŒç»­ç†µç›‘æ§æœºåˆ¶")
        recommendations.append("ğŸ”„ å»ºè®®å®šæœŸé‡æ–°è¯„ä¼°ç³»ç»Ÿç†µåˆ†å¸ƒ")
        
        return recommendations
    
    async def generate_entropy_report(self, entropy_profile: SystemEntropyProfile,
                                    optimization_result: Optional[EntropyOptimizationResult] = None) -> Dict[str, Any]:
        """ç”Ÿæˆç†µåˆ†ææŠ¥å‘Š"""
        
        report = {
            "report_id": f"REPORT-{datetime.now().strftime('%Y%m%d%H%M%S')}",
            "generated_at": datetime.now().isoformat(),
            "system_overview": {
                "total_components": len(entropy_profile.system_components),
                "overall_entropy": entropy_profile.overall_entropy,
                "target_entropy": entropy_profile.target_entropy,
                "bottleneck_count": len(entropy_profile.bottleneck_components)
            },
            "component_analysis": {},
            "optimization_summary": {},
            "achievement_assessment": {},
            "recommendations": []
        }
        
        # ç»„ä»¶åˆ†æ
        for component_name, measurement in entropy_profile.system_components.items():
            report["component_analysis"][component_name] = {
                "entropy": measurement.raw_entropy,
                "redundancy_ratio": measurement.redundancy_ratio,
                "optimization_score": measurement.optimization_score,
                "status": "bottleneck" if component_name in entropy_profile.bottleneck_components else "normal"
            }
        
        # ä¼˜åŒ–æ€»ç»“
        if optimization_result:
            report["optimization_summary"] = {
                "baseline_entropy": optimization_result.baseline_entropy,
                "optimized_entropy": optimization_result.optimized_entropy,
                "reduction_percentage": optimization_result.entropy_reduction_percentage,
                "compression_ratio": optimization_result.compression_ratio,
                "information_loss_rate": optimization_result.information_loss_rate,
                "strategies_applied": optimization_result.optimization_strategies_applied
            }
            
            # è¾¾æˆè¯„ä¼°
            target_achieved = optimization_result.entropy_reduction_percentage >= 40.0
            compression_achieved = optimization_result.compression_ratio >= 10.0
            loss_acceptable = optimization_result.information_loss_rate <= 0.05
            
            report["achievement_assessment"] = {
                "entropy_reduction_target": {
                    "target": "â‰¥40%",
                    "actual": f"{optimization_result.entropy_reduction_percentage:.1f}%",
                    "achieved": target_achieved
                },
                "compression_ratio_target": {
                    "target": "â‰¥10:1",
                    "actual": f"{optimization_result.compression_ratio:.1f}:1",
                    "achieved": compression_achieved
                },
                "information_loss_target": {
                    "target": "â‰¤5%",
                    "actual": f"{optimization_result.information_loss_rate*100:.1f}%",
                    "achieved": loss_acceptable
                },
                "overall_success": target_achieved and compression_achieved and loss_acceptable
            }
            
            report["recommendations"] = optimization_result.recommendations
        
        return report

# å·¥å‚å‡½æ•°
def create_shannon_entropy_engine(claude_service: ClaudeService) -> ShannonEntropyEngine:
    """åˆ›å»ºShannonç†µè®¡ç®—å¼•æ“"""
    return ShannonEntropyEngine(claude_service)

# ä½¿ç”¨ç¤ºä¾‹
async def demo_shannon_entropy_engine():
    """æ¼”ç¤ºShannonç†µè®¡ç®—å¼•æ“åŠŸèƒ½"""
    from ...claude_integration import create_claude_service
    
    claude_service = create_claude_service()
    entropy_engine = create_shannon_entropy_engine(claude_service)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_information_units = {
        "requirements": [
            InformationUnit(
                unit_id="REQ-001",
                content="ç”¨æˆ·éœ€è¦èƒ½å¤Ÿç™»å½•ç³»ç»Ÿ",
                information_type=InformationType.REQUIREMENTS,
                source="user_story",
                timestamp=datetime.now()
            ),
            InformationUnit(
                unit_id="REQ-002", 
                content="ç³»ç»Ÿå¿…é¡»æ”¯æŒç”¨æˆ·ç™»å½•åŠŸèƒ½",
                information_type=InformationType.REQUIREMENTS,
                source="requirement_doc",
                timestamp=datetime.now()
            )
        ],
        "design": [
            InformationUnit(
                unit_id="DES-001",
                content="é‡‡ç”¨JWT tokenè¿›è¡Œèº«ä»½éªŒè¯",
                information_type=InformationType.DESIGN,
                source="architecture_doc",
                timestamp=datetime.now()
            )
        ],
        "communications": [
            InformationUnit(
                unit_id="COM-001",
                content="Agent Aå‘Agent Bå‘é€ä»»åŠ¡åˆ†é…è¯·æ±‚",
                information_type=InformationType.COMMUNICATIONS,
                source="agent_log",
                timestamp=datetime.now()
            ),
            InformationUnit(
                unit_id="COM-002",
                content="Agent Aåˆ†é…ä»»åŠ¡ç»™Agent B",
                information_type=InformationType.COMMUNICATIONS,
                source="system_log",
                timestamp=datetime.now()
            )
        ]
    }
    
    print("=== Shannonç†µè®¡ç®—å¼•æ“æ¼”ç¤º ===")
    
    try:
        # 1. æµ‹é‡ç³»ç»Ÿç†µ
        print("\n1. æµ‹é‡ç³»ç»ŸåŸºçº¿ç†µ...")
        entropy_profile = await entropy_engine.measure_system_entropy(test_information_units)
        
        print(f"ç³»ç»Ÿæ•´ä½“ç†µå€¼: {entropy_profile.overall_entropy:.3f}")
        print(f"ç»„ä»¶æ•°é‡: {len(entropy_profile.system_components)}")
        print(f"ç“¶é¢ˆç»„ä»¶: {', '.join(entropy_profile.bottleneck_components)}")
        print(f"ä¼˜åŒ–æœºä¼šæ•°: {len(entropy_profile.optimization_opportunities)}")
        
        # 2. ä¼˜åŒ–ç³»ç»Ÿç†µ
        print("\n2. æ‰§è¡Œç†µä¼˜åŒ–...")
        optimization_result = await entropy_engine.optimize_system_entropy(
            entropy_profile, 
            EntropyOptimizationLevel.STANDARD
        )
        
        print(f"åŸºçº¿ç†µå€¼: {optimization_result.baseline_entropy:.3f}")
        print(f"ä¼˜åŒ–åç†µå€¼: {optimization_result.optimized_entropy:.3f}")
        print(f"ç†µé™ä½: {optimization_result.entropy_reduction_percentage:.1f}%")
        print(f"å‹ç¼©æ¯”: {optimization_result.compression_ratio:.1f}:1")
        print(f"ä¿¡æ¯æŸå¤±ç‡: {optimization_result.information_loss_rate*100:.1f}%")
        
        # 3. ç”ŸæˆæŠ¥å‘Š
        print("\n3. ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š...")
        report = await entropy_engine.generate_entropy_report(entropy_profile, optimization_result)
        
        print(f"æŠ¥å‘ŠID: {report['report_id']}")
        achievement = report["achievement_assessment"]
        print(f"ç†µé™ä½ç›®æ ‡è¾¾æˆ: {'âœ…' if achievement['entropy_reduction_target']['achieved'] else 'âŒ'}")
        print(f"å‹ç¼©æ¯”ç›®æ ‡è¾¾æˆ: {'âœ…' if achievement['compression_ratio_target']['achieved'] else 'âŒ'}")
        print(f"ä¿¡æ¯æŸå¤±æ§åˆ¶: {'âœ…' if achievement['information_loss_target']['achieved'] else 'âŒ'}")
        print(f"æ•´ä½“æˆåŠŸ: {'âœ…' if achievement['overall_success'] else 'âŒ'}")
        
        print("\nä¸»è¦å»ºè®®:")
        for rec in optimization_result.recommendations[:3]:
            print(f"- {rec}")
        
        print(f"\nå¤„ç†æ—¶é—´: {optimization_result.processing_time:.2f}ç§’")
        
        print("\nğŸ‰ Shannonç†µè®¡ç®—å¼•æ“æ ¸å¿ƒåŠŸèƒ½éªŒè¯å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {str(e)}")

if __name__ == "__main__":
    asyncio.run(demo_shannon_entropy_engine())