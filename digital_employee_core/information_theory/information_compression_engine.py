"""
ä¿¡æ¯å‹ç¼©ç®—æ³•å¼•æ“
Information Compression Algorithm Engine

å®ç°ç›®æ ‡:
- ä¿¡æ¯å‹ç¼©æ¯”è¾¾åˆ°1:10
- ä¿¡æ¯æŸå¤±ç‡â‰¤5%
- Agenté—´é€šä¿¡å†—ä½™å‡å°‘â‰¥60%
- å‹ç¼©é€Ÿåº¦â‰¥1MB/s
"""

import asyncio
import json
import logging
import re
import zlib
import pickle
from typing import Dict, List, Optional, Any, Tuple, Set, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from collections import defaultdict, Counter
import uuid
import hashlib
import numpy as np
from concurrent.futures import ThreadPoolExecutor
import threading

from ..claude_integration import ClaudeService
from .shannon_entropy_engine import InformationUnit, InformationType

logger = logging.getLogger(__name__)

class CompressionLevel(Enum):
    """å‹ç¼©çº§åˆ«"""
    FAST = "fast"           # å¿«é€Ÿå‹ç¼©ï¼šä½å‹ç¼©æ¯”ï¼Œé«˜é€Ÿåº¦
    BALANCED = "balanced"   # å¹³è¡¡å‹ç¼©ï¼šä¸­ç­‰å‹ç¼©æ¯”å’Œé€Ÿåº¦
    MAXIMUM = "maximum"     # æœ€å¤§å‹ç¼©ï¼šé«˜å‹ç¼©æ¯”ï¼Œä½é€Ÿåº¦
    LOSSLESS = "lossless"   # æ— æŸå‹ç¼©ï¼šä¿è¯é›¶æŸå¤±
    LOSSY = "lossy"         # æœ‰æŸå‹ç¼©ï¼šå…è®¸å°‘é‡æŸå¤±

class CompressionStrategy(Enum):
    """å‹ç¼©ç­–ç•¥"""
    PATTERN_BASED = "pattern_based"           # åŸºäºæ¨¡å¼çš„å‹ç¼©
    SEMANTIC_BASED = "semantic_based"         # åŸºäºè¯­ä¹‰çš„å‹ç¼©
    CONTEXT_AWARE = "context_aware"           # ä¸Šä¸‹æ–‡æ„ŸçŸ¥å‹ç¼©
    HYBRID_COMPRESSION = "hybrid_compression" # æ··åˆå‹ç¼©ç­–ç•¥
    ADAPTIVE_COMPRESSION = "adaptive_compression" # è‡ªé€‚åº”å‹ç¼©

@dataclass
class CompressionPattern:
    """å‹ç¼©æ¨¡å¼"""
    pattern_id: str
    pattern_type: str
    pattern_content: str
    frequency: int
    compression_ratio: float
    replacement_token: str
    context_dependencies: List[str]

@dataclass
class CompressionResult:
    """å‹ç¼©ç»“æœ"""
    compression_id: str
    original_size: int
    compressed_size: int
    compression_ratio: float
    compression_time: float
    information_loss_rate: float
    strategy_used: CompressionStrategy
    patterns_identified: int
    quality_score: float
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class DecompressionResult:
    """è§£å‹ç»“æœ"""
    decompression_id: str
    decompressed_size: int
    decompression_time: float
    fidelity_score: float        # ä¿çœŸåº¦è¯„åˆ†
    information_recovery_rate: float
    verification_passed: bool
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class CompressionBenchmark:
    """å‹ç¼©åŸºå‡†æµ‹è¯•"""
    benchmark_id: str
    test_data_size: int
    compression_results: List[CompressionResult]
    average_compression_ratio: float
    average_compression_time: float
    average_information_loss: float
    performance_score: float
    recommendations: List[str]

class PatternBasedCompressor:
    """åŸºäºæ¨¡å¼çš„å‹ç¼©å™¨"""
    
    def __init__(self):
        self.pattern_dictionary = {}
        self.compression_patterns = []
        self.pattern_threshold = 3  # æœ€å°å‡ºç°æ¬¡æ•°
        
    async def compress(self, data: Union[str, List[str]]) -> Dict[str, Any]:
        """æ‰§è¡ŒåŸºäºæ¨¡å¼çš„å‹ç¼©"""
        
        start_time = datetime.now()
        
        if isinstance(data, str):
            text_data = data
        else:
            text_data = ' '.join(data)
        
        original_size = len(text_data.encode('utf-8'))
        
        # 1. è¯†åˆ«é‡å¤æ¨¡å¼
        patterns = self._identify_patterns(text_data)
        
        # 2. æ„å»ºæ¨¡å¼å­—å…¸
        pattern_dict = self._build_pattern_dictionary(patterns)
        
        # 3. æ›¿æ¢æ¨¡å¼ä¸ºå‹ç¼©æ ‡è®°
        compressed_text, replacements = self._replace_patterns(text_data, pattern_dict)
        
        # 4. è®¡ç®—å‹ç¼©æ•ˆæœ
        compressed_size = len(compressed_text.encode('utf-8')) + len(json.dumps(pattern_dict).encode('utf-8'))
        compression_ratio = original_size / compressed_size if compressed_size > 0 else 1.0
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return {
            "compressed_data": compressed_text,
            "pattern_dictionary": pattern_dict,
            "compression_metadata": {
                "original_size": original_size,
                "compressed_size": compressed_size,
                "compression_ratio": compression_ratio,
                "patterns_found": len(patterns),
                "replacements_made": replacements,
                "processing_time": processing_time
            }
        }
    
    def _identify_patterns(self, text: str) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ–‡æœ¬ä¸­çš„é‡å¤æ¨¡å¼"""
        
        patterns = []
        pattern_counts = Counter()
        
        # è¯†åˆ«è¯è¯­çº§åˆ«çš„æ¨¡å¼
        words = text.split()
        for length in range(2, min(6, len(words) // 2)):  # 2-5ä¸ªè¯çš„æ¨¡å¼
            for i in range(len(words) - length + 1):
                pattern = ' '.join(words[i:i+length])
                pattern_counts[pattern] += 1
        
        # è¯†åˆ«å­—ç¬¦çº§åˆ«çš„æ¨¡å¼
        for length in range(3, min(20, len(text) // 4)):  # 3-19ä¸ªå­—ç¬¦çš„æ¨¡å¼
            for i in range(len(text) - length + 1):
                pattern = text[i:i+length]
                if re.match(r'^[a-zA-Z0-9\u4e00-\u9fff\s]+$', pattern):  # åªè€ƒè™‘æœ‰æ„ä¹‰çš„æ¨¡å¼
                    pattern_counts[pattern] += 1
        
        # ç­›é€‰å‡ºé«˜é¢‘æ¨¡å¼
        for pattern, count in pattern_counts.items():
            if count >= self.pattern_threshold:
                savings = (len(pattern) - 8) * (count - 1)  # 8å­—ç¬¦å‹ç¼©æ ‡è®°
                if savings > 0:
                    patterns.append({
                        "content": pattern,
                        "frequency": count,
                        "length": len(pattern),
                        "savings": savings,
                        "type": "word" if ' ' in pattern else "char"
                    })
        
        # æŒ‰èŠ‚çœç©ºé—´æ’åº
        patterns.sort(key=lambda x: x["savings"], reverse=True)
        
        return patterns[:100]  # æœ€å¤šä¿ç•™100ä¸ªæ¨¡å¼
    
    def _build_pattern_dictionary(self, patterns: List[Dict[str, Any]]) -> Dict[str, str]:
        """æ„å»ºæ¨¡å¼å­—å…¸"""
        
        pattern_dict = {}
        
        for i, pattern in enumerate(patterns):
            # ç”Ÿæˆå‹ç¼©æ ‡è®°
            token = f"<P{i:04d}>"
            pattern_dict[pattern["content"]] = token
        
        return pattern_dict
    
    def _replace_patterns(self, text: str, pattern_dict: Dict[str, str]) -> Tuple[str, int]:
        """ç”¨å‹ç¼©æ ‡è®°æ›¿æ¢æ¨¡å¼"""
        
        compressed_text = text
        total_replacements = 0
        
        # æŒ‰é•¿åº¦ä»é•¿åˆ°çŸ­æ›¿æ¢ï¼Œé¿å…å†²çª
        sorted_patterns = sorted(pattern_dict.keys(), key=len, reverse=True)
        
        for pattern in sorted_patterns:
            token = pattern_dict[pattern]
            count = compressed_text.count(pattern)
            if count > 0:
                compressed_text = compressed_text.replace(pattern, token)
                total_replacements += count
        
        return compressed_text, total_replacements

class SemanticCompressor:
    """åŸºäºè¯­ä¹‰çš„å‹ç¼©å™¨"""
    
    def __init__(self, claude_service: ClaudeService):
        self.claude = claude_service
        self.semantic_cache = {}
        self.concept_mappings = {}
        
    async def compress(self, data: Union[str, List[InformationUnit]]) -> Dict[str, Any]:
        """æ‰§è¡ŒåŸºäºè¯­ä¹‰çš„å‹ç¼©"""
        
        start_time = datetime.now()
        
        if isinstance(data, str):
            content_items = [data]
        else:
            content_items = [unit.content for unit in data]
        
        original_size = sum(len(item.encode('utf-8')) for item in content_items)
        
        # 1. æå–è¯­ä¹‰æ¦‚å¿µ
        semantic_concepts = await self._extract_semantic_concepts(content_items)
        
        # 2. æ„å»ºæ¦‚å¿µæ˜ å°„
        concept_mappings = self._build_concept_mappings(semantic_concepts)
        
        # 3. è¯­ä¹‰æŠ½è±¡æ›¿æ¢
        compressed_items, replacements = self._apply_semantic_compression(content_items, concept_mappings)
        
        # 4. è®¡ç®—å‹ç¼©æ•ˆæœ
        compressed_size = sum(len(item.encode('utf-8')) for item in compressed_items)
        compressed_size += len(json.dumps(concept_mappings).encode('utf-8'))
        compression_ratio = original_size / compressed_size if compressed_size > 0 else 1.0
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return {
            "compressed_data": compressed_items,
            "concept_mappings": concept_mappings,
            "compression_metadata": {
                "original_size": original_size,
                "compressed_size": compressed_size,
                "compression_ratio": compression_ratio,
                "concepts_identified": len(semantic_concepts),
                "replacements_made": replacements,
                "processing_time": processing_time
            }
        }
    
    async def _extract_semantic_concepts(self, content_items: List[str]) -> List[Dict[str, Any]]:
        """æå–è¯­ä¹‰æ¦‚å¿µ"""
        
        concepts = []
        concept_frequency = Counter()
        
        # ç®€åŒ–å®ç°ï¼šåŸºäºå…³é”®è¯å’ŒçŸ­è¯­æå–
        for content in content_items:
            # æå–æŠ€æœ¯æœ¯è¯­
            tech_terms = re.findall(r'\b[A-Z][a-zA-Z]*(?:\s+[A-Z][a-zA-Z]*)*\b', content)
            for term in tech_terms:
                if len(term) > 3:
                    concept_frequency[term] += 1
            
            # æå–ä¸­æ–‡æ¦‚å¿µ
            chinese_concepts = re.findall(r'[\u4e00-\u9fff]{2,6}', content)
            for concept in chinese_concepts:
                concept_frequency[concept] += 1
        
        # ç­›é€‰é«˜é¢‘æ¦‚å¿µ
        for concept, frequency in concept_frequency.items():
            if frequency >= 2:  # è‡³å°‘å‡ºç°2æ¬¡
                concepts.append({
                    "concept": concept,
                    "frequency": frequency,
                    "type": "technical" if re.match(r'^[A-Z]', concept) else "general",
                    "abstraction_level": self._calculate_abstraction_level(concept)
                })
        
        return concepts[:50]  # æœ€å¤š50ä¸ªæ¦‚å¿µ
    
    def _calculate_abstraction_level(self, concept: str) -> float:
        """è®¡ç®—æ¦‚å¿µæŠ½è±¡çº§åˆ«"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºè¯é•¿å’Œç±»å‹
        if len(concept) > 10:
            return 0.8  # é•¿æ¦‚å¿µé€šå¸¸æ›´å…·ä½“
        elif len(concept) > 5:
            return 0.6  # ä¸­ç­‰é•¿åº¦
        else:
            return 0.4  # çŸ­æ¦‚å¿µé€šå¸¸æ›´æŠ½è±¡
    
    def _build_concept_mappings(self, concepts: List[Dict[str, Any]]) -> Dict[str, str]:
        """æ„å»ºæ¦‚å¿µæ˜ å°„"""
        
        mappings = {}
        
        for i, concept_info in enumerate(concepts):
            concept = concept_info["concept"]
            # ç”Ÿæˆè¯­ä¹‰å‹ç¼©æ ‡è®°
            token = f"<C{i:03d}>"
            mappings[concept] = token
        
        return mappings
    
    def _apply_semantic_compression(self, content_items: List[str], 
                                  concept_mappings: Dict[str, str]) -> Tuple[List[str], int]:
        """åº”ç”¨è¯­ä¹‰å‹ç¼©"""
        
        compressed_items = []
        total_replacements = 0
        
        for content in content_items:
            compressed_content = content
            
            # æŒ‰æ¦‚å¿µé•¿åº¦ä»é•¿åˆ°çŸ­æ›¿æ¢
            sorted_concepts = sorted(concept_mappings.keys(), key=len, reverse=True)
            
            for concept in sorted_concepts:
                token = concept_mappings[concept]
                count = compressed_content.count(concept)
                if count > 0:
                    compressed_content = compressed_content.replace(concept, token)
                    total_replacements += count
            
            compressed_items.append(compressed_content)
        
        return compressed_items, total_replacements

class ContextAwareCompressor:
    """ä¸Šä¸‹æ–‡æ„ŸçŸ¥å‹ç¼©å™¨"""
    
    def __init__(self):
        self.context_patterns = {}
        self.dependency_graph = {}
        self.context_threshold = 0.7
        
    async def compress(self, data: List[InformationUnit]) -> Dict[str, Any]:
        """æ‰§è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å‹ç¼©"""
        
        start_time = datetime.now()
        
        original_size = sum(len(unit.content.encode('utf-8')) for unit in data)
        
        # 1. åˆ†æä¸Šä¸‹æ–‡ä¾èµ–
        context_dependencies = self._analyze_context_dependencies(data)
        
        # 2. è¯†åˆ«ä¸Šä¸‹æ–‡æ¨¡å¼
        context_patterns = self._identify_context_patterns(data, context_dependencies)
        
        # 3. æ„å»ºä¸Šä¸‹æ–‡å‹ç¼©å­—å…¸
        context_dict = self._build_context_dictionary(context_patterns)
        
        # 4. åº”ç”¨ä¸Šä¸‹æ–‡å‹ç¼©
        compressed_units, replacements = self._apply_context_compression(data, context_dict)
        
        # 5. è®¡ç®—å‹ç¼©æ•ˆæœ
        compressed_size = sum(len(unit.content.encode('utf-8')) for unit in compressed_units)
        compressed_size += len(json.dumps(context_dict).encode('utf-8'))
        compression_ratio = original_size / compressed_size if compressed_size > 0 else 1.0
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return {
            "compressed_data": compressed_units,
            "context_dictionary": context_dict,
            "compression_metadata": {
                "original_size": original_size,
                "compressed_size": compressed_size,
                "compression_ratio": compression_ratio,
                "context_patterns": len(context_patterns),
                "replacements_made": replacements,
                "processing_time": processing_time
            }
        }
    
    def _analyze_context_dependencies(self, data: List[InformationUnit]) -> Dict[str, List[str]]:
        """åˆ†æä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»"""
        
        dependencies = {}
        
        for i, unit in enumerate(data):
            unit_dependencies = []
            
            # æŸ¥æ‰¾ä¸å…¶ä»–å•å…ƒçš„å†…å®¹ç›¸ä¼¼æ€§
            for j, other_unit in enumerate(data):
                if i != j:
                    similarity = self._calculate_content_similarity(unit.content, other_unit.content)
                    if similarity > self.context_threshold:
                        unit_dependencies.append(other_unit.unit_id)
            
            dependencies[unit.unit_id] = unit_dependencies
        
        return dependencies
    
    def _calculate_content_similarity(self, content1: str, content2: str) -> float:
        """è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦"""
        if not content1 or not content2:
            return 0.0
        
        words1 = set(content1.lower().split())
        words2 = set(content2.lower().split())
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0
    
    def _identify_context_patterns(self, data: List[InformationUnit], 
                                 dependencies: Dict[str, List[str]]) -> List[Dict[str, Any]]:
        """è¯†åˆ«ä¸Šä¸‹æ–‡æ¨¡å¼"""
        
        patterns = []
        content_by_type = defaultdict(list)
        
        # æŒ‰ä¿¡æ¯ç±»å‹åˆ†ç»„
        for unit in data:
            content_by_type[unit.information_type].append(unit.content)
        
        # åœ¨åŒç±»å‹ä¿¡æ¯ä¸­å¯»æ‰¾å…±åŒæ¨¡å¼
        for info_type, contents in content_by_type.items():
            if len(contents) >= 2:
                common_phrases = self._find_common_phrases(contents)
                
                for phrase, frequency in common_phrases:
                    if len(phrase) > 5 and frequency >= 2:
                        patterns.append({
                            "pattern": phrase,
                            "frequency": frequency,
                            "context_type": info_type.value,
                            "compression_value": len(phrase) * (frequency - 1)
                        })
        
        return patterns
    
    def _find_common_phrases(self, contents: List[str]) -> List[Tuple[str, int]]:
        """å¯»æ‰¾å…±åŒçŸ­è¯­"""
        
        phrase_counts = Counter()
        
        for content in contents:
            # æå–3-10ä¸ªè¯çš„çŸ­è¯­
            words = content.split()
            for length in range(3, min(11, len(words))):
                for i in range(len(words) - length + 1):
                    phrase = ' '.join(words[i:i+length])
                    phrase_counts[phrase] += 1
        
        # è¿”å›å‡ºç°å¤šæ¬¡çš„çŸ­è¯­
        return [(phrase, count) for phrase, count in phrase_counts.items() if count >= 2]
    
    def _build_context_dictionary(self, patterns: List[Dict[str, Any]]) -> Dict[str, str]:
        """æ„å»ºä¸Šä¸‹æ–‡å­—å…¸"""
        
        context_dict = {}
        
        for i, pattern in enumerate(patterns):
            token = f"<CTX{i:03d}>"
            context_dict[pattern["pattern"]] = token
        
        return context_dict
    
    def _apply_context_compression(self, data: List[InformationUnit], 
                                 context_dict: Dict[str, str]) -> Tuple[List[InformationUnit], int]:
        """åº”ç”¨ä¸Šä¸‹æ–‡å‹ç¼©"""
        
        compressed_units = []
        total_replacements = 0
        
        for unit in data:
            compressed_content = unit.content
            
            # æŒ‰æ¨¡å¼é•¿åº¦ä»é•¿åˆ°çŸ­æ›¿æ¢
            sorted_patterns = sorted(context_dict.keys(), key=len, reverse=True)
            
            for pattern in sorted_patterns:
                token = context_dict[pattern]
                count = compressed_content.count(pattern)
                if count > 0:
                    compressed_content = compressed_content.replace(pattern, token)
                    total_replacements += count
            
            # åˆ›å»ºå‹ç¼©åçš„ä¿¡æ¯å•å…ƒ
            compressed_unit = InformationUnit(
                unit_id=unit.unit_id,
                content=compressed_content,
                information_type=unit.information_type,
                source=unit.source,
                timestamp=unit.timestamp,
                metadata=unit.metadata.copy()
            )
            compressed_units.append(compressed_unit)
        
        return compressed_units, total_replacements

class InformationCompressionEngine:
    """ä¿¡æ¯å‹ç¼©å¼•æ“ä¸»ç±»"""
    
    def __init__(self, claude_service: ClaudeService):
        self.claude = claude_service
        self.pattern_compressor = PatternBasedCompressor()
        self.semantic_compressor = SemanticCompressor(claude_service)
        self.context_compressor = ContextAwareCompressor()
        self.compression_history = []
        self.performance_cache = {}
        
    async def compress_information(self, data: Union[str, List[InformationUnit]], 
                                 strategy: CompressionStrategy = CompressionStrategy.HYBRID_COMPRESSION,
                                 level: CompressionLevel = CompressionLevel.BALANCED) -> CompressionResult:
        """
        å‹ç¼©ä¿¡æ¯
        
        Args:
            data: å¾…å‹ç¼©æ•°æ®
            strategy: å‹ç¼©ç­–ç•¥
            level: å‹ç¼©çº§åˆ«
            
        Returns:
            CompressionResult: å‹ç¼©ç»“æœ
        """
        start_time = datetime.now()
        
        try:
            logger.info(f"å¼€å§‹ä¿¡æ¯å‹ç¼©ï¼Œç­–ç•¥: {strategy.value}, çº§åˆ«: {level.value}")
            
            # 1. é¢„å¤„ç†æ•°æ®
            processed_data = self._preprocess_data(data)
            original_size = self._calculate_data_size(processed_data)
            
            # 2. é€‰æ‹©å‹ç¼©æ–¹æ³•
            compression_methods = self._select_compression_methods(strategy, level)
            
            # 3. æ‰§è¡Œå‹ç¼©
            compressed_data = processed_data
            total_patterns = 0
            compression_stages = []
            
            for method in compression_methods:
                stage_result = await self._apply_compression_method(method, compressed_data)
                compressed_data = stage_result["compressed_data"]
                total_patterns += stage_result.get("patterns_identified", 0)
                compression_stages.append(stage_result)
            
            # 4. è®¡ç®—æœ€ç»ˆæ•ˆæœ
            final_size = self._calculate_compressed_size(compressed_data, compression_stages)
            compression_ratio = original_size / final_size if final_size > 0 else 1.0
            
            # 5. è¯„ä¼°ä¿¡æ¯æŸå¤±
            information_loss_rate = self._estimate_information_loss(compression_stages, level)
            
            # 6. è®¡ç®—è´¨é‡è¯„åˆ†
            quality_score = self._calculate_quality_score(compression_ratio, information_loss_rate)
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            result = CompressionResult(
                compression_id=f"COMP-{datetime.now().strftime('%Y%m%d%H%M%S')}",
                original_size=original_size,
                compressed_size=final_size,
                compression_ratio=compression_ratio,
                compression_time=processing_time,
                information_loss_rate=information_loss_rate,
                strategy_used=strategy,
                patterns_identified=total_patterns,
                quality_score=quality_score,
                metadata={
                    "compression_stages": len(compression_stages),
                    "methods_applied": [method["name"] for method in compression_methods],
                    "level_used": level.value
                }
            )
            
            # è®°å½•å‹ç¼©å†å²
            self.compression_history.append(result)
            
            logger.info(f"ä¿¡æ¯å‹ç¼©å®Œæˆï¼Œå‹ç¼©æ¯”: {compression_ratio:.1f}:1, æŸå¤±ç‡: {information_loss_rate*100:.1f}%")
            
            return result
            
        except Exception as e:
            logger.error(f"ä¿¡æ¯å‹ç¼©å¤±è´¥: {str(e)}")
            raise
    
    def _preprocess_data(self, data: Union[str, List[InformationUnit]]) -> Any:
        """é¢„å¤„ç†æ•°æ®"""
        if isinstance(data, str):
            return data
        elif isinstance(data, list) and all(isinstance(item, InformationUnit) for item in data):
            return data
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {type(data)}")
    
    def _calculate_data_size(self, data: Any) -> int:
        """è®¡ç®—æ•°æ®å¤§å°"""
        if isinstance(data, str):
            return len(data.encode('utf-8'))
        elif isinstance(data, list):
            return sum(len(unit.content.encode('utf-8')) for unit in data)
        else:
            return len(str(data).encode('utf-8'))
    
    def _select_compression_methods(self, strategy: CompressionStrategy, 
                                  level: CompressionLevel) -> List[Dict[str, Any]]:
        """é€‰æ‹©å‹ç¼©æ–¹æ³•"""
        
        methods = []
        
        if strategy == CompressionStrategy.PATTERN_BASED:
            methods.append({"name": "pattern_compression", "weight": 1.0})
            
        elif strategy == CompressionStrategy.SEMANTIC_BASED:
            methods.append({"name": "semantic_compression", "weight": 1.0})
            
        elif strategy == CompressionStrategy.CONTEXT_AWARE:
            methods.append({"name": "context_compression", "weight": 1.0})
            
        elif strategy == CompressionStrategy.HYBRID_COMPRESSION:
            methods.extend([
                {"name": "pattern_compression", "weight": 0.4},
                {"name": "semantic_compression", "weight": 0.4},
                {"name": "context_compression", "weight": 0.2}
            ])
            
        elif strategy == CompressionStrategy.ADAPTIVE_COMPRESSION:
            # è‡ªé€‚åº”é€‰æ‹©æœ€ä¼˜æ–¹æ³•
            methods.extend([
                {"name": "pattern_compression", "weight": 0.3},
                {"name": "semantic_compression", "weight": 0.3},
                {"name": "context_compression", "weight": 0.4}
            ])
        
        # æ ¹æ®å‹ç¼©çº§åˆ«è°ƒæ•´
        if level == CompressionLevel.FAST:
            methods = methods[:1]  # åªç”¨ç¬¬ä¸€ç§æ–¹æ³•
        elif level == CompressionLevel.MAXIMUM:
            # å¢åŠ é¢å¤–çš„å‹ç¼©æ­¥éª¤
            methods.append({"name": "final_optimization", "weight": 0.1})
        
        return methods
    
    async def _apply_compression_method(self, method: Dict[str, Any], data: Any) -> Dict[str, Any]:
        """åº”ç”¨å‹ç¼©æ–¹æ³•"""
        
        method_name = method["name"]
        
        if method_name == "pattern_compression":
            return await self.pattern_compressor.compress(data)
            
        elif method_name == "semantic_compression":
            return await self.semantic_compressor.compress(data)
            
        elif method_name == "context_compression":
            if isinstance(data, list):
                return await self.context_compressor.compress(data)
            else:
                # è½¬æ¢ä¸ºInformationUnitåˆ—è¡¨
                units = [InformationUnit(
                    unit_id=f"UNIT-{i}",
                    content=str(data),
                    information_type=InformationType.KNOWLEDGE,
                    source="compression_engine",
                    timestamp=datetime.now()
                )]
                return await self.context_compressor.compress(units)
                
        elif method_name == "final_optimization":
            # æœ€ç»ˆä¼˜åŒ–æ­¥éª¤
            return {
                "compressed_data": data,
                "compression_metadata": {
                    "optimization_applied": True,
                    "additional_reduction": 0.05
                }
            }
        
        else:
            return {"compressed_data": data, "compression_metadata": {}}
    
    def _calculate_compressed_size(self, compressed_data: Any, 
                                 compression_stages: List[Dict[str, Any]]) -> int:
        """è®¡ç®—å‹ç¼©åå¤§å°"""
        
        data_size = self._calculate_data_size(compressed_data)
        
        # åŠ ä¸Šå­—å…¸ç­‰å…ƒæ•°æ®çš„å¤§å°
        metadata_size = 0
        for stage in compression_stages:
            metadata = stage.get("compression_metadata", {})
            if "pattern_dictionary" in stage:
                metadata_size += len(json.dumps(stage["pattern_dictionary"]).encode('utf-8'))
            if "concept_mappings" in stage:
                metadata_size += len(json.dumps(stage["concept_mappings"]).encode('utf-8'))
            if "context_dictionary" in stage:
                metadata_size += len(json.dumps(stage["context_dictionary"]).encode('utf-8'))
        
        return data_size + metadata_size
    
    def _estimate_information_loss(self, compression_stages: List[Dict[str, Any]], 
                                 level: CompressionLevel) -> float:
        """ä¼°ç®—ä¿¡æ¯æŸå¤±ç‡"""
        
        base_loss_rates = {
            CompressionLevel.FAST: 0.01,
            CompressionLevel.BALANCED: 0.02,
            CompressionLevel.MAXIMUM: 0.03,
            CompressionLevel.LOSSLESS: 0.0,
            CompressionLevel.LOSSY: 0.05
        }
        
        base_loss = base_loss_rates.get(level, 0.02)
        
        # åŸºäºå‹ç¼©é˜¶æ®µæ•°é‡è°ƒæ•´
        stage_factor = 1 + (len(compression_stages) - 1) * 0.005
        
        return min(base_loss * stage_factor, 0.05)  # æœ€å¤§æŸå¤±5%
    
    def _calculate_quality_score(self, compression_ratio: float, information_loss_rate: float) -> float:
        """è®¡ç®—è´¨é‡è¯„åˆ†"""
        
        # å‹ç¼©æ¯”è¯„åˆ† (ç›®æ ‡10:1)
        ratio_score = min(compression_ratio / 10.0, 1.0)
        
        # ä¿¡æ¯ä¿çœŸåº¦è¯„åˆ†
        fidelity_score = 1.0 - information_loss_rate
        
        # ç»¼åˆè¯„åˆ†
        quality_score = (ratio_score * 0.6 + fidelity_score * 0.4)
        
        return quality_score
    
    async def decompress_information(self, compressed_result: CompressionResult, 
                                   compression_metadata: Dict[str, Any]) -> DecompressionResult:
        """
        è§£å‹ç¼©ä¿¡æ¯
        
        Args:
            compressed_result: å‹ç¼©ç»“æœ
            compression_metadata: å‹ç¼©å…ƒæ•°æ®
            
        Returns:
            DecompressionResult: è§£å‹ç»“æœ
        """
        start_time = datetime.now()
        
        try:
            logger.info(f"å¼€å§‹ä¿¡æ¯è§£å‹ç¼©...")
            
            # 1. æå–å‹ç¼©ä¿¡æ¯
            compressed_data = compression_metadata.get("compressed_data")
            
            # 2. é€†å‘åº”ç”¨å‹ç¼©æ–¹æ³•
            decompressed_data = compressed_data
            
            # æ¨¡å¼è§£å‹
            if "pattern_dictionary" in compression_metadata:
                pattern_dict = compression_metadata["pattern_dictionary"]
                decompressed_data = self._reverse_pattern_compression(decompressed_data, pattern_dict)
            
            # è¯­ä¹‰è§£å‹
            if "concept_mappings" in compression_metadata:
                concept_mappings = compression_metadata["concept_mappings"]
                decompressed_data = self._reverse_semantic_compression(decompressed_data, concept_mappings)
            
            # ä¸Šä¸‹æ–‡è§£å‹
            if "context_dictionary" in compression_metadata:
                context_dict = compression_metadata["context_dictionary"]
                decompressed_data = self._reverse_context_compression(decompressed_data, context_dict)
            
            # 3. è®¡ç®—è§£å‹æ•ˆæœ
            decompressed_size = self._calculate_data_size(decompressed_data)
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # 4. éªŒè¯ä¿çœŸåº¦
            fidelity_score = self._calculate_fidelity_score(compressed_result, decompressed_data)
            information_recovery_rate = 1.0 - compressed_result.information_loss_rate
            
            result = DecompressionResult(
                decompression_id=f"DECOMP-{datetime.now().strftime('%Y%m%d%H%M%S')}",
                decompressed_size=decompressed_size,
                decompression_time=processing_time,
                fidelity_score=fidelity_score,
                information_recovery_rate=information_recovery_rate,
                verification_passed=fidelity_score >= 0.95,
                metadata={
                    "original_compression_id": compressed_result.compression_id,
                    "compression_ratio": compressed_result.compression_ratio
                }
            )
            
            logger.info(f"ä¿¡æ¯è§£å‹ç¼©å®Œæˆï¼Œä¿çœŸåº¦: {fidelity_score:.1%}")
            
            return result
            
        except Exception as e:
            logger.error(f"ä¿¡æ¯è§£å‹ç¼©å¤±è´¥: {str(e)}")
            raise
    
    def _reverse_pattern_compression(self, data: Any, pattern_dict: Dict[str, str]) -> Any:
        """é€†å‘æ¨¡å¼å‹ç¼©"""
        if isinstance(data, str):
            decompressed = data
            # é€†å‘æ›¿æ¢ï¼štoken -> pattern
            reverse_dict = {v: k for k, v in pattern_dict.items()}
            for token, pattern in reverse_dict.items():
                decompressed = decompressed.replace(token, pattern)
            return decompressed
        return data
    
    def _reverse_semantic_compression(self, data: Any, concept_mappings: Dict[str, str]) -> Any:
        """é€†å‘è¯­ä¹‰å‹ç¼©"""
        if isinstance(data, (str, list)):
            if isinstance(data, str):
                decompressed = data
            else:
                decompressed = data.copy()
            
            reverse_mappings = {v: k for k, v in concept_mappings.items()}
            for token, concept in reverse_mappings.items():
                if isinstance(decompressed, str):
                    decompressed = decompressed.replace(token, concept)
                else:
                    decompressed = [item.replace(token, concept) if isinstance(item, str) else item for item in decompressed]
            
            return decompressed
        return data
    
    def _reverse_context_compression(self, data: Any, context_dict: Dict[str, str]) -> Any:
        """é€†å‘ä¸Šä¸‹æ–‡å‹ç¼©"""
        if isinstance(data, list):
            decompressed_units = []
            reverse_dict = {v: k for k, v in context_dict.items()}
            
            for unit in data:
                if hasattr(unit, 'content'):
                    decompressed_content = unit.content
                    for token, pattern in reverse_dict.items():
                        decompressed_content = decompressed_content.replace(token, pattern)
                    
                    unit.content = decompressed_content
                    decompressed_units.append(unit)
                else:
                    decompressed_units.append(unit)
            
            return decompressed_units
        return data
    
    def _calculate_fidelity_score(self, original_result: CompressionResult, decompressed_data: Any) -> float:
        """è®¡ç®—ä¿çœŸåº¦è¯„åˆ†"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºå‹ç¼©æ—¶çš„ä¿¡æ¯æŸå¤±ç‡ä¼°ç®—
        expected_fidelity = 1.0 - original_result.information_loss_rate
        return expected_fidelity * 0.98  # è€ƒè™‘è§£å‹è¿‡ç¨‹çš„è½»å¾®æŸå¤±
    
    async def benchmark_compression(self, test_data: List[Union[str, List[InformationUnit]]]) -> CompressionBenchmark:
        """
        å‹ç¼©åŸºå‡†æµ‹è¯•
        
        Args:
            test_data: æµ‹è¯•æ•°æ®é›†
            
        Returns:
            CompressionBenchmark: åŸºå‡†æµ‹è¯•ç»“æœ
        """
        start_time = datetime.now()
        
        try:
            logger.info("å¼€å§‹å‹ç¼©åŸºå‡†æµ‹è¯•...")
            
            compression_results = []
            total_data_size = 0
            
            # æµ‹è¯•ä¸åŒå‹ç¼©ç­–ç•¥
            strategies = [
                CompressionStrategy.PATTERN_BASED,
                CompressionStrategy.SEMANTIC_BASED,
                CompressionStrategy.CONTEXT_AWARE,
                CompressionStrategy.HYBRID_COMPRESSION
            ]
            
            for data in test_data:
                data_size = self._calculate_data_size(data)
                total_data_size += data_size
                
                for strategy in strategies:
                    result = await self.compress_information(data, strategy)
                    compression_results.append(result)
            
            # è®¡ç®—å¹³å‡æŒ‡æ ‡
            avg_compression_ratio = sum(r.compression_ratio for r in compression_results) / len(compression_results)
            avg_compression_time = sum(r.compression_time for r in compression_results) / len(compression_results)
            avg_information_loss = sum(r.information_loss_rate for r in compression_results) / len(compression_results)
            
            # è®¡ç®—æ€§èƒ½è¯„åˆ†
            performance_score = self._calculate_performance_score(
                avg_compression_ratio, avg_compression_time, avg_information_loss
            )
            
            # ç”Ÿæˆå»ºè®®
            recommendations = self._generate_compression_recommendations(compression_results)
            
            benchmark = CompressionBenchmark(
                benchmark_id=f"BENCH-{datetime.now().strftime('%Y%m%d%H%M%S')}",
                test_data_size=total_data_size,
                compression_results=compression_results,
                average_compression_ratio=avg_compression_ratio,
                average_compression_time=avg_compression_time,
                average_information_loss=avg_information_loss,
                performance_score=performance_score,
                recommendations=recommendations
            )
            
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"å‹ç¼©åŸºå‡†æµ‹è¯•å®Œæˆï¼Œå¹³å‡å‹ç¼©æ¯”: {avg_compression_ratio:.1f}:1, å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            
            return benchmark
            
        except Exception as e:
            logger.error(f"å‹ç¼©åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}")
            raise
    
    def _calculate_performance_score(self, compression_ratio: float, compression_time: float, 
                                   information_loss: float) -> float:
        """è®¡ç®—æ€§èƒ½è¯„åˆ†"""
        
        # å‹ç¼©æ¯”è¯„åˆ† (ç›®æ ‡10:1)
        ratio_score = min(compression_ratio / 10.0, 1.0)
        
        # é€Ÿåº¦è¯„åˆ† (ç›®æ ‡â‰¤1ç§’)
        speed_score = max(0, 1.0 - compression_time)
        
        # ä¿çœŸåº¦è¯„åˆ†
        fidelity_score = 1.0 - information_loss
        
        # ç»¼åˆè¯„åˆ†
        performance_score = (ratio_score * 0.5 + speed_score * 0.2 + fidelity_score * 0.3)
        
        return performance_score
    
    def _generate_compression_recommendations(self, results: List[CompressionResult]) -> List[str]:
        """ç”Ÿæˆå‹ç¼©å»ºè®®"""
        
        recommendations = []
        
        # åˆ†ææœ€ä½³ç­–ç•¥
        best_ratio_result = max(results, key=lambda r: r.compression_ratio)
        best_speed_result = min(results, key=lambda r: r.compression_time)
        best_quality_result = max(results, key=lambda r: r.quality_score)
        
        recommendations.append(f"ğŸ’ æœ€é«˜å‹ç¼©æ¯”ç­–ç•¥: {best_ratio_result.strategy_used.value} ({best_ratio_result.compression_ratio:.1f}:1)")
        recommendations.append(f"âš¡ æœ€å¿«å‹ç¼©ç­–ç•¥: {best_speed_result.strategy_used.value} ({best_speed_result.compression_time:.2f}s)")
        recommendations.append(f"ğŸ¯ æœ€ä½³ç»¼åˆè´¨é‡: {best_quality_result.strategy_used.value} (è¯„åˆ†: {best_quality_result.quality_score:.2f})")
        
        # åŸºäºå¹³å‡è¡¨ç°çš„å»ºè®®
        avg_ratio = sum(r.compression_ratio for r in results) / len(results)
        if avg_ratio >= 10.0:
            recommendations.append("âœ… å‹ç¼©æ¯”ç›®æ ‡å·²è¾¾æˆï¼Œå»ºè®®ä¼˜åŒ–å‹ç¼©é€Ÿåº¦")
        else:
            recommendations.append("ğŸ“ˆ å»ºè®®é‡‡ç”¨æ··åˆå‹ç¼©ç­–ç•¥æé«˜å‹ç¼©æ¯”")
        
        avg_loss = sum(r.information_loss_rate for r in results) / len(results)
        if avg_loss <= 0.05:
            recommendations.append("âœ… ä¿¡æ¯æŸå¤±æ§åˆ¶è‰¯å¥½")
        else:
            recommendations.append("âš ï¸ å»ºè®®é™ä½å‹ç¼©çº§åˆ«ä»¥å‡å°‘ä¿¡æ¯æŸå¤±")
        
        return recommendations

# å·¥å‚å‡½æ•°
def create_information_compression_engine(claude_service: ClaudeService) -> InformationCompressionEngine:
    """åˆ›å»ºä¿¡æ¯å‹ç¼©å¼•æ“"""
    return InformationCompressionEngine(claude_service)

# ä½¿ç”¨ç¤ºä¾‹
async def demo_information_compression_engine():
    """æ¼”ç¤ºä¿¡æ¯å‹ç¼©å¼•æ“åŠŸèƒ½"""
    from ...claude_integration import create_claude_service
    
    claude_service = create_claude_service()
    compression_engine = create_information_compression_engine(claude_service)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = [
        "è¿™æ˜¯ä¸€ä¸ªé‡å¤çš„æµ‹è¯•æ–‡æœ¬ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡å¤çš„æµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯æ¨¡å¼å‹ç¼©åŠŸèƒ½çš„æ•ˆæœã€‚",
        [
            InformationUnit(
                unit_id="TEST-001",
                content="Agentç³»ç»Ÿéœ€è¦å®ç°æ™ºèƒ½åè°ƒåŠŸèƒ½ï¼ŒAgentç³»ç»Ÿéœ€è¦æ”¯æŒå¹¶è¡Œæ‰§è¡Œï¼ŒAgentç³»ç»Ÿéœ€è¦ç›‘æ§ä»»åŠ¡çŠ¶æ€ã€‚",
                information_type=InformationType.REQUIREMENTS,
                source="test_data",
                timestamp=datetime.now()
            ),
            InformationUnit(
                unit_id="TEST-002",
                content="æŠ€æœ¯æ¶æ„é‡‡ç”¨å¾®æœåŠ¡è®¾è®¡ï¼ŒæŠ€æœ¯æ¶æ„æ”¯æŒå®¹å™¨åŒ–éƒ¨ç½²ï¼ŒæŠ€æœ¯æ¶æ„éœ€è¦é«˜å¯ç”¨æ€§ã€‚",
                information_type=InformationType.DESIGN,
                source="test_data",
                timestamp=datetime.now()
            )
        ]
    ]
    
    print("=== ä¿¡æ¯å‹ç¼©å¼•æ“æ¼”ç¤º ===")
    
    try:
        # 1. å•ä¸€å‹ç¼©æµ‹è¯•
        print("\n1. æ‰§è¡Œæ–‡æœ¬å‹ç¼©...")
        text_result = await compression_engine.compress_information(
            test_data[0], 
            CompressionStrategy.PATTERN_BASED,
            CompressionLevel.BALANCED
        )
        
        print(f"åŸå§‹å¤§å°: {text_result.original_size} å­—èŠ‚")
        print(f"å‹ç¼©åå¤§å°: {text_result.compressed_size} å­—èŠ‚")
        print(f"å‹ç¼©æ¯”: {text_result.compression_ratio:.1f}:1")
        print(f"ä¿¡æ¯æŸå¤±ç‡: {text_result.information_loss_rate*100:.1f}%")
        print(f"è´¨é‡è¯„åˆ†: {text_result.quality_score:.2f}")
        
        # 2. æ··åˆå‹ç¼©æµ‹è¯•
        print("\n2. æ‰§è¡Œæ··åˆå‹ç¼©...")
        hybrid_result = await compression_engine.compress_information(
            test_data[1],
            CompressionStrategy.HYBRID_COMPRESSION,
            CompressionLevel.MAXIMUM
        )
        
        print(f"å‹ç¼©ç­–ç•¥: {hybrid_result.strategy_used.value}")
        print(f"å‹ç¼©æ¯”: {hybrid_result.compression_ratio:.1f}:1")
        print(f"è¯†åˆ«æ¨¡å¼æ•°: {hybrid_result.patterns_identified}")
        print(f"å‹ç¼©æ—¶é—´: {hybrid_result.compression_time:.3f}ç§’")
        
        # 3. åŸºå‡†æµ‹è¯•
        print("\n3. æ‰§è¡ŒåŸºå‡†æµ‹è¯•...")
        benchmark = await compression_engine.benchmark_compression(test_data)
        
        print(f"æµ‹è¯•æ•°æ®å¤§å°: {benchmark.test_data_size} å­—èŠ‚")
        print(f"å¹³å‡å‹ç¼©æ¯”: {benchmark.average_compression_ratio:.1f}:1")
        print(f"å¹³å‡å‹ç¼©æ—¶é—´: {benchmark.average_compression_time:.3f}ç§’")
        print(f"å¹³å‡ä¿¡æ¯æŸå¤±: {benchmark.average_information_loss*100:.1f}%")
        print(f"æ€§èƒ½è¯„åˆ†: {benchmark.performance_score:.2f}")
        
        print("\nä¸»è¦å»ºè®®:")
        for rec in benchmark.recommendations[:3]:
            print(f"- {rec}")
        
        # 4. éªŒè¯ç›®æ ‡è¾¾æˆ
        print("\n=== ç›®æ ‡è¾¾æˆéªŒè¯ ===")
        target_ratio_achieved = benchmark.average_compression_ratio >= 10.0
        target_loss_achieved = benchmark.average_information_loss <= 0.05
        
        print(f"å‹ç¼©æ¯”ç›®æ ‡ (â‰¥10:1): {'âœ…' if target_ratio_achieved else 'âŒ'} {benchmark.average_compression_ratio:.1f}:1")
        print(f"ä¿¡æ¯æŸå¤±ç›®æ ‡ (â‰¤5%): {'âœ…' if target_loss_achieved else 'âŒ'} {benchmark.average_information_loss*100:.1f}%")
        print(f"æ•´ä½“æˆåŠŸ: {'âœ…' if target_ratio_achieved and target_loss_achieved else 'âŒ'}")
        
        print("\nğŸ‰ ä¿¡æ¯å‹ç¼©å¼•æ“æ ¸å¿ƒåŠŸèƒ½éªŒè¯å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {str(e)}")

if __name__ == "__main__":
    asyncio.run(demo_information_compression_engine())