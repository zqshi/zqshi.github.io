# 水平Pod自动扩缩容配置
# Horizontal Pod Autoscaler Configuration

# 数字员工应用HPA
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: digital-employee-app-hpa
  namespace: digital-employee
  labels:
    app: digital-employee
    component: hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: digital-employee-app
  minReplicas: 3
  maxReplicas: 20
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # 5分钟稳定窗口
      policies:
      - type: Percent
        value: 50  # 每次最多缩容50%
        periodSeconds: 60
      - type: Pods
        value: 2   # 每次最多缩容2个Pod
        periodSeconds: 60
      selectPolicy: Min  # 选择更保守的策略
    scaleUp:
      stabilizationWindowSeconds: 60   # 1分钟稳定窗口
      policies:
      - type: Percent
        value: 100  # 每次最多扩容100%
        periodSeconds: 60
      - type: Pods
        value: 4    # 每次最多扩容4个Pod
        periodSeconds: 60
      selectPolicy: Max  # 选择更激进的策略
  metrics:
  # CPU使用率指标
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # CPU使用率超过70%时扩容
  # 内存使用率指标
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80  # 内存使用率超过80%时扩容
  # 自定义指标：每秒请求数
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"  # 每个Pod平均100RPS时扩容
  # 自定义指标：响应时间
  - type: Pods
    pods:
      metric:
        name: http_request_duration_p95
      target:
        type: AverageValue
        averageValue: "2000m"  # 95%响应时间超过2秒时扩容

---
# Nginx负载均衡器HPA
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
  namespace: digital-employee
  labels:
    app: nginx
    component: hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx
  minReplicas: 2
  maxReplicas: 6
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 33  # 每次最多缩容33%
        periodSeconds: 60
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Max
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 70
  # 并发连接数指标
  - type: Pods
    pods:
      metric:
        name: nginx_active_connections
      target:
        type: AverageValue
        averageValue: "500"  # 每个Nginx实例平均500个活跃连接时扩容

---
# Redis集群HPA（针对Redis Sentinel）
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: redis-sentinel-hpa
  namespace: digital-employee
  labels:
    app: redis
    component: sentinel-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: redis-sentinel
  minReplicas: 3
  maxReplicas: 7  # 保持奇数个Sentinel
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 600  # 10分钟稳定窗口（较长）
      policies:
      - type: Pods
        value: 2  # 每次最多缩容2个，保持奇数
        periodSeconds: 120
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 120
      policies:
      - type: Pods
        value: 2  # 每次最多扩容2个，保持奇数
        periodSeconds: 60
      selectPolicy: Max
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 60

---
# 垂直Pod自动扩缩容配置（VPA）
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: digital-employee-app-vpa
  namespace: digital-employee
  labels:
    app: digital-employee
    component: vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: digital-employee-app
  updatePolicy:
    updateMode: "Auto"  # 自动更新资源限制
  resourcePolicy:
    containerPolicies:
    - containerName: digital-employee
      minAllowed:
        cpu: 200m
        memory: 512Mi
      maxAllowed:
        cpu: 4000m
        memory: 8Gi
      controlledResources: ["cpu", "memory"]
      controlledValues: RequestsAndLimits

---
# PodDisruptionBudget - 确保高可用性
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: digital-employee-app-pdb
  namespace: digital-employee
  labels:
    app: digital-employee
    component: pdb
spec:
  minAvailable: 2  # 至少保持2个Pod运行
  selector:
    matchLabels:
      app: digital-employee
      component: app

---
# Nginx PodDisruptionBudget
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: nginx-pdb
  namespace: digital-employee
  labels:
    app: nginx
    component: pdb
spec:
  minAvailable: 1  # 至少保持1个Nginx Pod运行
  selector:
    matchLabels:
      app: nginx
      component: loadbalancer

---
# PostgreSQL PodDisruptionBudget
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: postgres-pdb
  namespace: digital-employee
  labels:
    app: postgres
    component: pdb
spec:
  minAvailable: 1  # 至少保持1个PostgreSQL Pod运行（主库）
  selector:
    matchLabels:
      app: postgres
      component: primary

---
# Redis PodDisruptionBudget
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: redis-pdb
  namespace: digital-employee
  labels:
    app: redis
    component: pdb
spec:
  minAvailable: 1  # 至少保持1个Redis Master运行
  selector:
    matchLabels:
      app: redis
      component: master

---
# Prometheus PodDisruptionBudget
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: prometheus-pdb
  namespace: digital-employee
  labels:
    app: prometheus
    component: pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: prometheus
      component: server

---
# 自定义指标服务器配置（用于HPA自定义指标）
apiVersion: v1
kind: Service
metadata:
  name: custom-metrics-server
  namespace: digital-employee
  labels:
    app: custom-metrics
    component: server
spec:
  type: ClusterIP
  ports:
  - port: 8080
    targetPort: 8080
    protocol: TCP
    name: metrics
  selector:
    app: custom-metrics
    component: server

---
# 自定义指标适配器Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: custom-metrics-adapter
  namespace: digital-employee
  labels:
    app: custom-metrics
    component: adapter
spec:
  replicas: 1
  selector:
    matchLabels:
      app: custom-metrics
      component: adapter
  template:
    metadata:
      labels:
        app: custom-metrics
        component: adapter
    spec:
      serviceAccountName: custom-metrics-sa
      containers:
      - name: custom-metrics-adapter
        image: k8s.gcr.io/prometheus-adapter/prometheus-adapter:v0.11.0
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 6443
          name: https
          protocol: TCP
        - containerPort: 8080
          name: http
          protocol: TCP
        args:
        - --cert-dir=/var/run/serving-cert
        - --config=/etc/adapter/config.yaml
        - --logtostderr=true
        - --prometheus-url=http://prometheus-svc:9090/
        - --secure-port=6443
        - --tls-cert-file=/var/run/serving-cert/tls.crt
        - --tls-private-key-file=/var/run/serving-cert/tls.key
        volumeMounts:
        - name: config
          mountPath: /etc/adapter
          readOnly: true
        - name: serving-cert
          mountPath: /var/run/serving-cert
          readOnly: true
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 30
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /healthz
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          timeoutSeconds: 5
      volumes:
      - name: config
        configMap:
          name: adapter-config
      - name: serving-cert
        secret:
          secretName: cm-adapter-serving-certs

---
# 自定义指标ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: custom-metrics-sa
  namespace: digital-employee

---
# 自定义指标ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: custom-metrics-server
rules:
- apiGroups: [""]
  resources: ["pods", "nodes", "nodes/stats"]
  verbs: ["get", "list"]
- apiGroups: ["extensions"]
  resources: ["deployments"]
  verbs: ["get", "list"]

---
# 自定义指标ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: custom-metrics-server
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: custom-metrics-server
subjects:
- kind: ServiceAccount
  name: custom-metrics-sa
  namespace: digital-employee

---
# 自定义指标适配器配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: adapter-config
  namespace: digital-employee
  labels:
    app: custom-metrics
    component: config
data:
  config.yaml: |
    rules:
    # HTTP请求率指标
    - seriesQuery: 'http_requests_total{namespace!="",pod!=""}'
      resources:
        overrides:
          namespace: {resource: "namespace"}
          pod: {resource: "pod"}
      name:
        matches: "^(.*)_total"
        as: "${1}_per_second"
      metricsQuery: 'rate(<<.Series>>{<<.LabelMatchers>>}[2m])'
    
    # HTTP请求响应时间指标
    - seriesQuery: 'http_request_duration_seconds{namespace!="",pod!=""}'
      resources:
        overrides:
          namespace: {resource: "namespace"}
          pod: {resource: "pod"}
      name:
        matches: "^(.*)_seconds"
        as: "${1}_p95"
      metricsQuery: 'histogram_quantile(0.95, rate(<<.Series>>_bucket{<<.LabelMatchers>>}[2m]))'
    
    # Nginx活跃连接数指标
    - seriesQuery: 'nginx_active_connections{namespace!="",pod!=""}'
      resources:
        overrides:
          namespace: {resource: "namespace"}
          pod: {resource: "pod"}
      name:
        as: "nginx_active_connections"
      metricsQuery: '<<.Series>>{<<.LabelMatchers>>}'
    
    # 数据库连接数指标
    - seriesQuery: 'postgresql_active_connections{namespace!="",pod!=""}'
      resources:
        overrides:
          namespace: {resource: "namespace"}
          pod: {resource: "pod"}
      name:
        as: "db_active_connections"
      metricsQuery: '<<.Series>>{<<.LabelMatchers>>}'
    
    # Redis内存使用指标
    - seriesQuery: 'redis_memory_used_bytes{namespace!="",pod!=""}'
      resources:
        overrides:
          namespace: {resource: "namespace"}
          pod: {resource: "pod"}
      name:
        as: "redis_memory_usage_percent"
      metricsQuery: '(<<.Series>>{<<.LabelMatchers>>} / redis_memory_max_bytes{<<.LabelMatchers>>}) * 100'

---
# 扩缩容事件监控
apiVersion: v1
kind: ConfigMap
metadata:
  name: hpa-monitoring-script
  namespace: digital-employee
  labels:
    app: monitoring
    component: hpa-script
data:
  monitor-hpa.sh: |
    #!/bin/bash
    # HPA扩缩容事件监控脚本
    
    NAMESPACE="digital-employee"
    WEBHOOK_URL="${WEBHOOK_URL:-http://webhook-service:5000/hpa-events}"
    
    # 监控HPA事件
    kubectl get events -n $NAMESPACE --field-selector type=Normal,reason=SuccessfulRescale -w --output-watch-events | \
    while read event; do
        # 解析事件信息
        object=$(echo $event | jq -r '.object.involvedObject.name')
        message=$(echo $event | jq -r '.object.message')
        timestamp=$(echo $event | jq -r '.object.firstTimestamp')
        
        # 发送到Webhook
        curl -X POST $WEBHOOK_URL \
            -H "Content-Type: application/json" \
            -d "{
                \"type\": \"hpa_event\",
                \"object\": \"$object\",
                \"message\": \"$message\",
                \"timestamp\": \"$timestamp\",
                \"namespace\": \"$NAMESPACE\"
            }"
    done

---
# HPA状态监控CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hpa-status-monitor
  namespace: digital-employee
  labels:
    app: monitoring
    component: hpa-cronjob
spec:
  schedule: "*/5 * * * *"  # 每5分钟执行一次
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: digital-employee-sa
          containers:
          - name: hpa-monitor
            image: bitnami/kubectl:latest
            imagePullPolicy: IfNotPresent
            command:
            - /bin/bash
            - -c
            - |
              echo "=== HPA Status Monitor ==="
              echo "Timestamp: $(date)"
              
              # 检查所有HPA状态
              kubectl get hpa -n digital-employee -o json | jq -r '
                .items[] | 
                "HPA: \(.metadata.name) | Current: \(.status.currentReplicas) | Desired: \(.status.desiredReplicas) | Min: \(.spec.minReplicas) | Max: \(.spec.maxReplicas)"
              '
              
              # 检查是否有扩缩容异常
              kubectl get hpa -n digital-employee -o json | jq -r '
                .items[] | 
                select(.status.conditions[]? | select(.type == "AbleToScale" and .status == "False")) |
                "WARNING: HPA \(.metadata.name) unable to scale: \(.status.conditions[] | select(.type == "AbleToScale").message)"
              '
              
              echo "=== End HPA Status Monitor ==="
            resources:
              requests:
                memory: "64Mi"
                cpu: "50m"
              limits:
                memory: "128Mi"
                cpu: "100m"
          restartPolicy: Never
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1